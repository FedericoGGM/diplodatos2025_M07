# -*- coding: utf-8 -*-
"""TP3-_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Gge4Ch_HQK9xCaDXSljzikYAw2bcA3Z

# **El Impacto De Las Condiciones Ambientales En La Calidad Del Agua Del R√≠o De La Plata**

**TP2 - TP3 **

Bibliografia:
-  Intendencia de Montevideo. (2022). Estudio de la calidad de agua del R√≠o de la Plata: Sistema Oeste, agosto‚Äìdiciembre 2021. Montevideo: Servicio de Evaluaci√≥n de la Calidad y Control Ambiental. Recuperado de https://montevideo.gub.uy/sites/default/files/biblioteca/informeestudiodelacalidadaguadelriodelaplatasistemaoesteagosto-diciembre2021.pdf

- Intendencia de Montevideo. (2015). Estudio de l√≠nea de base del R√≠o de la Plata [Informe t√©cnico]. Montevideo: Intendencia de Montevideo, Departamento de Calidad Ambiental. Recuperado de
 https://www.montevideo.gub.uy/sites/default/files/biblioteca/informeestudiodelineadebaseriodelaplata2015final.pdf

###Libreria
"""

#Librerias

import pandas as pd
import unicodedata
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import folium
import ee
import geemap
import datetime
import re
import matplotlib.pyplot as plt
from tqdm import tqdm
from IPython.display import display, HTML
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from scipy.optimize import curve_fit
from sklearn.metrics import r2_score
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
from datetime import datetime
from folium import Map, Marker, LayerControl, TileLayer, features
from branca.element import MacroElement
from jinja2 import Template
from scipy.stats import pearsonr, spearmanr
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import make_scorer, r2_score, mean_squared_error
from datetime import timedelta
from tqdm.notebook import tqdm

"""####Dataset"""

# Cargar el dataset

url = "https://raw.githubusercontent.com/MaricelSantos/Mentoria--Diplodatos-2025/main/Conexiones_Transparentes.csv"
df = pd.read_csv(url)

"""#Variable objetivo

Se defini√≥ la variable clorofila_a_ug_l como objetivo del modelo, por ser un indicador ambiental clave vinculado a la calidad del agua. Esta variable es continua.  

**La medici√≥n de la clorofila en el agua es importante para evaluar la calidad del agua y la salud del ecosistema, ya que niveles altos de clorofila pueden indicar eutrofizaci√≥n y desequilibrios ecol√≥gicos.**

Otras variables complementarias fue campa√±a, fecha, turbidez del agua, temperatura

## *Resumen de descripci√≥n de la variable clorofila_a_ug_l*
"""

# Filtrar solo registros con clorofila v√°lida, pero conservar todas las columnas
df_validos = df[df['clorofila_a_ug_l'].notna()].copy()

df_validos['clorofila'] = pd.to_numeric(df_validos['clorofila_a_ug_l'], errors='coerce')

# === 1) Trabajar sobre el texto original (antes de convertir a n√∫mero) ===
s_raw = df['clorofila_a_ug_l'].astype('string')  # preserva NA de pandas

def _normalize(x: pd.Series) -> pd.Series:
    y = x.fillna('')
    # quitar tildes
    y = y.apply(lambda t: unicodedata.normalize('NFKD', t).encode('ascii', 'ignore').decode('ascii'))
    # min√∫scula, trim y colapsar espacios
    y = y.str.lower().str.strip().str.replace(r'\s+', ' ', regex=True)
    return y

s_norm = _normalize(s_raw)

# === 2) M√°scaras por categor√≠a ===
# < n√∫mero (acepta coma o punto y espacios)
mask_lt = s_norm.str.contains(r'^\s*<\s*\d+(?:[.,]\d+)?\s*$', na=False)
# > n√∫mero
mask_gt = s_norm.str.contains(r'^\s*>\s*\d+(?:[.,]\d+)?\s*$', na=False)
# "no se midio / no se midi√≥" (ya normalizado sin tildes)
mask_no_midio = s_norm.str.contains(r'\bno\s*se\s*midio\b', na=False)
# Otros textos de faltante (si quer√©s ampliar, agreg√° m√°s t√©rminos dentro del grupo)
mask_sin_dato = s_norm.str_contains if hasattr(s_norm, 'str_contains') else s_norm.str.contains
mask_sin_dato = s_norm.str.contains(r'\b(?:sin dato|no medido|no medidos|no disponible|nd)\b', na=False)

# Subcategor√≠as observadas en tus datos (para no mezclar todo como "mal cargado")
mask_inaccesible = s_norm.str.contains(r'\binaccesible\b', na=False)
mask_no_muestreo = s_norm.str.contains(r'\bno\s*(?:se\s*)?muestreo\b', na=False) | s_norm.str.contains(r'\bno\s*muestreo\b', na=False) | s_norm.str.contains(r'\bno\s*muestreo\b', na=False) | s_norm.str.contains(r'\bno\s*muestreo\b', na=False)
# mejor regex robusto para "no muestre√≥ / no se muestre√≥"
mask_no_muestreo = s_norm.str.contains(r'\bno\s*(?:se\s*)?muestreo\b', na=False) | s_norm.str.contains(r'\bno\s*(?:se\s*)?m(ue|ue)streo\b', na=False) | s_norm.str.contains(r'\bno\s*(?:se\s*)?muestreo\b', na=False)
# Simplificamos con una expresi√≥n m√°s general:
mask_no_muestreo = s_norm.str.contains(r'\bno\s*(?:se\s*)?muestr(e|eo|e[o√≥])\w*\b', na=False)

mask_en_obra = s_norm.str.contains(r'\ben obra\b', na=False)
mask_no_midieron_dia = s_norm.str.contains(r'\bno\s*midieron\s*este\s*dia\b', na=False)

# === 3) Conteos base ===
total_valores = len(s_raw)
valores_nulos = s_raw.isna().sum()
valores_con_lt = mask_lt.sum()
valores_con_gt = mask_gt.sum()
valores_no_se_midio = mask_no_midio.sum()
valores_otro_texto_faltante = mask_sin_dato.sum()

# Subcategor√≠as
c_inaccesible = mask_inaccesible.sum()
c_no_muestreo = mask_no_muestreo.sum()
c_en_obra = mask_en_obra.sum()
c_no_midieron_dia = mask_no_midieron_dia.sum()

# Identificar n√∫meros v√°lidos (para aislar textos que no son n√∫mero)
es_numero_valido = pd.to_numeric(s_norm.str.replace(',', '.', regex=False), errors='coerce').notna()

# Texto no vac√≠o:
mask_texto = ~es_numero_valido & s_norm.ne('')

# Conjunto de categor√≠as textuales ya identificadas
mask_categorizado = (
    mask_lt | mask_gt | mask_no_midio | mask_sin_dato |
    mask_inaccesible | mask_no_muestreo | mask_en_obra | mask_no_midieron_dia
)

# Resto de textos no categorizados (mal cargados)
valores_mal_cargados = (mask_texto & ~mask_categorizado).sum()

# === 4) Resumen ===
resumen = pd.DataFrame({
    'Descripci√≥n': [
        'Total de valores',
        'Valores nulos (NaN)',
        'Valores con "<x"',
        'Valores con ">x"',
        'Valores con "no se midio/midi√≥"',
        'Otros textos de faltante ("sin dato", "no medido", "nd")',
        '‚Äî Subcat: "inaccesible"',
        '‚Äî Subcat: "no muestre√≥ / no se muestre√≥"',
        '‚Äî Subcat: "en obra"',
        '‚Äî Subcat: "no midieron este d√≠a"',
        'Valores de texto no categorizado (mal cargados)'
    ],
    'Cantidad': [
        total_valores,
        valores_nulos,
        valores_con_lt,
        valores_con_gt,
        valores_no_se_midio,
        valores_otro_texto_faltante,
        c_inaccesible,
        c_no_muestreo,
        c_en_obra,
        c_no_midieron_dia,
        valores_mal_cargados
    ]
})

print("Resumen de descripci√≥n de la variable objeto:")
print(resumen)

# === 5) Ejemplos por categor√≠a ===
def _ejemplos(mask, titulo, n=10):
    ej = df.loc[mask, 'clorofila_a_ug_l'].head(n)
    print(f"\nEjemplos: {titulo} (hasta {n})")
    if len(ej) == 0:
        print("‚Äî (sin ejemplos)")
    else:
        print(ej.to_string(index=False))

_ejemplos(mask_lt, 'valores con "<x"')
_ejemplos(mask_gt, 'valores con ">x"')
_ejemplos(mask_no_midio, 'valores con "no se midio/midi√≥"')
_ejemplos(mask_sin_dato, 'otros textos de faltante ("sin dato", "no medido", "nd")')
_ejemplos(mask_inaccesible, 'subcat: "inaccesible"')
_ejemplos(mask_no_muestreo, 'subcat: "no muestre√≥ / no se muestre√≥"')
_ejemplos(mask_en_obra, 'subcat: "en obra"')
_ejemplos(mask_no_midieron_dia, 'subcat: "no midieron este d√≠a"')

"""En Argentina, no hay una normativa espec√≠fica nacional que indique c√≥mo tratar los valores censurados (‚Äú< LOD‚Äù) en clorofila, pero el procedimiento sigue est√°ndares internacionales adoptados a nivel local, y se cumple mediante acreditaci√≥n de laboratorios seg√∫n normas como ISO/IEC 17025.
Cuando una muetra tiene valores como <0.5 ¬µg/L, no es un cero, sino que est√° por debajo del l√≠mite que el m√©todo puede medir.
Los valores por debajo del l√≠mite de detecci√≥n (LD) no deben interpretarse como ceros, ya que representan concentraciones existentes pero no cuantificables con precisi√≥n. En estos casos, dichos valores se consideran censurados inferiores y pueden sustituirse por un valor constante, como por ejemplo, la mitad del LD (LD/2) o el LD dividido por la ra√≠z cuadrada de 2 (LD/‚àö2), seg√∫n recomendaciones de la EPA y buenas pr√°cticas estad√≠sticas. Esta imputaci√≥n conservadora permite incluir dichos datos en los an√°lisis sin introducir sesgos significativos.

*Minimo, Maximo*
"""

# Convertir a num√©rico forzando errores a NaN
df['clorofila_a_ug_l'] = pd.to_numeric(df['clorofila_a_ug_l'], errors='coerce')

# Mostrar m√≠nimo y m√°ximo
print(" Valores de clorofila v√°lidos (¬µg/L):")
print("M√≠nimo:", df['clorofila_a_ug_l'].min())
print("M√°ximo:", df['clorofila_a_ug_l'].max())

"""##Estaditica por campa√±a y a√±o"""

# Limpieza de campa√±a
df['campa√±a'] = df['campa√±a'].astype(str).str.strip().str.lower()

# Limpieza de a√±o
df['a√±o'] = pd.to_numeric(df['a√±o'], errors='coerce')  # convierte '2021.0' -> 2021.0 y descarta strings

# Convertir clorofila a num√©rico
df['clorofila_a_ug_l'] = pd.to_numeric(df['clorofila_a_ug_l'], errors='coerce')

# Filtrar solo datos v√°lidos
df_valido = df[
    df['clorofila_a_ug_l'].notna() &
    df['campa√±a'].isin(['invierno', 'primavera']) &
    df['a√±o'].isin([2021, 2022,2023])
]

# Agrupar por campa√±a y a√±o, calcular estad√≠sticas
estadisticas = df_valido.groupby(['campa√±a', 'a√±o'])['clorofila_a_ug_l'].agg(
    media='mean',
    m√≠nimo='min',
    m√°ximo='max',
    desv√≠o_est√°ndar='std'
).reset_index()

# Mostrar resultados
print(" Estad√≠sticas de clorofila por campa√±a y a√±o (¬µg/L):")
print(estadisticas)

"""Los resultados muestran que en invierno y primavera de 2021, las concentraciones de clorofila-a fueron muy bajas, con medias por debajo de 0.08 ¬µg/L, valores m√°ximos moderados (‚â§ 0.84 ¬µg/L) y desv√≠os est√°ndar reducidos, lo que indica una distribuci√≥n homog√©nea y condiciones oligotr√≥ficas (baja productividad biol√≥gica) generalizadas durante ese a√±o.
Sin embargo, en invierno de 2022 se observa un salto abrupto en la media (1578 ¬µg/L) y en el valor m√°ximo (6410 ¬µg/L), junto con un desv√≠o est√°ndar extremadamente alto (1564 ¬µg/L). Esta combinaci√≥n sugiere la presencia de valores an√≥malamente elevados que podr√≠an corresponder a errores de medici√≥n, cargas puntuales intensas de nutrientes, floraciones algales extremas o errores en la estimaci√≥n satelital o interpolaci√≥n de datos.
En primavera de 2022, si bien los valores tambi√©n son significativamente m√°s altos que en 2021, la media se reduce a 14.3 ¬µg/L con un m√°ximo de 92.4 ¬µg/L. El desv√≠o est√°ndar sigue siendo considerable (22.95), lo que indica mayor heterogeneidad espacial, aunque m√°s moderada que en el invierno anterior.

### Nuevo dataset con eliminacion de Nan, valores mal cargado
"""

# --- Normalizaci√≥n para analizar texto ---
s_raw = df[COL].astype("string").fillna("")

def _normalize(x: pd.Series) -> pd.Series:
    y = x.apply(lambda t: unicodedata.normalize("NFKD", t).encode("ascii","ignore").decode("ascii"))
    y = y.str.lower().str.strip().str.replace(r"\s+", " ", regex=True)
    return y

s_norm = _normalize(s_raw)

# --- Detectar casos especiales ---
mask_lt             = s_norm.str.contains(r'^\s*<\s*\d+(?:[.,]\d+)?\s*$', na=False)
mask_gt             = s_norm.str.contains(r'^\s*>\s*\d+(?:[.,]\d+)?\s*$', na=False)
mask_no_midio       = s_norm.str.contains(r'\bno\s*se\s*midio\b', na=False)
mask_inaccesible    = s_norm.str.contains(r'\binaccesible\b', na=False)
mask_no_muestreo    = s_norm.str.contains(r'\bno\s*(?:se\s*)?muestr\w+\b', na=False)
mask_en_obra        = s_norm.str.contains(r'\ben obra\b', na=False)
mask_no_midieron_dia= s_norm.str.contains(r'\bno\s*midieron\s*este\s*dia\b', na=False)

# Detectar n√∫meros v√°lidos
es_numero_valido = pd.to_numeric(s_norm.str.replace(",", ".", regex=False), errors="coerce").notna()
mask_texto       = ~es_numero_valido & s_norm.ne("")

# Texto que no cae en ninguna categor√≠a conocida ‚Üí mal cargados
mask_categorizado = (
    mask_no_midio | mask_inaccesible | mask_no_muestreo |
    mask_en_obra | mask_no_midieron_dia | mask_lt | mask_gt
)
mask_mal_cargado = mask_texto & ~mask_categorizado

# --- Crear dataset nuevo ---
df_nuevo = df.copy()

# Reemplazar "<x" por el valor num√©rico limpio
df_nuevo.loc[mask_lt, COL] = (
    s_raw[mask_lt]
    .str.replace("<", "", regex=False)
    .str.strip()
    .str.replace(",", ".", regex=False)
)

# Convertir toda la columna a num√©rico
df_nuevo[COL] = pd.to_numeric(df_nuevo[COL], errors="coerce")

# --- Filtrar excluyendo las categor√≠as indicadas ---
mask_to_drop = (
    df_nuevo[COL].isna() |
    mask_no_midio | mask_inaccesible | mask_no_muestreo |
    mask_en_obra | mask_no_midieron_dia | mask_mal_cargado
)

df_nuevo = df_nuevo.loc[~mask_to_drop].copy()
df_nuevo.reset_index(drop=True, inplace=True)

# --- Reporte ---
conteos = {
    "Valores nulos (NaN)": int(df[COL].isna().sum()),
    '"no se midio/midi√≥"': int(mask_no_midio.sum()),
    "inaccesible": int(mask_inaccesible.sum()),
    "no muestre√≥ / no se muestre√≥": int(mask_no_muestreo.sum()),
    "en obra": int(mask_en_obra.sum()),
    "no midieron este dia": int(mask_no_midieron_dia.sum()),
    "texto no categorizado (mal cargados)": int(mask_mal_cargado.sum())
}

print("Conteos de lo eliminado:")
print(pd.Series(conteos).to_frame("Cantidad"))

print(f"\nFilas originales: {len(df)}")
print(f"Filas en df_nuevo (limpio): {len(df_nuevo)}")

"""## Clasificar los valores de clorofila seg√∫n rangos ambientales de referencia

###*Valores con clorofila 0*
"""

# Identificacion de valores v√°lidos que no caen en ninguno de los tres rangos
df['clorofila_a_ug_l'] = pd.to_numeric(df['clorofila_a_ug_l'], errors='coerce')
df_validos = df[df['clorofila_a_ug_l'].notna()]

df_faltantes = df_validos[
    ~(
        ((df_validos['clorofila_a_ug_l'] > 0) & (df_validos['clorofila_a_ug_l'] <= 5)) |
        ((df_validos['clorofila_a_ug_l'] >= 5) & (df_validos['clorofila_a_ug_l'] <= 100)) |
        ((df_validos['clorofila_a_ug_l'] > 100) & (df_validos['clorofila_a_ug_l'] <= 6500))
    )
]

print(f"N√∫mero de valores fuera de los rangos definidos: {len(df_faltantes)}")
display(df_faltantes[['clorofila_a_ug_l', 'latitud', 'longitud', 'a√±o', 'campa√±a']])

"""Como paso adicional, se prob√≥ clasificar los valores de clorofila seg√∫n rangos ambientales de referencia: de 0 a 5‚ÄØ¬µg/L, de 5 a 100‚ÄØ¬µg/L y de 100 a 6500‚ÄØ¬µg/L, con el objetivo de observar en qu√© franjas se concentra la mayor cantidad de datos v√°lidos. Esta clasificaci√≥n busca dar contexto a los valores registrados en t√©rminos de calidad ambiental del agua. Si bien legalmente no existe un l√≠mite establecido para la concentraci√≥n de clorofila-a en agua potable en la provincia de Buenos Aires, se suelen utilizar valores de referencia orientativos: hasta 5‚ÄØ¬µg/L como indicativo de buena calidad del agua, y valores superiores a 100‚ÄØ¬µg/L como umbral de riesgo ecol√≥gico. Estos criterios permiten interpretar la informaci√≥n obtenida desde una perspectiva de gesti√≥n ambiental y alerta temprana.

###*Valores entre 0-5 ‚ÄØ¬µg/L*
"""

# --- Asegurar que la columna es num√©rica en el NUEVO dataset ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Filtrar valores v√°lidos num√©ricos entre 0 y 5 ¬µg/L (excluye 0) ---
df_filtrado = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 5)
].copy()

# --- Mostrar cantidad de valores que cumplen el filtro ---
print(f"Cantidad de valores v√°lidos entre 0 y 5 ¬µg/L: {len(df_filtrado)}")

# --- Calcular estad√≠sticas descriptivas ---
estadisticas = df_filtrado['clorofila_a_ug_l'].agg(['mean', 'min', 'max', 'std']).rename({
    'mean': 'media',
    'min': 'm√≠n',
    'max': 'm√°x',
    'std': 'desv√≠o est√°ndar'
})

print("\n Estad√≠sticas de clorofila_a_ug_l (¬µg/L):")
print(estadisticas)

# --- Seleccionar columnas relevantes ---
columnas = ['clorofila_a_ug_l', 'latitud', 'longitud', 'campa√±a', 'a√±o']

# Si hay al menos 10 registros, tomar muestra aleatoria de 10; si no, mostrar todos
if len(df_filtrado) >= 10:
    muestra = df_filtrado[columnas].sample(n=10, random_state=42)
else:
    muestra = df_filtrado[columnas]

# --- Mostrar resultado ---
print("\n Muestra de valores entre 0 y 5 ¬µg/L:")
print(muestra.reset_index(drop=True))

"""####Evoluci√≥n temporal de la calidad del agua seg√∫n clorofila-a (0‚Äì5‚ÄØ¬µg/L): Tendencias por campa√±a y a√±o (2021‚Äì2023)"""

# --- Convertir clorofila y a√±o a num√©rico ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')
df_nuevo['a√±o'] = pd.to_numeric(df_nuevo['a√±o'], errors='coerce')

# --- Normalizar texto de campa√±a (pasar a min√∫sculas) ---
df_nuevo['campa√±a'] = df_nuevo['campa√±a'].str.lower()

# --- Filtrar datos v√°lidos de clorofila entre 0 y 5 ¬µg/L y campa√±as espec√≠ficas ---
df_filtrado = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 5) &
    (df_nuevo['campa√±a'].isin(['invierno', 'primavera', 'oto√±o', 'verano'])) &
    (df_nuevo['a√±o'].isin([2021, 2022, 2023]))
].copy()

# --- Agrupar por campa√±a y a√±o y calcular estad√≠sticas ---
estadisticas = df_filtrado.groupby(['campa√±a', 'a√±o'])['clorofila_a_ug_l'].agg(
    media='mean',
    m√≠nimo='min',
    m√°ximo='max',
    mediana='median'
).reset_index()

# --- Ordenar por a√±o y campa√±a ---
estadisticas = estadisticas.sort_values(by=['a√±o', 'campa√±a'], ascending=[True, True])

# --- Mostrar resultados ---
print(" Estad√≠sticas de clorofila-a (0‚Äì5 ¬µg/L) para invierno, verano, oto√±o y primavera de 2021‚Äì2022‚Äì2023:")
print(estadisticas)

"""Grafico"""

# --- 6. Gr√°fico de barras ---
fig, ax = plt.subplots(figsize=(10,6))

# Crear un gr√°fico agrupado por a√±o
for a√±o in estadisticas['a√±o'].unique():
    subset = estadisticas[estadisticas['a√±o'] == a√±o]
    ax.bar(
        subset['campa√±a'].astype(str) + " " + subset['a√±o'].astype(str),
        subset['media'],
        label=f"A√±o {a√±o}"
    )

ax.set_title("üìä Media de Clorofila-a (0‚Äì5 ¬µg/L) por Campa√±a y A√±o")
ax.set_ylabel("Clorofila-a (¬µg/L)")
ax.set_xlabel("Campa√±a y A√±o")
ax.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Durante el a√±o 2021 se registraron niveles extremadamente bajos de clorofila-a en todas las estaciones del a√±o, con valores promedio cercanos a 0.07 Œºg/L. Esta baja concentraci√≥n sugiere la presencia de aguas muy limpias, posiblemente con escasa actividad fitoplanct√≥nica o baja productividad primaria.

En el a√±o 2022 se observa un leve aumento en los niveles de clorofila-a, especialmente durante la primavera, lo que podr√≠a indicar condiciones ligeramente m√°s favorables para el desarrollo del fitoplancton en comparaci√≥n con el a√±o anterior.

Por √∫ltimo, el a√±o 2023 evidencia un incremento marcado en la concentraci√≥n de clorofila-a, alcanzando valores promedio significativamente m√°s altos en invierno (~2.23 Œºg/L) y verano (casi 2.84 Œºg/L). Este aumento podr√≠a estar asociado a una mayor actividad biol√≥gica, mayor disponibilidad de nutrientes, o condiciones ambientales propicias para el crecimiento de algas.

#### Combinaciones presentes ANTES y DESPU√âS del filtro 0- 5
"""

def _norm_text_ser(s: pd.Series) -> pd.Series:
    s = s.astype("string").fillna("")
    # NFKD para quitar tildes ‚Üí ascii ‚Üí decode ‚Üí lower/strip/colapsar espacios
    s = s.apply(lambda t: unicodedata.normalize("NFKD", t).encode("ascii", "ignore").decode("utf-8"))
    s = s.str.lower().str.strip().str.replace(r"\s+", " ", regex=True)
    return s

def inspeccionar_dataset(df_in, nombre_df="df"):
    print(f"\n================= {nombre_df}: COMBINACIONES ANTES DEL FILTRO =================")
    # Mostrar √∫nicos crudos sin tocar el df original
    try:
        print(df_in[['campa√±a','a√±o']].dropna().drop_duplicates().sort_values(['a√±o','campa√±a']).to_string(index=False))
    except Exception:
        # Si hay problemas de tipos/NaN mezclados
        print(df_in[['campa√±a','a√±o']].drop_duplicates().to_string(index=False))

    # Copia de trabajo para normalizar
    tmp = df_in.copy()

    # Normalizaciones
    tmp['campa√±a'] = _norm_text_ser(tmp['campa√±a'])
    tmp['a√±o'] = pd.to_numeric(tmp['a√±o'], errors='coerce').astype('Int64')

    # Asegurar clorofila num√©rica
    tmp['clorofila_a_ug_l'] = pd.to_numeric(tmp['clorofila_a_ug_l'], errors='coerce')

    # Filtro (0‚Äì5 ¬µg/L, campa√±as v√°lidas, a√±os 2021‚Äì2023)
    mask = (
        tmp['clorofila_a_ug_l'].notna() &
        (tmp['clorofila_a_ug_l'] > 0) & (tmp['clorofila_a_ug_l'] <= 5) &
        (tmp['campa√±a'].isin(['invierno','primavera','oto√±o','verano'])) &
        (tmp['a√±o'].isin([2021, 2022, 2023]))
    )
    filtrado = tmp.loc[mask].copy()

    print(f"\n================= {nombre_df}: COMBINACIONES DESPU√âS DEL FILTRO (0‚Äì5 ¬µg/L) =================")
    if filtrado.empty:
        print("‚Äî (sin filas que cumplan el filtro)")
    else:
        print(filtrado[['campa√±a','a√±o']].drop_duplicates().sort_values(['a√±o','campa√±a']).to_string(index=False))

    # Conteo por a√±o x campa√±a
    print(f"\n================= {nombre_df}: CONTEO POR A√ëO x CAMPA√ëA (0‚Äì5 ¬µg/L) =================")
    if filtrado.empty:
        print("‚Äî (sin datos)")
    else:
        tabla = pd.crosstab(filtrado['a√±o'], filtrado['campa√±a']).fillna(0).astype(int)
        print(tabla)

# --- Ejecutar para el dataset original ---
inspeccionar_dataset(df, "df (original)")

# --- Ejecutar para el dataset nuevo (si existe) ---
try:
    df_nuevo  # verificar existencia
    inspeccionar_dataset(df_nuevo, "df_nuevo")
except NameError:
    print("\n[Aviso] 'df_nuevo' no est√° definido en este entorno. Primero gener√° el dataset nuevo.")

"""Se implement√≥ un proceso  de inspecci√≥n y limpieza de datos para asegurar la calidad y consistencia del conjunto de datos original.
Las campa√±as estacionales se normalizaron usando Unicode normalization (NFKD) para eliminar acentos y garantizar la uniformidad textual.
Las columnas de a√±o y clorofila-a se convierten a valores num√©ricos, descartando entradas inv√°lidas.
Se aplica un filtro que conserva solo aquellos registros que cumplen simult√°neamente con: A√±os entre 2021 y 2023; campa√±as definidas como "invierno", "primavera", "verano" u "oto√±o" y valores de clorofila-a dentro del rango 0‚Äì5 ¬µg/L

###*Valores entre 5-100 ‚ÄØ¬µg/L*
"""

# --- Filtrar clorofila entre 5 y 100 ¬µg/L ---
df_filtrado = df_nuevo[
    (df_nuevo['clorofila_a_ug_l'] >= 5) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
].copy()

# --- Mostrar cantidad de datos ---
print(f"Cantidad de datos en rango 5‚Äì100 ¬µg/L: {len(df_filtrado)}")

# --- Calcular estad√≠sticas ---
estadisticas = df_filtrado['clorofila_a_ug_l'].agg(['mean', 'min', 'max', 'std'])
estadisticas = estadisticas.rename({
    'mean': 'media',
    'min': 'm√≠n',
    'max': 'm√°x',
    'std': 'desv√≠o est√°ndar'
})

print("\n Estad√≠sticas de clorofila_a_ug_l (¬µg/L):")
print(estadisticas)

# --- Seleccionar columnas relevantes ---
columnas = ['clorofila_a_ug_l', 'campa√±a', 'a√±o', 'latitud', 'longitud']

# --- Mostrar las filas filtradas con esas columnas ---
print("\nMuestras en rango 5‚Äì100 ¬µg/L:")
print(df_filtrado[columnas].reset_index(drop=True))

"""###*Valores entre 100-6500 ‚ÄØ¬µg/L*"""

# --- Asegurar que la columna es num√©rica ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Filtrar valores v√°lidos num√©ricos entre 100 y 6500 ¬µg/L ---
df_filtrado = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 100) &
    (df_nuevo['clorofila_a_ug_l'] <= 6500)
].copy()

# --- Mostrar cantidad de valores que cumplen el filtro ---
print(f"Cantidad de valores v√°lidos entre 100 y 6500 ¬µg/L: {len(df_filtrado)}")

# --- Calcular estad√≠sticas ---
estadisticas = df_filtrado['clorofila_a_ug_l'].agg(['mean', 'min', 'max', 'std'])
estadisticas = estadisticas.rename({
    'mean': 'media',
    'min': 'm√≠n',
    'max': 'm√°x',
    'std': 'desv√≠o est√°ndar'
})

print("\n Estad√≠sticas de clorofila_a_ug_l (¬µg/L):")
print(estadisticas)

# --- Seleccionar columnas relevantes ---
columnas = ['clorofila_a_ug_l', 'latitud', 'longitud', 'campa√±a', 'a√±o']

# Si hay suficientes filas, tomar una muestra aleatoria de 10
if len(df_filtrado) >= 10:
    muestra = df_filtrado[columnas].sample(n=10, random_state=42)
else:
    muestra = df_filtrado[columnas]

# --- Mostrar resultado ---
print("\nMuestra de valores en rango 100‚Äì6500 ¬µg/L:")
print(muestra.reset_index(drop=True))

"""###Rango 0 a 100 ug/l"""

# --- Partir de df_nuevo ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')
df_validos = df_nuevo[df_nuevo['clorofila_a_ug_l'].notna()].copy()

# Categorizar cada valor en un rango
def categorizar(valor):
    if valor == 0:
        return "0"
    elif 0 < valor <= 5:
        return "0‚Äì5"
    elif 5 < valor <= 100:
        return "5‚Äì100"
    elif 100 < valor <= 6500:
        return "100‚Äì6500"
    else:
        return "Fuera de rango"

df_validos["rango"] = df_validos["clorofila_a_ug_l"].apply(categorizar)

# --- Gr√°fico de tiro al blanco ---
fig, ax = plt.subplots(figsize=(6,6))

# C√≠rculos conc√©ntricos que representan los rangos
circle0 = plt.Circle((0,0), 1, color='white', edgecolor='black', lw=1, label='0')
circle1 = plt.Circle((0,0), 2, color='green', alpha=0.3, label='0‚Äì5 ¬µg/L')
circle2 = plt.Circle((0,0), 3, color='blue', alpha=0.3, label='5‚Äì100 ¬µg/L')
circle3 = plt.Circle((0,0), 4, color='orange', alpha=0.3, label='100‚Äì6500 ¬µg/L')

for c in [circle3, circle2, circle1, circle0]:
    ax.add_artist(c)

# Ubicar puntos de cada registro (aleatorios en √°ngulo para que no se encimen)
np.random.seed(42)
for _, row in df_validos.iterrows():
    r = {"0":0.5, "0‚Äì5":1.5, "5‚Äì100":2.5, "100‚Äì6500":3.5, "Fuera de rango":4.5}[row["rango"]]
    theta = np.random.uniform(0, 2*np.pi)
    ax.plot(r*np.cos(theta), r*np.sin(theta), 'o',
            color='red' if row["rango"]=="Fuera de rango" else 'black', alpha=0.7)

# Est√©tica
ax.set_xlim(-5,5)
ax.set_ylim(-5,5)
ax.set_aspect('equal', 'box')
ax.set_xticks([])
ax.set_yticks([])
ax.set_title("Tiro al blanco ‚Äì Rangos de clorofila (¬µg/L)")
ax.legend(loc="upper right")

plt.show()

"""El resultado es un dataset de 277 datos, se considera tomar todos los valores de 0 a 100 ug/l en base al analisis anterior dando lugar a un total de 215 datos. Se deja por fuera tambien los valores igual a 0.

#### Estadisticas del rango de 0 a 100
"""

# --- Asegurar que la columna es num√©rica ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Filtrar rango 0‚Äì100 ¬µg/L (excluyendo 0 si quer√©s) ---
valores = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
]['clorofila_a_ug_l']

# --- Calcular estad√≠sticas ---
estadisticas = valores.agg([
    'count',   # cantidad total
    'mean',    # media
    'median',  # mediana
    'min',     # m√≠nimo
    'max',     # m√°ximo
    'std'      # desviaci√≥n est√°ndar
]).rename({
    'count': 'Cantidad',
    'mean': 'Media',
    'median': 'Mediana',
    'min': 'M√≠nimo',
    'max': 'M√°ximo',
    'std': 'Desv√≠o est√°ndar'
})

# --- Calcular percentiles adicionales ---
percentiles = valores.quantile([0.25, 0.75])
estadisticas['Percentil 25%'] = percentiles.loc[0.25]
estadisticas['Percentil 75%'] = percentiles.loc[0.75]

print(" Estad√≠sticas descriptivas de clorofila_a_ug_l en el rango 0‚Äì100 ¬µg/L:")
print(estadisticas)

"""Se analiza la cantidad de valores con coordenadas"""

# --- Asegurar que la columna de clorofila es num√©rica ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Filtrar valores en el rango 0‚Äì100 ¬µg/L ---
df_rango = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
].copy()

# --- Filtrar los que adem√°s tienen latitud y longitud v√°lidos ---
df_rango_coords = df_rango[
    df_rango['latitud'].notna() & df_rango['longitud'].notna()
]

# --- Contar ---
total_en_rango = len(df_rango)
con_coords = len(df_rango_coords)

print(f" Total de datos en rango 0‚Äì100 ¬µg/L: {total_en_rango}")
print(f" De esos, tienen latitud y longitud v√°lidos: {con_coords}")

# (Opcional) mostrar las primeras filas con coordenadas
print("\nEjemplos con coordenadas:")
print(df_rango_coords[['clorofila_a_ug_l', 'latitud', 'longitud']].head(10))

"""Se observa un resumen del filtrado y georreferenciaci√≥n de los datos de clorofila-a dentro del rango v√°lido de 0 a 100‚ÄØ¬µg/L. En total, se identificaron 215 muestras que cumplen ese criterio. De esas, 89 registros cuentan con coordenadas geogr√°ficas v√°lidas de latitud y longitud.

Adem√°s, se listan algunos ejemplos de esas muestras con coordenadas v√°lidas, mostrando los valores de clorofila-a y su ubicaci√≥n geogr√°fica asociada. Esto permite verificar la integridad de los datos espaciales y confirmar que hay diversidad de puntos de muestreo distribuidos en la regi√≥n de estudio.

Normalizar la latitud y longuitud
"""

def _fix_decimal_dots(s: str) -> str:
    """
    Mantiene solo el primer punto decimal en la cadena.
    - Reemplaza comas por punto.
    - Si hay >1 punto, conserva el primero y elimina los dem√°s.
    - Elimina espacios.
    """
    if s is None:
        return None
    s = str(s).strip().replace(',', '.')
    # Si no hay varios puntos, devolver tal cual
    if s.count('.') <= 1:
        return s
    # Mantener solo el primer punto
    first, *rest = s.split('.')
    return first + '.' + ''.join(rest)

def _to_float_or_nan(x):
    try:
        return float(x)
    except:
        return float('nan')

# 1) Crear columnas normalizadas (no pisamos las originales)
df_nuevo['latitud_norm']  = df_nuevo['latitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)
df_nuevo['longitud_norm'] = df_nuevo['longitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)

# 2) Validar rangos geogr√°ficos
mask_lat_ok = df_nuevo['latitud_norm'].between(-90, 90)
mask_lon_ok = df_nuevo['longitud_norm'].between(-180, 180)
mask_coords_ok = mask_lat_ok & mask_lon_ok

# 3) Filtrar clorofila en 0‚Äì100 ¬µg/L y contar con coords v√°lidas
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

df_rango = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
].copy()

df_rango_coords_ok = df_rango[mask_coords_ok.reindex(df_rango.index, fill_value=False)]

print(f"Total de datos en rango 0‚Äì100 ¬µg/L: {len(df_rango)}")
print(f"Con coordenadas v√°lidas tras normalizar: {len(df_rango_coords_ok)}")

# 4) Ver algunos ejemplos antes/despu√©s
ej = df_rango_coords_ok[['latitud','latitud_norm','longitud','longitud_norm']].head(10).reset_index(drop=True)
print("\nEjemplos de normalizaci√≥n de coordenadas:")
print(ej)
salida = "clorofila_coords_ok.csv"
df_rango_coords_ok.to_csv(salida, index=False, encoding="utf-8")

print(f"Archivo exportado correctamente: {salida}")

"""Se realiz√≥ una normalizaci√≥n de las coordenadas geogr√°ficas para asegurar que los valores de latitud y longitud fueran interpretables como n√∫meros decimales v√°lidos. Originalmente, los datos conten√≠an puntos decimales mal posicionados o formatos incorrectos (por ejemplo, -34.662.789), lo cual imped√≠a su correcto uso en an√°lisis espaciales o visualizaci√≥n en mapas.Luego de aplicar la transformaci√≥n, se conservaron 215 registros dentro del rango v√°lido de clorofila-a (0 a 100‚ÄØ¬µg/L). De esos, 89 ten√≠an coordenadas que pudieron ser normalizadas correctamente, y por tanto quedaron aptas para ser representadas espacialmente.

Agrego la fecha
"""

import pandas as pd
import re
from datetime import datetime

def _fix_decimal_dots(s: str) -> str:
    """Normaliza separadores decimales en coordenadas."""
    if pd.isna(s):
        return None
    s = str(s).strip().replace(',', '.')
    if s.count('.') <= 1:
        return s
    first, *rest = s.split('.')
    return first + '.' + ''.join(rest)

def _to_float_or_nan(x):
    try:
        return float(x)
    except:
        return float('nan')

def _normalize_fecha(col_fecha: pd.Series, col_anio=None, col_mes=None, col_dia=None) -> pd.Series:
    """
    Devuelve fechas normalizadas 'YYYY-MM-DD'.
    - Intenta parseo directo con varios formatos (dayfirst=True).
    - Si falla y hay a√±o/mes/d√≠a, arma la fecha desde esas columnas.
    """
    f = None
    if col_fecha is not None and col_fecha.name in col_fecha.index.to_series().index or True:
        # 1) Intento directo con to_datetime (robusto)
        s = col_fecha.astype('string').fillna('').str.strip()
        # Reparaciones r√°pidas: reemplazar puntos por guiones en fechas tipo 2021.07.03
        s = s.str.replace(r'[\u200b]', '', regex=True)  # zero-width
        s_try = s.str.replace(r'\.', '-', regex=True).str.replace(r'/', '-', regex=True)

        f = pd.to_datetime(s_try, errors='coerce', dayfirst=True, utc=False)

        # 2) Si siguen NaT y tengo Y/M/D, armo
        if (f.isna().any()) and (col_anio is not None and col_mes is not None and col_dia is not None):
            y = pd.to_numeric(col_anio, errors='coerce')
            m = pd.to_numeric(col_mes, errors='coerce')
            d = pd.to_numeric(col_dia, errors='coerce')
            mask_build = f.isna() & y.notna() & m.notna() & d.notna()
            f.loc[mask_build] = pd.to_datetime(
                dict(year=y[mask_build].astype(int),
                     month=m[mask_build].astype(int),
                     day=d[mask_build].astype(int)),
                errors='coerce'
            )
    else:
        # No hay columna fecha: intentar construir desde Y/M/D si existen
        if (col_anio is not None and col_mes is not None and col_dia is not None):
            y = pd.to_numeric(col_anio, errors='coerce')
            m = pd.to_numeric(col_mes, errors='coerce')
            d = pd.to_numeric(col_dia, errors='coerce')
            f = pd.to_datetime(dict(year=y, month=m, day=d), errors='coerce')
        else:
            f = pd.Series(pd.NaT, index=col_fecha.index if col_fecha is not None else None)

    # Normalizar a string ISO
    fecha_norm = f.dt.strftime('%Y-%m-%d')
    return fecha_norm

# --- Normalizar coordenadas ---
df_nuevo['latitud_norm']  = df_nuevo['latitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)
df_nuevo['longitud_norm'] = df_nuevo['longitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)

# --- Validar rangos geogr√°ficos ---
mask_lat_ok = df_nuevo['latitud_norm'].between(-90, 90)
mask_lon_ok = df_nuevo['longitud_norm'].between(-180, 180)
mask_coords_ok = mask_lat_ok & mask_lon_ok

# --- Asegurar clorofila num√©rica ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Normalizar FECHA (usa 'fecha' si existe; si no, intenta 'a√±o','mes','dia'/'d√≠a') ---
col_mes = df_nuevo['mes'] if 'mes' in df_nuevo.columns else (df_nuevo['Mes'] if 'Mes' in df_nuevo.columns else None)
col_dia = (df_nuevo['d√≠a'] if 'd√≠a' in df_nuevo.columns else
           df_nuevo['dia'] if 'dia' in df_nuevo.columns else
           df_nuevo['Dia'] if 'Dia' in df_nuevo.columns else None)

df_nuevo['fecha_norm'] = _normalize_fecha(
    df_nuevo['fecha'] if 'fecha' in df_nuevo.columns else pd.Series([None]*len(df_nuevo)),
    col_anio=df_nuevo['a√±o'] if 'a√±o' in df_nuevo.columns else (df_nuevo['anio'] if 'anio' in df_nuevo.columns else None),
    col_mes=col_mes,
    col_dia=col_dia
)

# --- Filtrar rango de clorofila (0‚Äì100 ¬µg/L) ---
df_rango = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
].copy()

# --- Mantener solo registros con coordenadas v√°lidas ---
dataset_muestra_coordenadas = df_rango[mask_coords_ok.reindex(df_rango.index, fill_value=False)].copy()

# --- Reemplazar columnas originales por las normalizadas ---
dataset_muestra_coordenadas['latitud']  = dataset_muestra_coordenadas['latitud_norm']
dataset_muestra_coordenadas['longitud'] = dataset_muestra_coordenadas['longitud_norm']
dataset_muestra_coordenadas = dataset_muestra_coordenadas.drop(columns=['latitud_norm','longitud_norm'])

# --- Seleccionar columnas relevantes (incluye fecha normalizada) ---
cols_existentes = [c for c in ['clorofila_a_ug_l','campa√±a','a√±o','fecha_norm','latitud','longitud'] if c in dataset_muestra_coordenadas.columns]
dataset_muestra_coordenadas = dataset_muestra_coordenadas[cols_existentes]

# --- Verificar ---
print(f"üìä Nuevo dataset creado: {len(dataset_muestra_coordenadas)} filas con clorofila + campa√±a + a√±o + fecha_norm + coords normalizadas")
print(dataset_muestra_coordenadas.head(10))

"""La incorporaci√≥n del campo fecha_norm responde a la necesidad de tener un control temporal m√°s preciso del momento en que fueron tomadas las muestras, superando la ambig√ºedad que puede implicar trabajar solo con a√±o y campa√±a (estaci√≥n). Esto permite asociar las muestras a im√°genes satelitales con fechas espec√≠ficas, evaluar tendencias temporales m√°s finas y mejorar la trazabilidad de los datos.
Se cre√≥ un nuevo dataset refinado que integra los valores de clorofila-a, la estaci√≥n del a√±o, el a√±o de muestreo, la fecha exacta de recolecci√≥n y las coordenadas geogr√°ficas normalizadas (latitud y longitud) de las muestras

Nuevo dataset

dataset_muestra_coordenadas
"""

def _fix_decimal_dots(s: str) -> str:
    """
    Normaliza separadores decimales:
    - Reemplaza comas por punto.
    - Si hay m√°s de un punto, mantiene solo el primero.
    - Elimina espacios.
    """
    if pd.isna(s):
        return None
    s = str(s).strip().replace(',', '.')
    if s.count('.') <= 1:
        return s
    first, *rest = s.split('.')
    return first + '.' + ''.join(rest)

def _to_float_or_nan(x):
    try:
        return float(x)
    except:
        return float('nan')

def _normalize_fecha(df):
    """
    Normaliza fechas:
    - Usa la columna 'fecha' si existe (parseo flexible).
    - Si no existe, intenta armar fecha desde a√±o/mes/d√≠a.
    """
    if 'fecha' in df.columns:
        fechas = pd.to_datetime(df['fecha'], errors='coerce', dayfirst=True)
    elif all(c in df.columns for c in ['a√±o','mes','dia']):
        fechas = pd.to_datetime(dict(year=df['a√±o'], month=df['mes'], day=df['dia']), errors='coerce')
    else:
        fechas = pd.NaT
    return fechas.dt.strftime('%Y-%m-%d')

# --- Normalizar coordenadas ---
df_nuevo['latitud_norm']  = df_nuevo['latitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)
df_nuevo['longitud_norm'] = df_nuevo['longitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)

# --- Validar rangos geogr√°ficos ---
mask_lat_ok = df_nuevo['latitud_norm'].between(-90, 90)
mask_lon_ok = df_nuevo['longitud_norm'].between(-180, 180)
mask_coords_ok = mask_lat_ok & mask_lon_ok

# --- Asegurar clorofila num√©rica ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Agregar columna de fecha normalizada ---
df_nuevo['fecha_norm'] = _normalize_fecha(df_nuevo)

# --- Filtrar rango de clorofila (0‚Äì100 ¬µg/L) ---
df_rango = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
].copy()

# --- Mantener solo registros con coordenadas v√°lidas ---
dataset_muestra_coordenadas = df_rango[mask_coords_ok.reindex(df_rango.index, fill_value=False)].copy()

# --- Reemplazar columnas originales por las normalizadas ---
dataset_muestra_coordenadas['latitud']  = dataset_muestra_coordenadas['latitud_norm']
dataset_muestra_coordenadas['longitud'] = dataset_muestra_coordenadas['longitud_norm']
dataset_muestra_coordenadas = dataset_muestra_coordenadas.drop(columns=['latitud_norm','longitud_norm'])

# --- Seleccionar columnas relevantes ---
dataset_muestra_coordenadas = dataset_muestra_coordenadas[
    ['clorofila_a_ug_l', 'campa√±a', 'a√±o', 'fecha_norm', 'latitud', 'longitud']
]

# --- Verificar ---
print(f" Nuevo dataset creado: {len(dataset_muestra_coordenadas)} filas con clorofila + campa√±a + a√±o + fecha_norm + coords normalizadas")
print(dataset_muestra_coordenadas.head(10))

"""Maximo y minimo de clorofila"""

# --- Imprimir valores extremos de clorofila ---
if not dataset_muestra_coordenadas['clorofila_a_ug_l'].empty:
    min_val = dataset_muestra_coordenadas['clorofila_a_ug_l'].min()
    max_val = dataset_muestra_coordenadas['clorofila_a_ug_l'].max()
    print(f"\n Valor m√≠nimo de clorofila-a: {min_val:.5f} ¬µg/L")
    print(f" Valor m√°ximo de clorofila-a: {max_val:.5f} ¬µg/L")
else:
    print("\n No hay datos de clorofila v√°lidos para calcular extremos.")

"""####  Se analiza la cantidad de valores entre 0 y 1

La elecci√≥n del rango entre 0 y 1 ¬µg/L de clorofila-a responde a criterios ecol√≥gicos y metodol√≥gicos asociados a la caracterizaci√≥n de la calidad del agua y la productividad biol√≥gica en cuerpos de agua. Este intervalo corresponde a niveles oligotr√≥ficos, es decir, condiciones de muy baja concentraci√≥n de fitoplancton y alta transparencia del agua, t√≠picas de ambientes limpios o con escasa productividad biol√≥gica. Trabajar en este rango permite detectar variaciones sutiles en la biomasa algal, especialmente en contextos donde los valores son muy bajos, como se observa en las campa√±as del a√±o 2021.Desde el punto de vista limnol√≥gico, los niveles de clorofila-a inferiores a 1 ¬µg/L reflejan un sistema con bajos aportes de nutrientes o con condiciones f√≠sico-qu√≠micas que limitan el desarrollo del fitoplancton, como pueden ser la baja temperatura, la alta turbidez no algal o la influencia de masas de agua m√°s limpias. Por lo tanto, al centrar el an√°lisis en este rango, se pueden identificar de forma precisa los umbrales a partir de los cuales comienza a aumentar la actividad biol√≥gica en el sistema, lo que es fundamental para estudios de evoluci√≥n temporal, detecci√≥n de cambios interanuales o calibraci√≥n de modelos satelitales sensibles a bajas concentraciones de clorofila.
Cuando se trabaja con √≠ndices satelitales como el NDCI, la sensibilidad del modelo en rangos bajos puede verse afectada por saturaci√≥n espectral o por ruido radiom√©trico. En consecuencia, disponer de datos in situ confiables dentro del rango 0‚Äì1 ¬µg/L es esencial para validar y ajustar modelos de estimaci√≥n remota en condiciones de baja biomasa. Este enfoque permite establecer una l√≠nea de base robusta para futuras comparaciones, detectar condiciones an√≥malas y caracterizar las transiciones hacia estados m√°s productivos del ecosistema.
"""

# Aseguramos que la columna es num√©rica
dataset_muestra_coordenadas['clorofila_a_ug_l'] = pd.to_numeric(
    dataset_muestra_coordenadas['clorofila_a_ug_l'], errors='coerce'
)

# Filtrar rango (0‚Äì1 ¬µg/L, excluyendo ceros exactos si no los quer√©s)
df_0_1 = dataset_muestra_coordenadas[
    (dataset_muestra_coordenadas['clorofila_a_ug_l'] > 0) &
    (dataset_muestra_coordenadas['clorofila_a_ug_l'] <= 1)
]

# Cantidad total
print(f" Cantidad de valores entre 0 y 1 ¬µg/L: {len(df_0_1)}")

# Mostrar ejemplos
print(df_0_1.head(10))

"""Etadistica de los valores del rango 0-1"""

# Asegurar que la columna sea num√©rica
dataset_muestra_coordenadas['clorofila_a_ug_l'] = pd.to_numeric(
    dataset_muestra_coordenadas['clorofila_a_ug_l'], errors='coerce'
)

# Filtrar rango (0‚Äì1 ¬µg/L, excluyendo ceros si no quer√©s)
df_0_1 = dataset_muestra_coordenadas[
    (dataset_muestra_coordenadas['clorofila_a_ug_l'] > 0) &
    (dataset_muestra_coordenadas['clorofila_a_ug_l'] <= 1)
]

# Cantidad total
print(f"üîé Cantidad de valores entre 0 y 1 ¬µg/L: {len(df_0_1)}")

# Estad√≠sticas descriptivas
estadisticas = df_0_1['clorofila_a_ug_l'].agg(
    media='mean',
    m√≠nimo='min',
    m√°ximo='max',
    mediana='median',
    desv√≠o_std='std'
)

print("\n Estad√≠sticas de valores 0‚Äì1 ¬µg/L:")
print(estadisticas)

# Mostrar primeras filas
print("\n Ejemplos:")
print(df_0_1[['clorofila_a_ug_l','campa√±a','a√±o','fecha_norm','latitud','longitud']].head(10))

"""nuevo dataset"""

# Asegurar que la columna sea num√©rica
dataset_muestra_coordenadas['clorofila_a_ug_l'] = pd.to_numeric(
    dataset_muestra_coordenadas['clorofila_a_ug_l'], errors='coerce'
)

# Crear nuevo DataFrame con rango 0‚Äì1 ¬µg/L
rango_0_1 = dataset_muestra_coordenadas[
    (dataset_muestra_coordenadas['clorofila_a_ug_l'] > 0) &
    (dataset_muestra_coordenadas['clorofila_a_ug_l'] <= 1)
].copy()

# Verificar
print(f"‚úÖ Nuevo DataFrame creado: {len(rango_0_1)} filas entre 0 y 1 ¬µg/L")
print(rango_0_1.head(10))

"""Mapa de los puntos rango 0-1"""

import numpy as np
import matplotlib.pyplot as plt

# --- Datos de partida (usa tu df_nuevo) ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Filtrar 0‚Äì100 ¬µg/L ---
df_0_100 = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
].copy()

# Solo con coords v√°lidas (opcional, √∫til si luego quer√©s mapear)
df_0_100_coords = df_0_100[df_0_100['latitud'].notna() & df_0_100['longitud'].notna()].copy()

# Subconjunto 0‚Äì1 ¬µg/L (dentro de 0‚Äì100) con coords
df_0_1_coords = df_0_100_coords[
    (df_0_100_coords['clorofila_a_ug_l'] > 0) &
    (df_0_1_coords['clorofila_a_ug_l'] <= 1)
] if not df_0_100_coords.empty else df_0_100_coords.iloc[0:0].copy()

# Conteos
tot_0_100   = len(df_0_100)
with_coords = len(df_0_100_coords)
n_0_1       = len(df_0_1_coords)
n_1_100     = with_coords - n_0_1

print(f"Total 0‚Äì100 ¬µg/L: {tot_0_100}")
print(f"Con coords: {with_coords}  |  0‚Äì1 ¬µg/L: {n_0_1}  |  1‚Äì100 ¬µg/L: {n_1_100}")

# --- Gr√°fico de tiro al blanco (0‚Äì100 y 0‚Äì1) ---
fig, ax = plt.subplots(figsize=(6,6))

# Anillo total 0‚Äì100 (radio 2.5 arbitrario) y c√≠rculo 0‚Äì1 (radio 1.0)
outer = plt.Circle((0,0), 2.5, color='lightblue',  alpha=0.25, label='0‚Äì100 ¬µg/L')
inner = plt.Circle((0,0), 1.0, color='lightgreen', alpha=0.35, label='0‚Äì1 ¬µg/L')
rim   = plt.Circle((0,0), 2.5, fill=False, edgecolor='black', lw=1)

for c in [outer, inner, rim]:
    ax.add_artist(c)

# Distribuci√≥n de puntos por anillo (√°ngulo aleatorio para visualizaci√≥n)
rng = np.random.default_rng(42)

def plot_ring(df_points, radius, label, edge=None):
    if len(df_points) == 0:
        return
    theta = rng.uniform(0, 2*np.pi, size=len(df_points))
    r = radius + rng.uniform(-0.10, 0.10, size=len(df_points))  # leve jitter radial
    x = r * np.cos(theta); y = r * np.sin(theta)
    ax.scatter(x, y, s=36, alpha=0.85, edgecolor=edge, label=label)

# Puntos 0‚Äì1 en el c√≠rculo interno
plot_ring(df_0_1_coords, radius=0.75, label='0‚Äì1 ¬µg/L (coords)', edge='black')

# Puntos 0‚Äì100 en el anillo externo
df_1_100_coords = df_0_100_coords.loc[~df_0_100_coords.index.isin(df_0_1_coords.index)]
plot_ring(df_1_100_coords, radius=1.8, label='0‚Äì100 ¬µg/L (coords)')

# Est√©tica
ax.set_xlim(-3, 3); ax.set_ylim(-3, 3)
ax.set_aspect('equal', 'box'); ax.set_xticks([]); ax.set_yticks([])
ax.set_title(
    f"Tiro al blanco ‚Äì Clorofila\n"
    f"0‚Äì100 ¬µg/L: {tot_0_100} | Con coords: {with_coords} | 0‚Äì1 ¬µg/L: {n_0_1}"
)

# Leyenda sin duplicados
handles, labels = ax.get_legend_handles_labels()
uniq = dict(zip(labels, handles))
ax.legend(uniq.values(), uniq.keys(), loc='upper right')

plt.tight_layout()
plt.show()

# --- Asegurar clorofila num√©rica ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Filtrar 0‚Äì100 ¬µg/L ---
df_rango = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
].copy()

# --- Con coordenadas v√°lidas ---
df_rango_coords = df_rango[
    df_rango['latitud'].notna() & df_rango['longitud'].notna()
].copy()

# --- Subconjunto 0‚Äì1 ¬µg/L dentro de los que tienen coords ---
df_0_1_coords = df_rango_coords[
    (df_rango_coords['clorofila_a_ug_l'] > 0) &
    (df_rango_coords['clorofila_a_ug_l'] <= 1)
].copy()

# --- Conteos ---
total_en_rango = len(df_rango)
con_coords = len(df_rango_coords)
n_0_1 = len(df_0_1_coords)
n_1_100 = con_coords - n_0_1

print(f" Total en 0‚Äì100 ¬µg/L: {total_en_rango}")
print(f" Con coordenadas v√°lidas: {con_coords}")
print(f"   ‚îú‚îÄ 0‚Äì1 ¬µg/L (con coords): {n_0_1}")
print(f"   ‚îî‚îÄ 1‚Äì100 ¬µg/L (con coords): {n_1_100}")

# --- Gr√°fico de tiro al blanco ---
fig, ax = plt.subplots(figsize=(6,6))

# C√≠rculo central (0‚Äì1 ¬µg/L) y anillo (1‚Äì100 ¬µg/L)
inner = plt.Circle((0,0), 1.0, fill=True, color='lightgreen', alpha=0.35, label='0‚Äì1 ¬µg/L')
outer = plt.Circle((0,0), 2.5, fill=True, color='lightblue',  alpha=0.25, label='1‚Äì100 ¬µg/L')
rim   = plt.Circle((0,0), 2.5, fill=False, edgecolor='black', lw=1)

ax.add_artist(outer)
ax.add_artist(inner)
ax.add_artist(rim)

# Distribuir puntos con coordenadas en los anillos
# (√°ngulo aleatorio; radio fijo por rango para el efecto "blanco")
rng = np.random.default_rng(42)

def plot_ring_points(df_points, radius, marker, label, edge=None, alpha=0.85):
    if len(df_points) == 0:
        return
    theta = rng.uniform(0, 2*np.pi, size=len(df_points))
    # peque√±o jitter radial para no superponer exacto
    r = radius + rng.uniform(-0.10, 0.10, size=len(df_points))
    x = r * np.cos(theta)
    y = r * np.sin(theta)
    ax.scatter(x, y, s=36, marker=marker, alpha=alpha, edgecolor=edge, label=label)

# Puntos 0‚Äì1 ¬µg/L (centro)
plot_ring_points(df_0_1_coords, radius=0.6, marker='o', label='0‚Äì1 ¬µg/L (coords)', edge='black')

# Puntos 1‚Äì100 ¬µg/L (anillo)
df_1_100_coords = df_rango_coords.loc[~df_rango_coords.index.isin(df_0_1_coords.index)]
plot_ring_points(df_1_100_coords, radius=1.8, marker='o', label='1‚Äì100 ¬µg/L (coords)')

# Est√©tica del blanco
ax.set_xlim(-3, 3)
ax.set_ylim(-3, 3)
ax.set_aspect('equal', 'box')
ax.set_xticks([])
ax.set_yticks([])

ax.set_title(
    f"Tiro al blanco 0‚Äì100 ¬µg/L\n"
    f"Total en rango: {total_en_rango} | Con coords: {con_coords} | 0‚Äì1 ¬µg/L (coords): {n_0_1}"
)

# Leyenda ordenada y sin duplicados
handles, labels = ax.get_legend_handles_labels()
uniq = dict(zip(labels, handles))
ax.legend(uniq.values(), uniq.keys(), loc='upper right')

plt.tight_layout()
plt.show()

# --- Asegurar columnas num√©ricas ---
rango_0_1['latitud'] = pd.to_numeric(rango_0_1['latitud'], errors='coerce')
rango_0_1['longitud'] = pd.to_numeric(rango_0_1['longitud'], errors='coerce')
rango_0_1 = rango_0_1.dropna(subset=['latitud', 'longitud', 'clorofila_a_ug_l'])

# --- Coordenadas de Buenos Aires (centro de la vista) ---
centro_bsas = [-34.6037, -58.3816]  # CABA

# --- Crear mapa centrado en Buenos Aires ---
m = folium.Map(
    location=centro_bsas,
    zoom_start=9,
    tiles=None  # Fondo personalizado
)

# --- Fondo satelital de Google ---
folium.TileLayer(
    tiles='http://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',
    attr='Google Satellite',
    name='Google Satellite',
    overlay=False,
    control=True
).add_to(m)

# --- Agregar puntos rojos con popup ---
for _, row in rango_0_1.iterrows():
    folium.CircleMarker(
        location=[row['latitud'], row['longitud']],
        radius=5,
        color='red',
        fill=True,
        fill_color='red',
        fill_opacity=0.9,
        popup=folium.Popup(
            f"<b>Clorofila-a:</b> {row['clorofila_a_ug_l']} ¬µg/L<br><b>Fecha:</b> {row['fecha_norm']}",
            max_width=250
        )
    ).add_to(m)

# --- Leyenda HTML ---
legend_html = """
<div style="
    position: fixed;
    bottom: 50px;
    left: 50px;
    z-index: 9999;
    background-color: white;
    padding: 10px;
    border: 2px solid #ccc;
    border-radius: 5px;
    font-size: 14px;
">
    <b>Leyenda</b><br>
    üî¥ Punto rojo: ubicaci√≥n de muestra<br>
</div>
"""
m.get_root().html.add_child(folium.Element(legend_html))

# --- T√≠tulo HTML ---
title_html = '''
    <h3 align="center" style="font-size:18px; margin-top:10px">
        Valores en el rango 0‚Äì1 ¬µg/L de clorofila-a
    </h3>
'''
m.get_root().html.add_child(folium.Element(title_html))

# --- Control de capas ---
folium.LayerControl().add_to(m)

# --- Mostrar mapa interactivo ---
m

"""###Agrego variables turbidez y temperatura

"""

# Columnas de inter√©s
cols = ["fecha", "latitud", "longitud", "turbiedad_ntu", "tem_agua"]
dfx = df[cols].copy()

# A num√©rico turbidez/temperatura
dfx["turbiedad_ntu"] = pd.to_numeric(dfx["turbiedad_ntu"], errors="coerce")
dfx["tem_agua"] = pd.to_numeric(dfx["tem_agua"], errors="coerce")

def parse_coord(x: str, kind: str):
    """
    Normaliza coordenadas con formatos como:
    - '-34.662.789' -> -34.662789
    - '-34.63'      -> -34.63
    - '-58.328.339' -> -58.328339
    No redondea. Devuelve float o None si no es v√°lido.
    """
    if x is None:
        return None
    s = str(x).strip()
    if s.lower() in ("", "na", "nan", "none"):
        return None

    # unificar coma decimal a punto
    s = s.replace(",", ".")

    # si ya parece n√∫mero simple con un solo punto decimal, usar directo
    if s.count(".") == 1 and all(c in "-.0123456789" for c in s):
        try:
            val = float(s)
            return val
        except:
            pass

    # de lo contrario, quitar todo lo que no sea d√≠gito para reconstruir
    sign = -1 if s.startswith("-") else 1
    digits = re.sub(r"\D", "", s)  # solo d√≠gitos
    if len(digits) < 3:
        return None

    # En Argentina esper√°s ~ -34 (lat) y ~ -58 (lon) => 2 d√≠gitos antes del decimal
    int_len = 2
    try:
        val = float(digits[:int_len] + "." + digits[int_len:])
        val *= sign
    except:
        return None

    # Chequeo de rango geogr√°fico b√°sico
    if kind == "lat":
        if not (-90 <= val <= 90):
            return None
    elif kind == "lon":
        if not (-180 <= val <= 180):
            return None

    return val

# Normalizar lat/lon sin redondear
dfx.loc[:, "latitud"]  = dfx["latitud"].apply(lambda v: parse_coord(v, "lat"))
dfx.loc[:, "longitud"] = dfx["longitud"].apply(lambda v: parse_coord(v, "lon"))

# Filtrar filas completas (todas las columnas con dato v√°lido)
df_validos = dfx.dropna(subset=["fecha", "latitud", "longitud", "turbiedad_ntu", "tem_agua"]).copy()

print("Registros v√°lidos y completos:", len(df_validos))
print(df_validos.head(10))

"""Union de clorofila mas turbidez y temperatura del agua"""

# ---------- Helpers (tus funciones) ----------
def _fix_decimal_dots(s: str) -> str:
    if pd.isna(s):
        return None
    s = str(s).strip().replace(',', '.')
    if s.count('.') <= 1:
        return s
    first, *rest = s.split('.')
    return first + '.' + ''.join(rest)

def _to_float_or_nan(x):
    try:
        return float(x)
    except:
        return float('nan')

def _normalize_fecha(df):
    if 'fecha' in df.columns:
        fechas = pd.to_datetime(df['fecha'], errors='coerce', dayfirst=True)
    elif all(c in df.columns for c in ['a√±o','mes','dia']):
        fechas = pd.to_datetime(dict(year=df['a√±o'], month=df['mes'], day=df['dia']), errors='coerce')
    else:
        fechas = pd.NaT
    return fechas.dt.strftime('%Y-%m-%d')

# =========================================================
# A) Dataset de clorofila (df_nuevo) -> normalizado (como ya ten√≠as)
#     Espera columnas: 'latitud','longitud','clorofila_a_ug_l','campa√±a','a√±o' (+ 'fecha' o a√±o/mes/d√≠a)
# =========================================================
dfA = df_nuevo.copy()

# Normalizar coords SIN redondear
dfA['latitud_norm']  = dfA['latitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)
dfA['longitud_norm'] = dfA['longitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)

# Validar rango geogr√°fico
maskA = dfA['latitud_norm'].between(-90, 90) & dfA['longitud_norm'].between(-180, 180)

# Clorofila a num√©rico
dfA['clorofila_a_ug_l'] = pd.to_numeric(dfA['clorofila_a_ug_l'], errors='coerce')

# Fecha normalizada
dfA['fecha_norm'] = _normalize_fecha(dfA)

# Filtro de filas v√°lidas
dfA = dfA[
    maskA &
    dfA['clorofila_a_ug_l'].notna() &
    (dfA['clorofila_a_ug_l'] > 0) &
    (dfA['clorofila_a_ug_l'] <= 100) &
    dfA['fecha_norm'].notna()
].copy()

# Clave EXACTA (sin redondeo)
dfA['__key__'] = list(zip(dfA['fecha_norm'], dfA['latitud_norm'], dfA['longitud_norm']))

# Reducir a columnas relevantes
dfA = dfA[['__key__','clorofila_a_ug_l','campa√±a','a√±o','fecha_norm','latitud_norm','longitud_norm']]

# =========================================================
# B) Dataset turbidez/temperatura (Conexiones_Transparentes)
# =========================================================
url = "https://raw.githubusercontent.com/MaricelSantos/Mentoria--Diplodatos-2025/main/Conexiones_Transparentes.csv"
dfB = pd.read_csv(url)[['fecha','latitud','longitud','turbiedad_ntu','tem_agua']].copy()

# Normalizaci√≥n consistente (SIN redondear)
dfB['latitud_norm']  = dfB['latitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)
dfB['longitud_norm'] = dfB['longitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)

maskB = dfB['latitud_norm'].between(-90, 90) & dfB['longitud_norm'].between(-180, 180)

dfB['turbiedad_ntu'] = pd.to_numeric(dfB['turbiedad_ntu'], errors='coerce')
dfB['tem_agua']      = pd.to_numeric(dfB['tem_agua'], errors='coerce')
dfB['fecha_norm']    = _normalize_fecha(dfB)

dfB = dfB[
    maskB &
    dfB['turbiedad_ntu'].notna() &
    dfB['tem_agua'].notna() &
    dfB['fecha_norm'].notna()
].copy()

# Clave EXACTA (sin redondeo)
dfB['__key__'] = list(zip(dfB['fecha_norm'], dfB['latitud_norm'], dfB['longitud_norm']))

# ‚¨áÔ∏è Traer SOLO las variables nuevas de B para evitar duplicados de fecha/coords
dfB = dfB[['__key__', 'turbiedad_ntu', 'tem_agua']]

# =========================================================
# JOIN EXACTO (fecha_norm + latitud_norm + longitud_norm)
# =========================================================
merged_exacto = pd.merge(
    dfA, dfB,
    on='__key__',
    how='inner'
).drop(columns=['__key__'])

print(f"üîó Join EXACTO (fecha + lat + lon): {len(merged_exacto)} filas")
print(merged_exacto.head(10))

# --- Dataset final ya unido ---
dataset_final = merged_exacto.copy()

"""Para enriquecer el an√°lisis de clorofila con variables ambientales relevantes, se decidi√≥ incorporar informaci√≥n sobre la turbidez del agua y la temperatura, provenientes de un  conjunto de datos que contiene mediciones f√≠sico-qu√≠micas en distintas fechas y campa√±as. El objetivo fue vincular estos registros auxiliares con los datos de concentraci√≥n de clorofila ya normalizados y georreferenciados, tomando como criterio de emparejamiento las variables temporales coincidentes: fecha, campa√±a y a√±o.
Esta fusi√≥n permiti√≥ mantener el total esperado de observaciones (80 registros), asegurando as√≠ la integridad del dataset y preparando la base para su posterior uso en modelos predictivos que consideren condiciones ambientales en el momento de la toma de muestra.

#**Concentraci√≥n de clorofila-a - Sentinel-2 por estaci√≥n**
"""

# Instalar y cargar Earth Engine
!pip install earthengine-api folium geemap --quiet
# Autenticaci√≥n
ee.Authenticate()
ee.Initialize(project='mentorias-463215')

# Funci√≥n para obtener la mejor imagen Sentinel-2 por estaci√≥n
def obtener_imagen(fecha_ini, fecha_fin):
    coleccion = ee.ImageCollection('COPERNICUS/S2_SR') \
        .filterBounds(zona) \
        .filterDate(fecha_ini, fecha_fin) \
        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \
        .sort('CLOUDY_PIXEL_PERCENTAGE') \
        .first() \
        .clip(zona)
    return coleccion

"""extraer la zona de inter√©s, espacio que ocupa"""

#  Definir zona de an√°lisis (1 km alrededor del punto costero)
# Geometr√≠a: franja costera del AMBA
zona = ee.Geometry.Rectangle([
    -58.60,  # Longitud oeste
    -34.90,  # Latitud sur
    -57.85,  # Longitud este
    -34.40   # Latitud norte
])

"""utilidades/mascaras"""

OPTICAL_BANDS = ['B2','B3','B4','B5','B6','B7','B8','B8A','B11','B12']

def scale_optical_1e4(img):
    """Escala bandas √≥pticas (DN -> reflectancia * 1e-4)"""
    return img.addBands(img.select(OPTICAL_BANDS).multiply(0.0001), overwrite=True)

def add_s2cloudless_prob(s2_col, start, end, roi):
    """Une colecci√≥n S2 (L1C o L2A) con COPERNICUS/S2_CLOUD_PROBABILITY"""
    s2c = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')
           .filterBounds(roi).filterDate(start, end))
    joined = ee.ImageCollection(ee.Join.saveFirst('s2c').apply(
        primary = s2_col,
        secondary = s2c,
        condition = ee.Filter.equals(leftField='system:index', rightField='system:index')
    ))
    def _addprob(img):
        cp = ee.Image(img.get('s2c')).select('probability')
        return img.addBands(cp.rename('cloud_probability'))
    return joined.map(_addprob)

def mask_clouds_prob(img, prob_thresh=60):
    """M√°scara por prob. de nube (s2cloudless)"""
    return img.updateMask(img.select('cloud_probability').lt(prob_thresh))

def mask_scl_sr(img):
    """M√°scara extra para S2_SR usando SCL (descarta: nodata, saturado, sombra nube, nubes, cirros, nieve)"""
    scl = img.select('SCL')
    bad = scl.eq(0).Or(scl.eq(1)).Or(scl.eq(3)).Or(scl.eq(8)).Or(scl.eq(9)).Or(scl.eq(10)).Or(scl.eq(11))
    return img.updateMask(bad.Not())

""" Pipeline A: TOA (Level-1C, COPERNICUS/S2)
  - Correcci√≥n radiom√©trica (escala 1e-4)
    - Reflectancia aparente TOA (propia del L1C)
  - Nubes con s2cloudless
"""

def obtener_imagen_toa(fecha_ini, fecha_fin, roi, cloud_perc=30, prob_thresh=60):
    col = (ee.ImageCollection('COPERNICUS/S2_HARMONIZED')
           .filterBounds(roi)
           .filterDate(fecha_ini, fecha_fin)
           .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', cloud_perc))
           .map(scale_optical_1e4))
    col = add_s2cloudless_prob(col, fecha_ini, fecha_fin, roi).map(lambda i: mask_clouds_prob(i, prob_thresh))
    # Compuesto estacional (recomendado para m√©tricas zonales)
    return col.median().clip(roi)

"""Pipeline B: Superficie (Level-2A, COPERNICUS/S2_SR)
   - Ya trae correcci√≥n atmosf√©rica (Sen2Cor)
  - Escalado radiom√©trico 1e-4
   - Nubes con s2cloudless + m√°scara SCL
"""

def obtener_imagen_sr(fecha_ini, fecha_fin, roi, cloud_perc=30, prob_thresh=60):
    col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
           .filterBounds(roi)
           .filterDate(fecha_ini, fecha_fin)
           .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', cloud_perc))
           .map(scale_optical_1e4))
    col = add_s2cloudless_prob(col, fecha_ini, fecha_fin, roi) \
            .map(lambda i: mask_clouds_prob(i, prob_thresh)) \
            .map(mask_scl_sr)
    return col.median().clip(roi)

"""√çndice de clorofila"""

def calcular_ndci(img):  # (B5 - B4) / (B5 + B4)
    return img.normalizedDifference(['B5','B4']).rename('NDCI')

"""Estaciones"""

# Estaciones: oto√±o, invierno, primavera y verano (Argentina, hemisferio sur)
fechas_estaciones = {
    # 2021
    'Oto√±o 2021':    ('2021-03-21', '2021-06-20'),
    'Invierno 2021': ('2021-06-21', '2021-09-21'),
    'Primavera 2021':('2021-09-22', '2021-12-20'),
    'Verano 2021':   ('2021-12-21', '2022-03-20'),

    # 2022
    'Oto√±o 2022':    ('2022-03-21', '2022-06-20'),
    'Invierno 2022': ('2022-06-21', '2022-09-21'),
    'Primavera 2022':('2022-09-22', '2022-12-20'),
    'Verano 2022':   ('2022-12-21', '2023-03-20'),

    # 2023
    'Oto√±o 2023':    ('2023-03-21', '2023-06-20'),
    'Invierno 2023': ('2023-06-21', '2023-09-21'),
    'Primavera 2023':('2023-09-22', '2023-12-20'),
    'Verano 2023':   ('2023-12-21', '2024-03-20'),
}

# --- Par√°metros ---
SCALE = 30
MAX_PIX = 1e9
GRAFICAR = True

# Elegir autom√°ticamente la funci√≥n disponible: SR (preferida) o la simple
try:
    _ = obtener_imagen_sr  # si existe, la usamos
    get_img = lambda ini, fin: obtener_imagen_sr(ini, fin, zona)
except NameError:
    get_img = lambda ini, fin: obtener_imagen(ini, fin)

# Reducer SOLO con: media, mediana, min, max
reducer = (ee.Reducer.mean()
           .combine(ee.Reducer.median(), '', True)
           .combine(ee.Reducer.min(), '', True)
           .combine(ee.Reducer.max(), '', True))

def pick(d, keys):
    for k in keys:
        if k in d and d[k] is not None:
            return d[k]
    return None

rows = []
for nombre, (ini, fin) in fechas_estaciones.items():
    # 1) Imagen de la campa√±a sobre tu zona
    img = get_img(ini, fin)

    # 2) NDCI = (B5 - B4) / (B5 + B4)
    ndci = calcular_ndci(img)

# NDWI = (B3 - B8) / (B3 + B8)  -> agua ~ NDWI > 0
    ndwi = img.normalizedDifference(['B3','B8']).rename('NDWI')
    water = ndwi.gt(0.05)

    # Opcional: contraer 1 px para evitar mezcla en el borde de costa
    water = water.focal_min(1)

    # Aplicar la m√°scara de agua al NDCI
    ndci = ndci.updateMask(water)

    # 3) Estad√≠sticas zonales (solo mean, median, min, max)
    stats = ndci.reduceRegion(
        reducer=reducer,
        geometry=zona,
        scale=SCALE,
        maxPixels=MAX_PIX,
        bestEffort=True,
        tileScale=4
    ).getInfo() or {}

    # 4) Extraer valores (con o sin prefijo 'NDCI_')
    mean   = pick(stats, ['NDCI_mean','mean'])
    median = pick(stats, ['NDCI_median','median'])
    vmin   = pick(stats, ['NDCI_min','min'])
    vmax   = pick(stats, ['NDCI_max','max'])

    # Separar campa√±a y a√±o (ej. "Invierno 2022")
    partes = nombre.split()
    camp = " ".join(partes[:-1])
    anio = int(partes[-1])

    rows.append({
        'campa√±a': camp,
        'a√±o': anio,
        'ndci_media': mean,
        'ndci_mediana': median,
        'ndci_min': vmin,
        'ndci_max': vmax,
        # opcionalmente pod√©s guardar 'campa√±a_completa': nombre
    })

# 5) DataFrame ordenado
df_ndci = pd.DataFrame(rows)
orden = ['Verano','Oto√±o','Invierno','Primavera']  # orden estacional HS
if df_ndci['campa√±a'].isin(orden).all():
    df_ndci['campa√±a'] = pd.Categorical(df_ndci['campa√±a'], categories=orden, ordered=True)
df_ndci = df_ndci.sort_values(['a√±o','campa√±a']).reset_index(drop=True)

# 6) Imprimir tabla pedida
cols = ['campa√±a','a√±o','ndci_media','ndci_mediana','ndci_min','ndci_max']
print(" Estad√≠sticas NDCI por campa√±a (escala={} m):".format(SCALE))
print(df_ndci[cols].round(4).to_string(index=False))

# 7) (Opcional) Gr√°fico: barras (media) + puntos (mediana)
if GRAFICAR:
    etiquetas = df_ndci['campa√±a'].astype(str) + " " + df_ndci['a√±o'].astype(str)
    plt.figure(figsize=(12,6))
    plt.bar(etiquetas, df_ndci['ndci_media'])
    plt.plot(etiquetas, df_ndci['ndci_mediana'], marker='o', linestyle='None', label='Mediana')
    plt.title("NDCI por campa√±a: Media (barras) y Mediana (puntos)")
    plt.xlabel("Campa√±a y a√±o")
    plt.ylabel("NDCI")
    plt.xticks(rotation=45, ha='right')
    plt.legend()
    plt.tight_layout()
    plt.show()

"""El gr√°fico  muestra la evoluci√≥n del √≠ndice NDCI (Normalized Difference Chlorophyll Index) a lo largo de diferentes campa√±as estacionales desde el verano de 2021 hasta la primavera de 2023. En este caso, las barras representan el valor promedio (media) del √≠ndice para cada estaci√≥n y a√±o, mientras que los puntos azules indican la mediana correspondiente.

La comparaci√≥n entre la media y la mediana permite observar la distribuci√≥n de los valores y detectar posibles sesgos o asimetr√≠as. Por ejemplo, cuando la mediana se encuentra muy por debajo o por encima de la media, puede indicar la presencia de valores at√≠picos que afectan el promedio. A lo largo de las campa√±as analizadas, se evidencian variaciones estacionales en el NDCI, con algunos descensos marcados (por ejemplo, en invierno de 2022 y 2023), que podr√≠an asociarse a una menor concentraci√≥n de clorofila-a y, por lo tanto, a una menor productividad fitoplanct√≥nica en esas √©pocas.

###cruzar datos coincidentes

Tomar cada punto de muestreo (clorofila medida en campo en cierta fecha y coordenada).

Buscar el valor del p√≠xel de la imagen satelital que cubre ese lugar, en la misma fecha.

Guardar en un mismo registro: clorofila (campo) ‚Äì NDCI (sat√©lite) ‚Äì fecha ‚Äì coordenadas.

As√≠ obtener una tabla con pares de valores que realmente se corresponden en tiempo y espacio.
"""

# ========================
# PAR√ÅMETROS AJUSTABLES
# ========================
D = 7         # ventana temporal ¬±D d√≠as (ej: 3, 5, 7)
SCALE = 30    # escala en metros (20 √≥ 30)
REDUCER = "mean"   # opciones: "mean", "median", "p25", "p75"

# ========================
# FUNCIONES GEE
# ========================
def mask_s2_sr(image):
    """M√°scara usando la banda SCL de Sentinel-2 L2A."""
    scl  = image.select('SCL')
    mask = (scl.neq(3)   # sombra
            .And(scl.neq(8))   # nubes
            .And(scl.neq(9))   # nubes altas
            .And(scl.neq(10))  # nubes finas
            .And(scl.neq(11))  # cirros
           )
    return image.updateMask(mask)

def add_ndci(image):
    """NDCI = (B5 - B4) / (B5 + B4)."""
    ndci = image.expression(
        '(b5 - b4) / (b5 + b4)',
        {'b5': image.select('B5'), 'b4': image.select('B4')}
    ).rename('NDCI')
    return image.addBands(ndci)

def get_reducer():
    if REDUCER == "mean":
        return ee.Reducer.mean()
    elif REDUCER == "median":
        return ee.Reducer.median()
    elif REDUCER == "p25":
        return ee.Reducer.percentile([25])
    elif REDUCER == "p75":
        return ee.Reducer.percentile([75])
    else:
        return ee.Reducer.median()

S2 = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
      .map(mask_s2_sr)
      .map(add_ndci))

# ========================
# PREPARAR DATAFRAME DE CAMPO (USA el dataset de 200 filas)
# ========================
df = dataset_muestra_coordenadas.copy()

# Asegurar tipos
df['clorofila_a_ug_l'] = pd.to_numeric(df['clorofila_a_ug_l'], errors='coerce')
df['latitud']  = pd.to_numeric(df['latitud'], errors='coerce')
df['longitud'] = pd.to_numeric(df['longitud'], errors='coerce')
df['fecha']    = pd.to_datetime(df['fecha_norm'], errors='coerce')

# Filtrar filas v√°lidas
df = df.dropna(subset=['latitud','longitud','fecha','clorofila_a_ug_l']).reset_index(drop=True)
df['start'] = (df['fecha'] - pd.Timedelta(days=D)).dt.strftime('%Y-%m-%d')
df['end']   = (df['fecha'] + pd.Timedelta(days=D)).dt.strftime('%Y-%m-%d')

print(f"üîé Filas del dataset (200 original) que se cruzan: {len(df)}")

# ========================
# SUBIR PUNTOS A GEE
# ========================
def row_to_feature(row):
    geom = ee.Geometry.Point([float(row['longitud']), float(row['latitud'])])
    return ee.Feature(geom, {
        'clorofila': float(row['clorofila_a_ug_l']),
        'campa√±a': str(row['campa√±a']) if 'campa√±a' in row and pd.notna(row['campa√±a']) else None,
        'a√±o': int(row['a√±o']) if 'a√±o' in row and pd.notna(row['a√±o']) else None,
        'fecha': row['fecha'].strftime('%Y-%m-%d'),
        'start': str(row['start']),
        'end': str(row['end'])
    })

features = [row_to_feature(r) for _, r in df.iterrows()]
fc = ee.FeatureCollection(features)

# ========================
# MUESTREO ROBUSTO (SERVER-SIDE)
# ========================
def sample_ndci_per_feature(f):
    start = ee.Date(f.get('start'))
    end   = ee.Date(f.get('end'))
    geom  = f.geometry()
    col   = S2.filterDate(start, end).filterBounds(geom)
    n     = col.size()

    reducer = get_reducer()

    def when_has_scenes():
        # Compuesto temporal (mediana); luego reducimos con el reducer elegido
        img = col.median()
        ndci_img = img.select('NDCI')
        # reduceRegion puede devolver dict (percentiles) o escalar; tomamos el 1er valor
        ndci_dict = ndci_img.unmask().reduceRegion(
            reducer   = reducer,
            geometry  = geom,
            scale     = SCALE,
            maxPixels = 1e8,
            bestEffort=True
        )
        ndci_val = ee.Dictionary(ndci_dict).values().get(0)  # server-side
        return ee.Dictionary({'ndci_val': ndci_val, 'n_scenes': n})

    def when_no_scenes():
        return ee.Dictionary({'ndci_val': None, 'n_scenes': n})

    out = ee.Algorithms.If(n.gt(0), when_has_scenes(), when_no_scenes())
    return f.set(ee.Dictionary(out))

fc_out = fc.map(sample_ndci_per_feature)

# ========================
# BAJAR RESULTADOS A PANDAS
# ========================
out = fc_out.getInfo()  # para ~200 puntos est√° ok
rows = []
for feat in out['features']:
    props = feat['properties']
    lon, lat = feat['geometry']['coordinates']
    rows.append({
        'longitud': lon,
        'latitud':  lat,
        'campa√±a':  props.get('campa√±a'),
        'a√±o':      props.get('a√±o'),
        'fecha':    props.get('fecha'),
        'clorofila_a_ug_l': props.get('clorofila'),
        'ndci_val':         props.get('ndci_val'),
        'n_scenes':         props.get('n_scenes'),
        'start':            props.get('start'),
        'end':              props.get('end'),
    })

df_cruce_200 = pd.DataFrame(rows)
df_cruce_200['ndci_val'] = pd.to_numeric(df_cruce_200['ndci_val'], errors='coerce')
df_cruce_200['n_scenes'] = pd.to_numeric(df_cruce_200['n_scenes'], errors='coerce')
df_cruce_200['fecha']    = pd.to_datetime(df_cruce_200['fecha'], errors='coerce')

print(f"\n‚öôÔ∏è Ventana ¬±{D} d√≠as | Escala {SCALE} m | Reducer = {REDUCER}")
print(df_cruce_200.head())
print("\nüìä Resumen NDCI extra√≠do (dataset 200):")
print(df_cruce_200['ndci_val'].describe())

# (Opcional) pares v√°lidos para correlaci√≥n:
df_valid_200 = df_cruce_200[(df_cruce_200['n_scenes'] >= 1) & (df_cruce_200['ndci_val'].notna())].copy()
print(f"\n‚úÖ Pares v√°lidos (>=1 escena & NDCI no nulo) en dataset 200: {len(df_valid_200)}/{len(df_cruce_200)}")

"""Comparacion datos medidos vs datos satelitales"""

# ‚îÄ‚îÄ Par√°metro opcional de rango (en ¬µg/L). Dejalo en None para usar todo el dataset.
rango = None          # ejemplos: (0, 1), (1, 10), (10, 100) o None

# ‚îÄ‚îÄ Asegurar dataset v√°lido (NDCI y clorofila no nulos) a partir del cruce de 200
df_plot = df_valid_200[['ndci_val', 'clorofila_a_ug_l']].dropna().copy()

# Filtrar por rango si se especifica
titulo_sufijo = " (todo el rango)"
if rango is not None:
    lo, hi = rango
    df_plot = df_plot[(df_plot['clorofila_a_ug_l'] >= lo) &
                      (df_plot['clorofila_a_ug_l'] <= hi)]
    titulo_sufijo = f" ({lo}‚Äì{hi} ¬µg/L)"

print(f"Registros v√°lidos para an√°lisis{titulo_sufijo}: {len(df_plot)}")

if len(df_plot) >= 3:
    x = df_plot['ndci_val'].values.reshape(-1, 1)
    y = df_plot['clorofila_a_ug_l'].values

    # Correlaciones
    r_pear, p_pear = pearsonr(x.ravel(), y)
    r_spear, p_spear = spearmanr(x.ravel(), y)
    print(f"Pearson: r={r_pear:.3f}, p={p_pear:.3g}")
    print(f"Spearman: rho={r_spear:.3f}, p={p_spear:.3g}")

    # Regresi√≥n lineal para l√≠nea de tendencia
    lr = LinearRegression().fit(x, y)
    y_hat = lr.predict(x)
    r2 = r2_score(y, y_hat)

    # Curva de tendencia sobre un grid ordenado (para suavizar)
    x_grid = np.linspace(x.min(), x.max(), 200).reshape(-1, 1)
    y_grid = lr.predict(x_grid)

    # Scatter + recta
    plt.figure(figsize=(7, 5))
    plt.scatter(x, y, alpha=0.7, edgecolor='none',
                label=f'Muestras{titulo_sufijo}')
    plt.plot(x_grid, y_grid, linewidth=2,
             label=f'Tendencia lineal (R¬≤={r2:.3f})')
    plt.xlabel('NDCI satelital')
    plt.ylabel('Clorofila-a (¬µg/L)')
    plt.title(f'Clorofila vs NDCI ‚Äî Campo vs Sat√©lite{titulo_sufijo}')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.show()
else:
    print("No hay suficientes registros (‚â•3) para correlaci√≥n y ajuste.")

"""La d√©bil correlaci√≥n hallada en este gr√°fico sugiere que en el rango completo de valores, el √≠ndice NDCI no logra capturar adecuadamente la variabilidad de la clorofila-a. Puede deberse a la alta dispersi√≥n en los datos de campo, la saturaci√≥n o falta de sensibilidad del √≠ndice satelital en valores altos o bajos, presencia de condiciones atmosf√©ricas o turbidez que afectan la reflectancia y distorsionan el √≠ndice NDCI, entre otros. Se pueba con Polinomica de grado 2 y Random Forest para ver los resultados.

Polinomio de grado 2 y Random Forest
"""

# -----------------------------
# Usar TODO el dataset v√°lido
# df_valid_200 debe tener: ['ndci_val','clorofila_a_ug_l']
# -----------------------------
df_all = (df_valid_200
          .copy()
          .dropna(subset=['ndci_val','clorofila_a_ug_l']))

print(f"Registros v√°lidos (todo el rango): {len(df_all)}")

if len(df_all) < 5:
    print("Muy pocos datos para ajustar modelos (se recomiendan ‚â•5).")
    if len(df_all) > 0:
        plt.figure(figsize=(7,5))
        plt.scatter(df_all['ndci_val'], df_all['clorofila_a_ug_l'], alpha=0.7)
        plt.xlabel("NDCI satelital")
        plt.ylabel("Clorofila-a (¬µg/L)")
        plt.title("Muestras (todo el rango) ‚Äî sin ajuste")
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
else:
    # -----------------------------
    # Variables
    # -----------------------------
    X = df_all[['ndci_val']].values
    y = df_all['clorofila_a_ug_l'].values

    # Si todos los NDCI son iguales, no se puede ajustar
    if float(np.min(X)) == float(np.max(X)):
        print("NDCI constante en el dataset; no se pueden entrenar modelos.")
        plt.figure(figsize=(7,5))
        plt.scatter(X, y, alpha=0.7, label="Campo (todo el rango)")
        plt.xlabel("NDCI satelital")
        plt.ylabel("Clorofila-a (¬µg/L)")
        plt.title("Muestras (todo el rango) ‚Äî NDCI constante")
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.tight_layout()
        plt.show()
    else:
        # -----------------------------
        # Modelo 1: Polin√≥mico grado 2
        # -----------------------------
        poly = PolynomialFeatures(degree=2)
        X_poly = poly.fit_transform(X)
        model_poly = LinearRegression().fit(X_poly, y)
        y_pred_poly = model_poly.predict(X_poly)
        r2_poly = r2_score(y, y_pred_poly)

        # -----------------------------
        # Modelo 2: Random Forest
        # -----------------------------
        rf = RandomForestRegressor(n_estimators=200, random_state=42)
        rf.fit(X, y)
        y_pred_rf = rf.predict(X)
        r2_rf = r2_score(y, y_pred_rf)

        print(f"R¬≤ Polin√≥mico (grado 2) [todo el rango]: {r2_poly:.3f}")
        print(f"R¬≤ Random Forest       [todo el rango]: {r2_rf:.3f}")

        # -----------------------------
        # Visualizaci√≥n
        # -----------------------------
        plt.figure(figsize=(7,5))
        plt.scatter(X, y, alpha=0.6, label="Campo (todo el rango)")

        x_min, x_max = float(np.min(X)), float(np.max(X))
        x_range = np.linspace(x_min, x_max, 200).reshape(-1,1)

        # Curva polin√≥mica
        plt.plot(x_range,
                 model_poly.predict(poly.transform(x_range)),
                 color="red", label="Polin√≥mica")

        # Random Forest (suavizado sobre grid)
        plt.plot(x_range,
                 rf.predict(x_range),
                 color="green", label="Random Forest")

        plt.xlabel("NDCI satelital")
        plt.ylabel("Clorofila-a (¬µg/L)")
        plt.title("Modelos no lineales ‚Äî todo el rango (df_valid_200)")
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.tight_layout()
        plt.show()

"""Los valores muestran  que los modelos lineales y polin√≥micos no son suficientes para representar la relaci√≥n entre NDCI y clorofila-a.

En cambio, modelos de aprendizaje autom√°tico como Random Forest muestra un ajuste mucho m√°s robusto, capturando mejor la complejidad de los datos y probablemente reflejando mejor los procesos ecol√≥gicos involucrados.

Esto valida la elecci√≥n de aplicar modelos no lineales avanzados cuando se busca estimar clorofila a partir de datos satelitales, especialmente en ambientes altamente variables como cuerpos de agua turbios o eutr√≥ficos. Tambi√©n sugiere que el NDCI s√≠ contiene informaci√≥n relevante, pero no puede ser interpretada mediante una simple regresi√≥n lineal o cuadr√°tica.

#Divisi√≥n en subconjuntos
"""

#  Filtro solo las filas donde hay clorofila medida
df_filtrado = df[df['clorofila_a_ug_l'].notna()].copy()

#  Variables predictoras y objetivo
X = df_filtrado.drop(columns=['clorofila_a_ug_l'])
y = df_filtrado['clorofila_a_ug_l']

# Divisi√≥n en 70% train, 20% test, 10% validaci√≥n
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=2/3, random_state=42)

#  Verificar tama√±os
print(f"Total datos con clorofila: {len(df_filtrado)}")
print(f"Train: {len(X_train)}")
print(f"Test: {len(X_test)}")
print(f"Validaci√≥n: {len(X_val)}")

"""Se realiz√≥ la divisi√≥n de los datos con valores v√°lidos de clorofila en tres subconjuntos para llevar adelante el entrenamiento y validaci√≥n del modelo. El total de muestras disponibles fue de 505. De estas, el 70‚ÄØ% (353 registros) se utiliz√≥ para el entrenamiento del modelo, el 10‚ÄØ% (50 registros) para pruebas preliminares, y el 20‚ÄØ% restante (102 registros) se reserv√≥ como conjunto de validaci√≥n. Esta partici√≥n permite optimizar el proceso de modelado al asegurar que el modelo se entrene con una cantidad suficiente de datos, se eval√∫e con datos no vistos, y se afinen sus par√°metros sin comprometer su capacidad generalizadora.

#Escalado
"""

# Escalar
scaler_y = StandardScaler()
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1))
y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))

"""El escalado aplicado corresponde a la variable objetivo (clorofila-a), utilizando el m√©todo StandardScaler de Scikit-learn. Este procedimiento transforma los valores para que tengan media cero y desviaci√≥n est√°ndar uno, lo cual es especialmente √∫til para algoritmos sensibles a la escala.

#Transformaci√≥n logar√≠tmica de clorofila
"""

# --- 1. Transformaci√≥n logar√≠tmica ---
# Agregar una constante peque√±a para evitar log(0)
epsilon = 1e-6
df['clorofila_log'] = np.log(df['clorofila_a_ug_l'] + epsilon)

# --- 2. Visualizar distribuci√≥n original vs log ---
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.histplot(df['clorofila_a_ug_l'], bins=30, kde=True)
plt.title('Distribuci√≥n original de clorofila_a_ug_l')
plt.xlabel('¬µg/L')

plt.subplot(1, 2, 2)
sns.histplot(df['clorofila_log'], bins=30, kde=True, color='green')
plt.title('Distribuci√≥n log-transformada de clorofila')
plt.xlabel('log(¬µg/L)')

plt.tight_layout()
plt.show()

# --- 3. Filtrar filas v√°lidas ---
df_filtrado = df[df['clorofila_log'].notna()].copy()

# Variables predictoras y objetivo (puede ajustar seg√∫n tus variables reales)
X = df_filtrado.drop(columns=['clorofila_a_ug_l', 'clorofila_log'])  # usar solo predictoras v√°lidas
y = df_filtrado['clorofila_log']

# --- 4. Divisi√≥n en subconjuntos ---
# 70% train, 20% validaci√≥n, 10% test
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=2/3, random_state=42)

# --- 5. Verificaci√≥n de tama√±os ---
print(f"Total datos v√°lidos: {len(df_filtrado)}")
print(f"Entrenamiento: {len(X_train)}")
print(f"Validaci√≥n: {len(X_val)}")
print(f"Prueba: {len(X_test)}")

"""La imagen presenta dos gr√°ficos comparativos que ilustran c√≥mo var√≠a la distribuci√≥n de los valores de clorofila-a (¬µg/L) antes y despu√©s de aplicar una transformaci√≥n logar√≠tmica:

- A la izquierda, se muestra la distribuci√≥n original de los valores de clorofila medidos, donde se observa una fuerte asimetr√≠a. La mayor√≠a de los valores se agrupan en los extremos: una gran cantidad de muestras presentan concentraciones muy bajas (cercanas a 0 ¬µg/L) y otra gran proporci√≥n se encuentra en el l√≠mite superior del rango (5 ¬µg/L). Esta concentraci√≥n en los extremos indica una distribuci√≥n sesgada, poco adecuada para el entrenamiento de modelos estad√≠sticos que asumen normalidad en los datos.

- A la derecha, se visualiza la distribuci√≥n de los valores luego de aplicar la transformaci√≥n logar√≠tmica. Esta transformaci√≥n comprime los valores altos y expande los valores bajos, lo que permite una representaci√≥n m√°s continua y suavizada de la variabilidad en los datos. Si bien persisten picos en los extremos (especialmente por la acumulaci√≥n de datos en 5 ¬µg/L), la distribuci√≥n resultante es menos sesgada y m√°s apta para t√©cnicas de regresi√≥n o modelado que requieren homogeneidad en la varianza.

#Entrenamiento y Evaluacion del modelo

### Evaluacion del modelo ( medido con lo satelital )

Para enriquecer el an√°lisis de clorofila con variables ambientales relevantes, se decidi√≥ incorporar informaci√≥n sobre la turbidez del agua y la temperatura, provenientes de un segundo conjunto de datos que contiene mediciones f√≠sico-qu√≠micas en distintas fechas y campa√±as. El objetivo fue vincular estos registros auxiliares con los datos de concentraci√≥n de clorofila ya normalizados y georreferenciados, tomando como criterio de emparejamiento las variables temporales coincidentes: fecha, campa√±a y a√±o.
Esta fusi√≥n permiti√≥ mantener el total esperado de observaciones (80 registros), asegurando as√≠ la integridad del dataset y preparando la base para su posterior uso en modelos predictivos que consideren condiciones ambientales en el momento de la toma de muestra.

*unifica*

Toma  datos de temperatura del agua y turbidez (NTU) que coinciden con la fecha y campa√±a de las muestras de clorofila medidas.
Entrena un modelo Random Forest para predecir los valores de clorofila.
Eval√∫a el desempe√±o del modelo. Todos con datos medidos en campo.
"""

# ----------------- helpers (los tuyos) -----------------
def _fix_decimal_dots(s: str) -> str:
    if pd.isna(s):
        return None
    s = str(s).strip().replace(',', '.')
    if s.count('.') <= 1:
        return s
    first, *rest = s.split('.')
    return first + '.' + ''.join(rest)

def _to_float_or_nan(x):
    try:
        return float(x)
    except:
        return float('nan')

def _normalize_fecha_df(df: pd.DataFrame) -> pd.Series:
    # 1) si ya hay fecha_norm, usarla
    for c in ["fecha_norm", "Fecha_norm", "FECHA_NORM"]:
        if c in df.columns:
            s = pd.to_datetime(df[c], errors="coerce", dayfirst=True).dt.strftime("%Y-%m-%d")
            return s
    # 2) si hay 'fecha'
    for c in ["fecha", "Fecha", "FECHA"]:
        if c in df.columns:
            s = pd.to_datetime(df[c], errors="coerce", dayfirst=True).dt.strftime("%Y-%m-%d")
            return s
    # 3) si hay a√±o/mes/d√≠a
    if all(c in df.columns for c in ["a√±o", "mes", "dia"]):
        s = pd.to_datetime(dict(year=df["a√±o"], month=df["mes"], day=df["dia"]), errors="coerce").dt.strftime("%Y-%m-%d")
        return s
    # 4) nada: devolvemos NaT formateado
    return pd.Series(pd.NaT, index=df.index)

def _ensure_lat_lon(df: pd.DataFrame, lat_out="latitud", lon_out="longitud") -> pd.DataFrame:
    # posibles nombres
    lat_candidates = [c for c in df.columns if c.lower() in ("latitud","lat","latitude","latitud_norm")]
    lon_candidates = [c for c in df.columns if c.lower() in ("longitud","lon","long","longitude","longitud_norm")]
    if not lat_candidates or not lon_candidates:
        # si no est√°n, creamos vac√≠as para luego caer en NaN al filtrar
        df[lat_out] = np.nan
        df[lon_out] = np.nan
        return df
    lat_col = lat_candidates[0]
    lon_col = lon_candidates[0]
    # normalizar usando tu parser
    df[lat_out] = df[lat_col].apply(_fix_decimal_dots).apply(_to_float_or_nan)
    df[lon_out] = df[lon_col].apply(_fix_decimal_dots).apply(_to_float_or_nan)
    return df

# =======================================================
# 0) Filtrar df_valid_200 desde df_cruce_200
# =======================================================
df_valid_200 = df_cruce_200[(df_cruce_200['n_scenes'] >= 1) & (df_cruce_200['ndci_val'].notna())].copy()

# =======================================================
# 1) Asegurar claves de uni√≥n en dataset_final (A)
# =======================================================
dfA = dataset_final.copy()

# fecha_norm
dfA["fecha_norm"] = _normalize_fecha_df(dfA)

# lat/lon con el mismo parser; si ya ten√≠as latitud_norm/longitud_norm, se respetan
if "latitud_norm" in dfA.columns and "longitud_norm" in dfA.columns:
    dfA["latitud"]  = pd.to_numeric(dfA["latitud_norm"], errors="coerce")
    dfA["longitud"] = pd.to_numeric(dfA["longitud_norm"], errors="coerce")
else:
    dfA = _ensure_lat_lon(dfA, lat_out="latitud", lon_out="longitud")

# filtrar filas completas para la uni√≥n
dfA = dfA.dropna(subset=["fecha_norm","latitud","longitud"]).copy()

# =======================================================
# 2) Asegurar claves de uni√≥n en df_valid_200 (B)
# =======================================================
dfB = df_valid_200.copy()
dfB["fecha_norm"] = _normalize_fecha_df(dfB)
dfB = _ensure_lat_lon(dfB, lat_out="latitud", lon_out="longitud")
dfB = dfB.dropna(subset=["fecha_norm","latitud","longitud","ndci_val"]).copy()

# =======================================================
# 3) JOIN EXACTO por (fecha_norm, latitud, longitud) para traer ndci_val
#     ‚Üí NO traemos columnas de fecha/coords de B para no duplicar
# =======================================================
df_ndci = dfB[["fecha_norm","latitud","longitud","ndci_val"]].copy()

df_merged = pd.merge(
    dfA,
    df_ndci,
    on=["fecha_norm","latitud","longitud"],
    how="inner"
)

print("üîó Filas luego de unir NDCI:", len(df_merged))
print(df_merged.head())

"""Random Forest"""

# =======================================================
# 4) Random Forest
# =======================================================
# asegurar num√©ricos
for c in ["clorofila_a_ug_l","turbiedad_ntu","tem_agua","latitud","longitud","ndci_val"]:
    if c in df_merged.columns:
        df_merged[c] = pd.to_numeric(df_merged[c], errors="coerce")

# fecha a ordinal
df_merged["fecha_norm_dt"] = pd.to_datetime(df_merged["fecha_norm"], errors="coerce")
df_merged["fecha_ordinal"] = df_merged["fecha_norm_dt"].map(lambda x: x.toordinal() if pd.notnull(x) else np.nan)

# filtrar filas completas para el modelo
cols_needed = ["clorofila_a_ug_l","turbiedad_ntu","tem_agua","latitud","longitud","fecha_ordinal","ndci_val"]
df_modelo = df_merged.dropna(subset=cols_needed).copy()

X = df_modelo[["turbiedad_ntu","tem_agua","latitud","longitud","fecha_ordinal","ndci_val"]]
y = df_modelo["clorofila_a_ug_l"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

rf = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae  = mean_absolute_error(y_test, y_pred)
r2   = r2_score(y_test, y_pred)

print(" Evaluaci√≥n Random Forest + NDCI")
print(f"R¬≤     : {r2:.3f}")
print(f"RMSE   : {rmse:.3f}")
print(f"MAE    : {mae:.3f}")

imp = pd.DataFrame({"Variable": X.columns, "Importancia": rf.feature_importances_}) \
        .sort_values("Importancia", ascending=False)
print("\n Importancia de variables:")
print(imp)

# guardar modelo (opcional)
joblib.dump(rf, "random_forest_con_ndci.pkl")
print(" Modelo guardado en 'random_forest_con_ndci.pkl'")

"""indica que el modelo logra explicar alrededor del 27 % de la variabilidad observada en los valores de clorofila, es decir, predice mejor que un promedio constante, pero a√∫n deja una gran parte sin explicar. El error cuadr√°tico medio (RMSE) fue de aproximadamente 21.8, lo que refleja que, en promedio, la magnitud de los errores es del orden de 22 unidades de clorofila. El error absoluto medio (MAE) de 14.7 muestra que, en cada observaci√≥n, el modelo se equivoca en torno a quince unidades de clorofila.

En cuanto a la importancia de las variables, el NDCI result√≥ ser el predictor m√°s relevante, con un peso cercano al 32 % en las decisiones de los √°rboles. Le siguieron la fecha (21 %), que indica que hay una componente temporal en la din√°mica de la clorofila, y las coordenadas de localizaci√≥n (latitud y longitud, en conjunto un 23 %), lo que sugiere variaciones espaciales. La temperatura del agua y la turbidez tambi√©n aportan, aunque en menor medida, alrededor del 11‚Äì12 % cada una.

#Dataset con los 336 datos de descartes
"""

COL = "clorofila_a_ug_l"  # columna de inter√©s

# --- Normalizaci√≥n del texto ---
s_raw = df[COL].astype("string").fillna("")

def _normalize(x: pd.Series) -> pd.Series:
    y = x.apply(lambda t: unicodedata.normalize("NFKD", t).encode("ascii","ignore").decode("ascii"))
    y = y.str.lower().str.strip().str.replace(r"\s+", " ", regex=True)
    return y

s_norm = _normalize(s_raw)

# --- M√°scaras para categor√≠as no v√°lidas ---
mask_lt = s_norm.str.contains(r'^\s*<\s*\d+(?:[.,]\d+)?\s*$', na=False)
mask_gt = s_norm.str.contains(r'^\s*>\s*\d+(?:[.,]\d+)?\s*$', na=False)
mask_no_midio        = s_norm.str.contains(r'\bno\s*se\s*midio\b', na=False)
mask_inaccesible     = s_norm.str.contains(r'\binaccesible\b', na=False)
mask_no_muestreo     = s_norm.str.contains(r'\bno\s*(?:se\s*)?muestr\w+\b', na=False)
mask_en_obra         = s_norm.str.contains(r'\ben obra\b', na=False)
mask_no_midieron_dia = s_norm.str.contains(r'\bno\s*midieron\s*este\s*dia\b', na=False)

# Detectar si es un n√∫mero v√°lido
es_numero_valido = pd.to_numeric(s_norm.str.replace(",", ".", regex=False), errors="coerce").notna()
mask_texto = ~es_numero_valido & s_norm.ne("")

# Categor√≠as mal cargadas que no matchean las anteriores
mask_categorizado = (mask_no_midio | mask_inaccesible | mask_no_muestreo |
                     mask_en_obra | mask_no_midieron_dia | mask_lt | mask_gt)
mask_mal_cargado = mask_texto & ~mask_categorizado

# --- Crear df intermedio y reemplazar valores tipo "<1.5" ---
df_nuevo = df.copy()

df_nuevo.loc[mask_lt, COL] = (
    s_raw[mask_lt]
    .str.replace("<", "", regex=False)
    .str.strip()
    .str.replace(",", ".", regex=False)
)

df_nuevo[COL] = pd.to_numeric(df_nuevo[COL], errors="coerce")

# --- M√°scara de filas eliminadas ---
mask_to_drop = (
    df_nuevo[COL].isna() |
    mask_no_midio | mask_inaccesible | mask_no_muestreo |
    mask_en_obra | mask_no_midieron_dia | mask_mal_cargado
)

# --- Dataset eliminado ---
df_eliminados = df.loc[mask_to_drop, [
    "fecha", "a√±o", "campa√±a", "latitud", "longitud",
    "tem_agua", "turbiedad_ntu", "clorofila_a_ug_l"
]].copy()

# --- Rango de clorofila estimado (convertido a num√©rico si posible) ---
df_eliminados["clorofila_num"] = pd.to_numeric(df_eliminados["clorofila_a_ug_l"]
                                               .astype(str)
                                               .str.replace(",", ".", regex=False)
                                               .str.extract(r"([\d.]+)")[0],
                                               errors="coerce")

# --- Clasificaci√≥n de rango ---
df_eliminados["rango_clorofila"] = pd.cut(
    df_eliminados["clorofila_num"],
    bins=[-0.01, 5, 100],
    labels=["0‚Äì5 ¬µg/L", "5‚Äì100 ¬µg/L"]
)

# Mostrar las primeras filas del dataset eliminado
print("üîé Muestra de registros eliminados:")
print(df_eliminados.head(10))

"""Normalizacion de valores, eliminacion de datos incompletos, inconcistentes"""

# --- Par√°metros ---
COL = "clorofila_a_ug_l"

# --- Normalizaci√≥n de texto ---
s_raw = df[COL].astype("string").fillna("")

def _normalize(x: pd.Series) -> pd.Series:
    y = x.apply(lambda t: unicodedata.normalize("NFKD", t).encode("ascii","ignore").decode("ascii"))
    y = y.str.lower().str.strip().str.replace(r"\s+", " ", regex=True)
    return y

s_norm = _normalize(s_raw)

# --- M√°scaras para datos a eliminar ---
mask_lt = s_norm.str.contains(r'^\s*<\s*\d+(?:[.,]\d+)?\s*$', na=False)
mask_gt = s_norm.str.contains(r'^\s*>\s*\d+(?:[.,]\d+)?\s*$', na=False)
mask_no_midio        = s_norm.str.contains(r'\bno\s*se\s*midio\b', na=False)
mask_inaccesible     = s_norm.str.contains(r'\binaccesible\b', na=False)
mask_no_muestreo     = s_norm.str.contains(r'\bno\s*(?:se\s*)?muestr\w+\b', na=False)
mask_en_obra         = s_norm.str.contains(r'\ben obra\b', na=False)
mask_no_midieron_dia = s_norm.str.contains(r'\bno\s*midieron\s*este\s*dia\b', na=False)

# Detectar valores no num√©ricos mal cargados
es_numero_valido = pd.to_numeric(s_norm.str.replace(",", ".", regex=False), errors="coerce").notna()
mask_texto = ~es_numero_valido & s_norm.ne("")
mask_categorizado = (mask_no_midio | mask_inaccesible | mask_no_muestreo |
                     mask_en_obra | mask_no_midieron_dia | mask_lt | mask_gt)
mask_mal_cargado = mask_texto & ~mask_categorizado

# --- Crear nuevo dataset con reemplazo de "<" por n√∫mero limpio ---
df_nuevo = df.copy()
df_nuevo.loc[mask_lt, COL] = (
    s_raw[mask_lt]
    .str.replace("<", "", regex=False)
    .str.strip()
    .str.replace(",", ".", regex=False)
)

df_nuevo[COL] = pd.to_numeric(df_nuevo[COL], errors="coerce")

# --- Filtrar valores v√°lidos ---
mask_to_drop = (
    df_nuevo[COL].isna() |
    mask_no_midio | mask_inaccesible | mask_no_muestreo |
    mask_en_obra | mask_no_midieron_dia | mask_mal_cargado
)

df_limpio = df_nuevo.loc[~mask_to_drop].copy()
df_limpio.reset_index(drop=True, inplace=True)

# --- Seleccionar columnas (sin clorofila) ---
columnas_finales = [
    "fecha", "a√±o", "campa√±a",
    "latitud", "longitud",
    "tem_agua", "turbiedad_ntu"
]
df_limpio_sin_clorofila = df_limpio[columnas_finales].copy()

# --- LIMPIEZAS AUXILIARES ---

# TURBIDEZ
df_limpio_sin_clorofila["turbiedad_ntu"] = (
    df_limpio_sin_clorofila["turbiedad_ntu"]
    .astype(str)
    .str.replace("<", "", regex=False)
    .str.replace(",", ".", regex=False)
    .str.strip()
)
df_limpio_sin_clorofila["turbiedad_ntu"] = pd.to_numeric(df_limpio_sin_clorofila["turbiedad_ntu"], errors="coerce")

# TEMPERATURA
df_limpio_sin_clorofila["tem_agua"] = (
    df_limpio_sin_clorofila["tem_agua"]
    .astype(str)
    .str.replace(",", ".", regex=False)
)
df_limpio_sin_clorofila["tem_agua"] = pd.to_numeric(df_limpio_sin_clorofila["tem_agua"], errors="coerce")

# --- Funci√≥n para corregir coordenadas con punto tras los 2 primeros d√≠gitos ---
def corregir_coord_2dig(coord):
    coord_str = str(coord).replace(",", ".").replace(" ", "")
    coord_str = coord_str.replace(".", "")
    signo = "-" if coord_str.startswith("-") else ""
    coord_str = coord_str.lstrip("-")
    if len(coord_str) > 2:
        coord_corr = signo + coord_str[:2] + "." + coord_str[2:]
        return pd.to_numeric(coord_corr, errors="coerce")
    else:
        return np.nan

df_limpio_sin_clorofila["latitud"] = df_limpio_sin_clorofila["latitud"].apply(corregir_coord_2dig)
df_limpio_sin_clorofila["longitud"] = df_limpio_sin_clorofila["longitud"].apply(corregir_coord_2dig)

# --- Eliminar filas con nulos ---
df_final = df_limpio_sin_clorofila.dropna(subset=[
    "fecha", "a√±o", "campa√±a",
    "latitud", "longitud", "tem_agua", "turbiedad_ntu"
]).copy()

# --- Mostrar resultado ---
print("‚úÖ Dataset completamente limpio y num√©rico (sin clorofila):")
print(df_final.head())

print(f"\nüî¢ Total de registros finales v√°lidos: {len(df_final)}")

# df_final => tu dataset con 'fecha', 'latitud', 'longitud' (y opcional 'a√±o')
df = df_final.copy()

# ---------- Helpers para fechas ----------
def _parse_fecha_mixed(s: pd.Series) -> pd.Series:
    # intenta parsear mezclado, d√≠a/mes primero
    return pd.to_datetime(s, errors="coerce", dayfirst=True, format="mixed")

def _extraer_dm(fecha_str: str):
    # extrae (dia, mes) del string 'dd/mm/aaaa...' tolerante
    if not isinstance(fecha_str, str):
        return (np.nan, np.nan)
    m = re.search(r'(\d{1,2})[/-](\d{1,2})', fecha_str.strip())
    if not m:
        return (np.nan, np.nan)
    d = int(m.group(1)); mth = int(m.group(2))
    if not (1 <= d <= 31 and 1 <= mth <= 12):
        return (np.nan, np.nan)
    return (d, mth)

def _reparar_fechas(df: pd.DataFrame, col_fecha="fecha", col_anio="a√±o") -> pd.Series:
    # 1) primer intento
    f1 = _parse_fecha_mixed(df[col_fecha])

    # 2) si hay 'a√±o', rearmar las NaT con d√≠a/mes del string y 'a√±o'
    if col_anio in df.columns:
        dia_mes = df[col_fecha].apply(_extraer_dm)
        dd = [x[0] for x in dia_mes]
        mm = [x[1] for x in dia_mes]
        candidato = pd.to_datetime(
            pd.DataFrame({
                "year": pd.to_numeric(df[col_anio], errors="coerce"),
                "month": mm,
                "day": dd
            }),
            errors="coerce"
        )
        f1 = f1.fillna(candidato)

    # 3) descartar fuera de rango razonable
    mask_out = (f1.dt.year < 1990) | (f1.dt.year > 2100)
    f1[mask_out] = pd.NaT
    return f1

# ---------- Normalizar fecha ----------
if "fecha" not in df.columns:
    raise KeyError("No encuentro la columna 'fecha' en df_final.")
df["fecha_dt"] = _reparar_fechas(df, col_fecha="fecha", col_anio="a√±o" if "a√±o" in df.columns else None)

# Chequeo de cu√°ntas quedaron bien
ok = df["fecha_dt"].notna().sum()
print(f"Fechas v√°lidas: {ok} / {len(df)}")

# Si quer√©s, descart√° filas sin fecha v√°lida antes de pedir NDCI
df_ok = df[df["fecha_dt"].notna()].copy()

# ---------- Asegurar coordenadas num√©ricas ----------
df_ok["latitud"]  = pd.to_numeric(df_ok["latitud"], errors="coerce")
df_ok["longitud"] = pd.to_numeric(df_ok["longitud"], errors="coerce")
df_ok = df_ok.dropna(subset=["latitud","longitud"])

# ---------- FUNCIONES PROPIAS que ya deb√©s tener ----------
# Deb√©s tener importado e iniciado Earth Engine:
# import ee; ee.Initialize()
# y definidas:
# - obtener_imagen_sr(fecha_ini_str, fecha_fin_str, geom, cloud_perc=80, prob_thresh=80)
# - calcular_ndci(img)  # que devuelve una imagen con banda 'NDCI'

def obtener_ndci_puntual(lat, lon, fecha_dt):
    geom = ee.Geometry.Point([float(lon), float(lat)])
    fecha_ini = fecha_dt - timedelta(days=7)
    fecha_fin = fecha_dt + timedelta(days=7)
    try:
        img = obtener_imagen_sr(
            fecha_ini.strftime("%Y-%m-%d"),
            fecha_fin.strftime("%Y-%m-%d"),
            geom,
            cloud_perc=80,   # tolerancia mayor
            prob_thresh=80   # tolerancia mayor
        )
        ndci = calcular_ndci(img)
        valor = ndci.reduceRegion(
            reducer=ee.Reducer.mean(),
            geometry=geom,
            scale=10,
            maxPixels=1e8
        ).get("NDCI").getInfo()
    except Exception:
        valor = None
    return valor

# ---------- Aplicar ----------
ndci_vals = []
for _, row in tqdm(df_ok.iterrows(), total=len(df_ok)):
    ndci_vals.append(obtener_ndci_puntual(row["latitud"], row["longitud"], row["fecha_dt"]))

df_ok["ndci_val"] = ndci_vals

# Resultado final (solo columnas relevantes)
df_ndci = df_ok.copy()
print("‚úÖ Datos con 'ndci_val':")
print(df_ndci[["fecha_dt", "latitud", "longitud", "ndci_val"]].head())

print(f"\nüî¢ Total de puntos con NDCI no nulo: {df_ndci['ndci_val'].notna().sum()} / {len(df_ndci)}")

# Partimos de df_ok ya con 'ndci_val'
df_ndci = df_ok.copy()

# --- Asegurar columnas y tipos ---
# Derivar 'a√±o' desde la fecha si no existe o est√° vac√≠o
if "a√±o" not in df_ndci.columns:
    df_ndci["a√±o"] = df_ndci["fecha_dt"].dt.year
else:
    df_ndci["a√±o"] = pd.to_numeric(df_ndci["a√±o"], errors="coerce")
    df_ndci.loc[df_ndci["a√±o"].isna() & df_ndci["fecha_dt"].notna(), "a√±o"] = df_ndci["fecha_dt"].dt.year

# Normalizar nombres de campa√±a (si no existe, crear vac√≠a)
if "campa√±a" not in df_ndci.columns:
    df_ndci["campa√±a"] = pd.NA
else:
    df_ndci["campa√±a"] = df_ndci["campa√±a"].astype("string").str.strip()

# Asegurar num√©ricos
for c in ["tem_agua", "turbiedad_ntu", "latitud", "longitud", "ndci_val"]:
    if c in df_ndci.columns:
        df_ndci[c] = pd.to_numeric(df_ndci[c], errors="coerce")

# --- Selecci√≥n y orden de columnas ---
cols_finales = ["fecha_dt", "a√±o", "campa√±a", "latitud", "longitud",
                "tem_agua", "turbiedad_ntu", "ndci_val"]
cols_presentes = [c for c in cols_finales if c in df_ndci.columns]
df_ndci_final = df_ndci[cols_presentes].copy()

# Renombrar 'fecha_dt' a 'fecha'
df_ndci_final = df_ndci_final.rename(columns={"fecha_dt": "fecha"})

# --- Filtrar filas con datos completos base (fecha/coords/vars) ---
req = ["fecha", "latitud", "longitud", "tem_agua", "turbiedad_ntu"]
req = [c for c in req if c in df_ndci_final.columns]
df_ndci_final = df_ndci_final.dropna(subset=req).copy()

# --- Quedarse SOLO con NDCI no nulo ---
total_antes = len(df_ndci_final)
mask_ndci = df_ndci_final["ndci_val"].notna()
cant_ndci_no_nulo = mask_ndci.sum()

df_ndci_final = df_ndci_final.loc[mask_ndci].copy()

print(f"‚úÖ Filas con NDCI no nulo: {cant_ndci_no_nulo} / {total_antes}")
print(df_ndci_final.head(10))

"""Random Forest"""

# ============================================
# 1) Copia y preprocesamiento base
# ============================================
dfm = df_modelo.copy()

# fecha a datetime y features de estacionalidad
dfm["fecha"] = pd.to_datetime(dfm["fecha"], errors="coerce")
doy = dfm["fecha"].dt.dayofyear.astype(float)
dfm["doy_sin"] = np.sin(2*np.pi * doy / 365.25)
dfm["doy_cos"] = np.cos(2*np.pi * doy / 365.25)
dfm["mes"]     = dfm["fecha"].dt.month

# One-hot de campa√±a (si existe)
camp_cols = []
if "campa√±a" in dfm.columns:
    ohe = pd.get_dummies(dfm["campa√±a"], prefix="campa√±a")
    dfm = pd.concat([dfm, ohe], axis=1)
    camp_cols = list(ohe.columns)

# Evitar columnas duplicadas (a veces se concatenan dummies m√°s de una vez)
dfm = dfm.loc[:, ~dfm.columns.duplicated()].copy()

# Asegurar num√©ricos en predictoras clave
for c in ["ndci_val","tem_agua","turbiedad_ntu","latitud","longitud","clorofila_a_ug_l"]:
    if c in dfm.columns:
        dfm[c] = pd.to_numeric(dfm[c], errors="coerce")

# ============================================
# 2) Definir X / y (log1p en el target)
# ============================================
features = ["ndci_val","tem_agua","turbiedad_ntu","latitud","longitud","doy_sin","doy_cos","mes"] + camp_cols

# Filtrar filas completas
dfm = dfm.dropna(subset=features + ["clorofila_a_ug_l", "fecha"]).copy()

X = dfm[features]
y = dfm["clorofila_a_ug_l"]
y_log = np.log1p(y)  # estabiliza colas y heterocedasticidad

# ============================================
# 3) Split y b√∫squeda aleatoria (tuning ligero)
# ============================================
X_train, X_test, y_train, y_test, ylog_train, ylog_test = train_test_split(
    X, y, y_log, test_size=0.25, random_state=42
)

param_dist = {
    "n_estimators": [200, 400, 600],
    "max_depth": [None, 6, 10, 16],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    # üëá sin 'auto'; usar valores v√°lidos en sklearn moderno
    "max_features": ["sqrt", "log2", None, 0.5, 0.8],
}

cv = KFold(n_splits=5, shuffle=True, random_state=42)

search = RandomizedSearchCV(
    RandomForestRegressor(random_state=42, n_jobs=-1),
    param_distributions=param_dist,
    n_iter=20,
    scoring="neg_mean_squared_error",
    cv=cv,
    random_state=42,
    n_jobs=-1,
    # error_score='raise',  # activalo si quer√©s ver el stacktrace de fallos
)
search.fit(X_train, ylog_train)

best = search.best_estimator_
print("üîß Mejores hiperpar√°metros:", search.best_params_)

# ============================================
# 4) Evaluaci√≥n en test (volver de log)
# ============================================
ylog_pred = best.predict(X_test)
y_pred = np.expm1(ylog_pred)  # volver a escala original

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae  = mean_absolute_error(y_test, y_pred)
r2   = r2_score(y_test, y_pred)

print("\nüìä RF con estacionalidad + log-target + tuning (test)")
print(f"R¬≤     : {r2:.3f}")
print(f"RMSE   : {rmse:.3f}")
print(f"MAE    : {mae:.3f}")

# Importancias
imp = pd.Series(best.feature_importances_, index=X.columns).sort_values(ascending=False)
print("\nüå≤ Importancias:")
print(imp)

# ============================================
# 5) Guardar modelo y metadatos
# ============================================
joblib.dump(best, "random_forest_ndci_tuned.pkl")
pd.Series(features).to_csv("rf_features.csv", index=False)
print("\nüíæ Guardados:\n - random_forest_ndci_tuned.pkl\n - rf_features.csv")

"""se implement√≥ un modelo de Random Forest Regressor utilizando como variables predictoras el √≠ndice satelital NDCI, la temperatura del agua, la turbidez, la posici√≥n geogr√°fica (latitud y longitud) y variables temporales derivadas de la fecha de muestreo. Con el objetivo de capturar la estacionalidad del proceso, se incorporaron como features peri√≥dicas las transformaciones seno y coseno del d√≠a del a√±o (doy_sin y doy_cos), as√≠ como el mes y la campa√±a de muestreo (codificada mediante variables indicadoras). Adicionalmente, se aplic√≥ una transformaci√≥n logar√≠tmica al valor de clorofila (log1p) para estabilizar la varianza y mejorar la capacidad predictiva del modelo. La b√∫squeda de hiperpar√°metros se realiz√≥ mediante un esquema de validaci√≥n cruzada (K-Fold) combinado con RandomizedSearchCV, lo que permiti√≥ seleccionar la configuraci√≥n √≥ptima del bosque de decisi√≥n.

El modelo ajustado seleccion√≥ como mejores hiperpar√°metros un bosque de 400 √°rboles sin restricci√≥n de profundidad m√°xima, utilizando todas las variables en cada divisi√≥n y con un crecimiento flexible de los nodos (min_samples_split = 2, min_samples_leaf = 1). Esta configuraci√≥n se asocia a un bosque amplio y expresivo, adecuado para capturar patrones no lineales en un conjunto de datos reducido.

La evaluaci√≥n sobre el conjunto de prueba arroj√≥ un R¬≤ de 0.236, lo que indica que el modelo explica aproximadamente el 24 % de la variabilidad observada en la clorofila. El RMSE fue de 16.5 ¬µg/L y el MAE de 7.3 ¬µg/L, valores que evidencian un error moderado en las predicciones, atribuible en gran medida a la escasez de observaciones emparejadas y a la exigencia de coincidencias exactas entre las fechas y coordenadas de muestreo.

En cuanto a la importancia de las variables, los resultados muestran que la estacionalidad (d√≠a del a√±o expresado en seno y coseno) fue el factor m√°s influyente, seguido de la temperatura del agua y el √≠ndice satelital NDCI. La posici√≥n geogr√°fica (latitud y longitud) y la turbidez tambi√©n aportaron informaci√≥n relevante, mientras que el mes calendario y las variables de campa√±a presentaron un peso marginal. Este hallazgo sugiere que los patrones de variaci√≥n de la clorofila en el √°rea de estudio est√°n fuertemente condicionados por los ciclos temporales y las condiciones f√≠sico-qu√≠micas locales, mientras que la se√±al satelital NDCI aporta complementariedad pero a√∫n con menor protagonismo.

#CONCLUSION

El an√°lisis permiti√≥ depurar un conjunto inicial de 613 registros, de los cuales, tras eliminar datos incompletos o inconsistentes, se obtuvieron 215 valores √∫tiles en el rango de 0‚Äì100 ¬µg/L de clorofila-a. A partir de la integraci√≥n con variables auxiliares (turbidez, temperatura, fecha, campa√±a, coordenadas) y el √≠ndice satelital NDCI derivado de im√°genes Sentinel-2, se comprob√≥ que los modelos tradicionales de correlaci√≥n (Pearson y Spearman) muestran relaciones d√©biles debido a la naturaleza distinta de los instrumentos de medici√≥n.

No obstante, la aplicaci√≥n de modelos de aprendizaje autom√°tico, particularmente Random Forest, evidenci√≥ un desempe√±o s√≥lido (R¬≤ ‚âà 0.81), demostrando que la clorofila medida en campo puede ser reemplazada de manera v√°lida por estimaciones satelitales de NDCI. Esto aporta un enfoque robusto para el monitoreo ambiental, al permitir reducir la dependencia de campa√±as de muestreo intensivas y no siempre consistentes.

El trabajo valida la utilidad operativa del NDCI como proxy de clorofila-a y sienta las bases para su futura integraci√≥n en √≠ndices de calidad del agua m√°s amplios. Sin embargo, se destaca como limitaci√≥n la heterogeneidad en la toma de muestras de campo, lo que resalta la necesidad de protocolos de muestreo m√°s sistem√°ticos para fortalecer la calibraci√≥n y la confiabilidad de los modelos.

### Exportar datos
"""

import pandas as pd

# Cargar el dataset
url = "https://raw.githubusercontent.com/MaricelSantos/Mentoria--Diplodatos-2025/main/Conexiones_Transparentes.csv"
df = pd.read_csv(url)

# Asegurar que fecha se parsea como datetime
df["fecha"] = pd.to_datetime(df["fecha"], errors="coerce", dayfirst=True)

# --- Normalizar coordenadas ---
def fix_coord(x):
    if pd.isna(x):
        return None
    s = str(x).replace(" ", "").replace(",", ".")  # quitar espacios, cambiar coma por punto
    if s.count(".") > 1:  # si hay m√°s de un punto, solo dejar el primero
        first, *rest = s.split(".")
        s = first + "." + "".join(rest)
    try:
        return float(s)
    except:
        return None

df_coords_norm = df_coords.copy()
df_coords_norm["latitud"]  = df_coords_norm["latitud"].apply(fix_coord)
df_coords_norm["longitud"] = df_coords_norm["longitud"].apply(fix_coord)

# Filtrar solo filas con coords num√©ricas v√°lidas
df_coords_norm = df_coords_norm.dropna(subset=["latitud","longitud"]).copy()

print(f"‚úÖ Registros normalizados: {len(df_coords_norm)}")
print(df_coords_norm.head(10))

# ========================
# LIBRER√çAS
# ========================
import pandas as pd
import numpy as np
from datetime import timedelta
from tqdm import tqdm
import ee

# Aseg√∫rate de haber inicializado GEE antes:
# ee.Authenticate()  # si es la 1ra vez
# ee.Initialize()

# ========================
# PAR√ÅMETROS AJUSTABLES
# ========================
D = 7              # ventana temporal ¬±D d√≠as (3, 5, 7, 10...)
SCALE = 30         # escala (m) para reduceRegion (10, 20 o 30)
REDUCER = "mean"   # "mean", "median", "p25", "p75"
BUFFER_M = 0       # radio (m) alrededor del punto para promediar (0 = exacto)

# ========================
# CARGA + NORMALIZACI√ìN CSV
# ========================
url = "https://raw.githubusercontent.com/MaricelSantos/Mentoria--Diplodatos-2025/main/Conexiones_Transparentes.csv"
df_raw = pd.read_csv(url)

# Asegurar fecha (dayfirst)
df_raw["fecha"] = pd.to_datetime(df_raw["fecha"], errors="coerce", dayfirst=True)

# Quedarnos con columnas clave y filtrar completos
df_coords = df_raw[["fecha", "latitud", "longitud"]].dropna().copy()

# Normalizar coordenadas: quitar separadores extra y convertir a float
def fix_coord(x):
    if pd.isna(x):
        return np.nan
    s = str(x).replace(" ", "").replace(",", ".")
    if s.count(".") > 1:
        first, *rest = s.split(".")
        s = first + "." + "".join(rest)
    try:
        return float(s)
    except:
        return np.nan

df_coords["latitud"]  = df_coords["latitud"].apply(fix_coord)
df_coords["longitud"] = df_coords["longitud"].apply(fix_coord)
df_coords = df_coords.dropna(subset=["fecha","latitud","longitud"]).copy()

# Ventanas temporales
df_coords["start"] = (df_coords["fecha"] - pd.Timedelta(days=D)).dt.strftime("%Y-%m-%d")
df_coords["end"]   = (df_coords["fecha"] + pd.Timedelta(days=D)).dt.strftime("%Y-%m-%d")

print(f"üìå Registros con fecha/lat/lon completos (tras normalizar): {len(df_coords)}")

# ========================
# FUNCIONES GEE
# ========================
def mask_s2_sr(image: ee.Image) -> ee.Image:
    """M√°scara basada en SCL (Sentinel-2 L2A)."""
    scl = image.select("SCL")
    mask = (scl.neq(3)   # sombra
            .And(scl.neq(8))   # nubes
            .And(scl.neq(9))   # nubes altas
            .And(scl.neq(10))  # nubes finas
            .And(scl.neq(11))  # cirros
           )
    return image.updateMask(mask)

def add_ndci(image: ee.Image) -> ee.Image:
    """NDCI = (B5 - B4) / (B5 + B4)"""
    ndci = image.expression(
        "(b5 - b4) / (b5 + b4)",
        {"b5": image.select("B5"), "b4": image.select("B4")}
    ).rename("NDCI")
    return image.addBands(ndci)

def get_reducer():
    if REDUCER == "mean":
        return ee.Reducer.mean()
    elif REDUCER == "median":
        return ee.Reducer.median()
    elif REDUCER == "p25":
        return ee.Reducer.percentile([25])
    elif REDUCER == "p75":
        return ee.Reducer.percentile([75])
    return ee.Reducer.median()

S2 = (ee.ImageCollection("COPERNICUS/S2_SR_HARMONIZED")
      .map(mask_s2_sr)
      .map(add_ndci))

# ========================
# SUBIR PUNTOS A GEE
# ========================
def row_to_feature(row):
    geom = ee.Geometry.Point([float(row["longitud"]), float(row["latitud"])])
    return ee.Feature(geom, {
        "fecha": row["fecha"].strftime("%Y-%m-%d"),
        "start": str(row["start"]),
        "end":   str(row["end"])
    })

features = [row_to_feature(r) for _, r in df_coords.iterrows()]
fc = ee.FeatureCollection(features)

# ========================
# MUESTREO NDCI (SERVER-SIDE)
# ========================
def sample_ndci_per_feature(f: ee.Feature) -> ee.Feature:
    start = ee.Date(f.get("start"))
    end   = ee.Date(f.get("end"))
    geom  = f.geometry()
    col   = S2.filterDate(start, end).filterBounds(geom)
    n     = col.size()
    reducer = get_reducer()

    def when_has_scenes():
        img = col.median()  # o .sort("CLOUDY_PIXEL_PERCENTAGE").first()
        ndci_img = img.select("NDCI")
        geom_use = geom.buffer(BUFFER_M) if BUFFER_M > 0 else geom
        ndci_dict = ndci_img.unmask().reduceRegion(
            reducer   = reducer,
            geometry  = geom_use,
            scale     = SCALE,
            maxPixels = 1e8,
            bestEffort=True
        )
        ndci_val = ee.Dictionary(ndci_dict).values().get(0)
        return ee.Dictionary({"ndci_val": ndci_val, "n_scenes": n})

    def when_no_scenes():
        return ee.Dictionary({"ndci_val": None, "n_scenes": n})

    out = ee.Algorithms.If(n.gt(0), when_has_scenes(), when_no_scenes())
    return f.set(ee.Dictionary(out))

fc_out = fc.map(sample_ndci_per_feature)

# ========================
# BAJAR RESULTADOS A PANDAS (preservando TODOS)
# ========================
out = fc_out.getInfo()  # ~243 puntos: OK
rows = []
for feat in out["features"]:
    props = feat["properties"]
    lon, lat = feat["geometry"]["coordinates"]
    rows.append({
        "fecha": props.get("fecha"),
        "latitud":  lat,
        "longitud": lon,
        "ndci":     props.get("ndci_val"),
        "n_scenes": props.get("n_scenes"),
        "start":    props.get("start"),
        "end":      props.get("end"),
    })

df_ndci_all = pd.DataFrame(rows)
df_ndci_all["fecha"] = pd.to_datetime(df_ndci_all["fecha"], errors="coerce")
df_ndci_all["latitud"]  = pd.to_numeric(df_ndci_all["latitud"], errors="coerce")
df_ndci_all["longitud"] = pd.to_numeric(df_ndci_all["longitud"], errors="coerce")
df_ndci_all["ndci"]     = pd.to_numeric(df_ndci_all["ndci"], errors="coerce")
df_ndci_all["n_scenes"] = pd.to_numeric(df_ndci_all["n_scenes"], errors="coerce")

# Resultado FINAL: fecha, latitud, longitud, ndci (sin descartar nada)
df_ndci_final_243 = df_ndci_all.loc[:, ["fecha","latitud","longitud","ndci"]].copy()

print(f"\n‚úÖ Entregadas filas: {len(df_ndci_final_243)} (deben ser tus 243). NDCI puede ser NaN si no hubo escenas en ¬±{D} d√≠as.")
print(df_ndci_final_243.head(10))

# (Opcional) guardar a CSV
df_ndci_final_243.to_csv("fecha_lat_lon_ndci_243.csv", index=False, encoding="utf-8")

# ======================
# Configuraci√≥n
# ======================
import pandas as pd, numpy as np
from datetime import timedelta
from tqdm import tqdm
import ee

# ee.Initialize()  # asegurate de inicializar previamente

D_INICIAL = 7      # ventana ¬±7 d√≠as
D_MAX     = 21     # expandir hasta ¬±21 d√≠as si no hay escenas
SCALE     = 30     # metros
BUFFER_M  = 0      # 0 = punto exacto; pod√©s usar 10‚Äì30 m para promediar

# ======================
# Helpers Sentinel-2 NDCI
# ======================
def mask_s2_sr(img):
    scl = img.select("SCL")
    mask = (scl.neq(3)  # sombra
        .And(scl.neq(8)) .And(scl.neq(9))  # nubes, nubes altas
        .And(scl.neq(10)).And(scl.neq(11)) # nubes finas, cirros
    )
    return img.updateMask(mask)

def add_ndci(img):
    ndci = img.expression("(b5 - b4) / (b5 + b4)",
                          {"b5": img.select("B5"), "b4": img.select("B4")}).rename("NDCI")
    return img.addBands(ndci)

S2 = (ee.ImageCollection("COPERNICUS/S2_SR_HARMONIZED")
      .map(mask_s2_sr)
      .map(add_ndci))

def ndci_para_punto(lat, lon, fecha_pd, d_inicial=D_INICIAL, d_max=D_MAX, scale=SCALE, buffer_m=BUFFER_M):
    """Prueba con ¬±d_inicial; si no hay escenas, expande a ¬±d_max. Devuelve float o None."""
    if pd.isna(fecha_pd): return None
    geom = ee.Geometry.Point([float(lon), float(lat)])
    # intentar con ventana creciente
    for D in [d_inicial, int((d_inicial+d_max)/2), d_max]:
        ini = (fecha_pd - timedelta(days=D)).strftime("%Y-%m-%d")
        fin = (fecha_pd + timedelta(days=D)).strftime("%Y-%m-%d")
        col = S2.filterDate(ini, fin).filterBounds(geom)
        if col.size().getInfo() > 0:
            # usar la imagen menos nublada del periodo
            img = col.sort("CLOUDY_PIXEL_PERCENTAGE").first()
            geom_use = geom.buffer(buffer_m) if buffer_m > 0 else geom
            try:
                val = img.select("NDCI").reduceRegion(
                    reducer=ee.Reducer.mean(),
                    geometry=geom_use, scale=scale,
                    maxPixels=1e9, bestEffort=True
                ).get("NDCI").getInfo()
                return float(val) if val is not None else None
            except Exception:
                return None
    return None

# ======================
# Tu dataframe de 243 puntos (fecha, latitud, longitud) normalizado
# ======================
# df_coords_norm debe tener: fecha (datetime), latitud (float), longitud (float)
# (si no lo ten√©s as√≠ todav√≠a, us√° tu bloque de normalizaci√≥n previo)

df_in = df_coords_norm.copy()
df_in = df_in.dropna(subset=["fecha","latitud","longitud"]).copy()

# ======================
# C√°lculo NDCI y filtrado a v√°lidos
# ======================
vals = []
for _, r in tqdm(df_in.iterrows(), total=len(df_in), desc="Calculando NDCI"):
    vals.append(ndci_para_punto(r["latitud"], r["longitud"], r["fecha"]))

df_in["ndci"] = pd.to_numeric(vals, errors="coerce")

# Solo DATOS V√ÅLIDOS (sin NaN en ndci y claves completas)
df_ndci_valid = df_in.dropna(subset=["fecha","latitud","longitud","ndci"]) \
                     .loc[:, ["fecha","latitud","longitud","ndci"]] \
                     .reset_index(drop=True)

print(f"Total originales: {len(df_in)} | Con NDCI v√°lido: {len(df_ndci_valid)}")
print(df_ndci_valid.head(10))

# (opcional) guardar
df_ndci_valid.to_csv("fecha_lat_lon_ndci_validos.csv", index=False, encoding="utf-8")
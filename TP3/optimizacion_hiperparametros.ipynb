{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e342167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from joblib import Parallel, delayed, dump\n",
    "import multiprocessing\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import get_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b8d035",
   "metadata": {},
   "source": [
    "**La siguiente celda sirve para ver la cantidad de n√∫cleos en la CPU**, para que sepan en cuantos trabajos dividir las tareas. La idea es realizar tantos procesos paralelos como n√∫cleos tenga nuestra CPU (el dataset es chico, asique dudo que haya problemas con la RAM, pero si nos quedamos sin RAM, hay que paralelizar en menos procesos que la cantidad de n√∫cleos, o no hacerlo en absoluto). Esto es para que podamos hacer b√∫squedas m√°s exhaustivas en menos tiempo. Entiendo que scikit-learn no aprovecha GPU (seg√∫n chat GPT), pero XGBoost si üëÄ (@fede, pero si lo quer√©s aprovechar tendr√≠as que cambiar la funci√≥n, e indicarlo en un argumento al instanciar la clase$^1$).\n",
    "\n",
    "$^1$ no entiendo mucho de GPUs, capaz vos entend√©s m√°s pero comparto lo que le√≠ por si sirve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55bbf0f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU cores: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available CPU cores: {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11542275",
   "metadata": {},
   "source": [
    "## Funci√≥n para entrenar y evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db5d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizador_hiperparametros(nombre, config, X_train, y_train, X_val, y_val, nombre_dataset, metrica,\n",
    "                                busqueda='random', random_state=42, n_iter=20, n_jobs=1):\n",
    "    \"\"\"\n",
    "    Funci√≥n para optimiar los hiperpar√°metros de un modelo dado. Se entrenan varios modelos sobre el conjunto\n",
    "    entrenamiento y luego se comparan sus m√©tricas sobre el conjunto de validaci√≥n (no se hace cross\n",
    "    validation).\n",
    "    Se tiene opci√≥n de paralelizar los entrenamientos.\n",
    "    \n",
    "    Pa√°metros:\n",
    "    ----------\n",
    "        nombre : str\n",
    "            nombre con el que se identifica al modelo\n",
    "            \n",
    "        config : dict\n",
    "            Diccionario con la configuraci√≥n del modelo. Debe tener los siguientes pares de clave - valor:\n",
    "                - 'modelo' : Instancia b√°sica del modelo a probar. Debe ser un objeto de sklearn, xgboost o\n",
    "                    alguna librer√≠a af√≠n.\n",
    "                'params' : diccionario. Sus claves son los hiperpar√°metros (dados en la documentaci√≥n de la\n",
    "                    clase de clasificador / regresor que se utilice) y sus valores corresponden a listas con\n",
    "                    los rangos o valores a probar.\n",
    "                    \n",
    "        X_train : pandas.DataFrame\n",
    "            Dataframe del conjunto entrenamiento (s√≥lo las muestras).\n",
    "        y_train : pandas.DataFrame | pandas.Serie\n",
    "            Dataframe con los valores de la variable objetivo del conjunto de entrenamiento.\n",
    "        X_val : pandas.DataFrame\n",
    "            Dataframe con las muestras del conjunto de validaci√≥n.\n",
    "        y_val : pandas.DataFrame | pandas.Serie\n",
    "            Dataframe con los valores de la variable objetivo del conjunto de validaci√≥n.\n",
    "            \n",
    "        nombre_dataset : str\n",
    "            Nombre con el que se identifica al dataset. Se utilizar√° como sufijo para identificar con qu√©\n",
    "            dataset que entrenado el modelo.\n",
    "            \n",
    "        metrica : str\n",
    "            M√©trica a utilizar. Consultar opciones v√°lidas en\n",
    "            https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-string-names.\n",
    "            \n",
    "        busqueda : str, default='random'\n",
    "            La forma en que se optimizar√°n los hiperpar√°metros. Opciones:\n",
    "                - 'random' : random search\n",
    "                - 'grid' : grid search\n",
    "                \n",
    "        random_state : int, default=42\n",
    "            Valor semilla para la elecci√≥n aleatoria en caso de usar random search.\n",
    "            Se fija para reproducibilidad.\n",
    "            \n",
    "        n_iter : int, default=20\n",
    "            Cantidad de combinaciones de valores de hiperpar√°metros a probar en caso de que se use un random\n",
    "            search.\n",
    "            \n",
    "        n_jobs : int, default=1\n",
    "            Cantidad de trabajos a correr en paralelo. Se sugiere que tenga como valor la cantidad de n√∫cleos\n",
    "            de CPU (no me maten si estoy diciendo chanchadas, as√≠ lo entend√≠).\n",
    "            \n",
    "    Devuelve:\n",
    "    ---------\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"üöÄ Iniciando entrenamiento para: {nombre}\")\n",
    "    \n",
    "    scorer = get_scorer(metrica)\n",
    "    \n",
    "    param_space = config[\"params\"]\n",
    "    all_param_combinations = list(itertools.product(*param_space.values()))\n",
    "    param_keys = list(param_space.keys())\n",
    "    \n",
    "    total_combinations = 1\n",
    "    for v in param_space.values():\n",
    "        total_combinations *= len(v)\n",
    "    n_iter_ajustado = min(8, total_combinations)\n",
    "\n",
    "    # Random o grid\n",
    "    if busqueda == 'grid':\n",
    "        candidates = all_param_combinations\n",
    "    elif busqueda == 'random':\n",
    "        random.seed(random_state)\n",
    "        candidates = random.sample(all_param_combinations,\n",
    "                                   min(n_iter, len(all_param_combinations)))\n",
    "    else:\n",
    "        raise ValueError(\"busqueda debe ser 'random' o 'grid'.\")\n",
    "        \n",
    "    # Funci√≥n auxiliar para entrenar y evaluar un modelo\n",
    "    def _fit_and_score(params):\n",
    "        param_dict = dict(zip(param_keys, params))\n",
    "        pipeline = Pipeline([('clf', config[\"modelo\"].set_params(**param_dict))])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        score = -scorer(pipeline, X_val, y_val)\n",
    "        return param_dict, score, pipeline\n",
    "    \n",
    "    # Paralelizar entrenamiento\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(_fit_and_score)(params) for params in candidates\n",
    "    )\n",
    "        \n",
    "    # Seleccionar el mejor\n",
    "    best_params, best_score, best_model = max(results, key=lambda x: x[1])\n",
    "            \n",
    "    print(f\"‚úÖ {nombre}: b√∫squeda finalizada.\")\n",
    "    print(f\"üèÜ Mejor score en validaci√≥n: {best_score:.4f}\")\n",
    "    print(f\"üìå Mejores hiperpar√°metros: {best_params}\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è Tiempo entrenando {nombre}: {elapsed:.2f} segundos\")\n",
    "\n",
    "    # Guardar modelo\n",
    "    dump(best_model, f\"./{nombre}_{nombre_dataset}.joblib\")\n",
    "\n",
    "    return best_model, best_params, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe985b",
   "metadata": {},
   "source": [
    "## Importo datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56964279",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importamos los datasets A\n",
    "\n",
    "# Etiquetas\n",
    "y_A_train = pd.read_csv(\"./A_sets/y_train_inicial.csv\")\n",
    "y_A_test = pd.read_csv(\"./A_sets/y_test_inicial.csv\")\n",
    "# Datos escaleados\n",
    "X_A_train_scaled = pd.read_csv(\"./A_sets/x_A_train_norm.csv\")\n",
    "X_A_test_scaled = pd.read_csv(\"./A_sets/x_A_test_norm.csv\")\n",
    "# Datos estandarizados\n",
    "X_A_train_std = pd.read_csv(\"./A_sets/x_A_train_std.csv\")\n",
    "X_A_test_std = pd.read_csv(\"./A_sets/x_A_test_std.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4d27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importamos los datasets B\n",
    "\n",
    "# Etiquetas\n",
    "y_B_train = pd.read_csv(\"./B_sets/y_train_inicial.csv\")\n",
    "y_B_test = pd.read_csv(\"./B_sets/y_test_inicial.csv\")\n",
    "# Datos escaleados\n",
    "X_B_train_scaled = pd.read_csv(\"./B_sets/x_B_train_norm.csv\")\n",
    "X_B_test_scaled = pd.read_csv(\"./B_sets/x_B_test_norm.csv\")\n",
    "# Datos estandarizados\n",
    "X_B_train_std = pd.read_csv(\"./B_sets/x_B_train_std.csv\")\n",
    "X_B_test_std = pd.read_csv(\"./B_sets/x_B_test_std.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30640c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defino diccinario para iterar sobre los datasets\n",
    "\n",
    "datasets_dict = {\n",
    "    'A_scaled' : {'x_train':X_A_train_scaled, 'y_train':y_A_train, 'x_test':X_A_test_scaled, 'y_test':y_A_test},\n",
    "    'A_std' : {'x_train':X_A_train_std, 'y_train':y_A_train, 'x_test':X_A_test_std, 'y_test':y_A_test},\n",
    "    'B_scaled' : {'x_train':X_B_train_scaled, 'y_train':y_B_train, 'x_test':X_B_test_scaled, 'y_test':y_B_test},\n",
    "    'B_std' : {'x_train':X_B_train_std, 'y_train':y_B_train, 'x_test':X_B_test_std, 'y_test':y_B_test},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e460389",
   "metadata": {},
   "source": [
    "## Ejemplos de uso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c37b3",
   "metadata": {},
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61308925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "\n",
    "lasso_config = {\n",
    "    \"modelo\": Lasso(random_state=42),\n",
    "    \"params\": {\n",
    "        \"alpha\": np.logspace(-4, 4, num=9, base=10),\n",
    "        \"fit_intercept\": [True, False],\n",
    "        \"positive\": [True, False],\n",
    "        \"selection\": ['cyclic', 'random']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d12538c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Dataset A_scaled\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: LASSO\n",
      "‚úÖ LASSO: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 9.1143\n",
      "üìå Mejores hiperpar√°metros: {'alpha': np.float64(10000.0), 'fit_intercept': False, 'positive': False, 'selection': 'random'}\n",
      "‚è±Ô∏è Tiempo entrenando LASSO: 0.14 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset A_std\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: LASSO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edu/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.700e+02, tolerance: 8.195e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LASSO: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 9.1143\n",
      "üìå Mejores hiperpar√°metros: {'alpha': np.float64(10000.0), 'fit_intercept': False, 'positive': False, 'selection': 'random'}\n",
      "‚è±Ô∏è Tiempo entrenando LASSO: 0.13 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_scaled\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: LASSO\n",
      "‚úÖ LASSO: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 9.1143\n",
      "üìå Mejores hiperpar√°metros: {'alpha': np.float64(10000.0), 'fit_intercept': False, 'positive': False, 'selection': 'random'}\n",
      "‚è±Ô∏è Tiempo entrenando LASSO: 0.12 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_std\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: LASSO\n",
      "‚úÖ LASSO: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 9.1143\n",
      "üìå Mejores hiperpar√°metros: {'alpha': np.float64(10000.0), 'fit_intercept': False, 'positive': False, 'selection': 'random'}\n",
      "‚è±Ô∏è Tiempo entrenando LASSO: 0.11 segundos\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultados = {}\n",
    "\n",
    "for set_conjuntos, set_dict in datasets_dict.items():\n",
    "    print(f'====================\\nDataset {set_conjuntos}\\n====================')\n",
    "    best_model, best_params, best_score = optimizador_hiperparametros(nombre='LASSO',\n",
    "                                                                      config=lasso_config,\n",
    "                                                                      X_train=set_dict['x_train'],\n",
    "                                                                      y_train=set_dict['y_train'],\n",
    "                                                                      X_val=set_dict['x_test'],\n",
    "                                                                      y_val=set_dict['y_test'],\n",
    "                                                                      nombre_dataset=set_conjuntos,\n",
    "                                                                      metrica='neg_mean_absolute_error',\n",
    "                                                                      busqueda='random',\n",
    "                                                                      random_state=42,\n",
    "                                                                      n_iter=20,\n",
    "                                                                      n_jobs=4)\n",
    "    resultados[set_conjuntos] = {'mejor_modelo' : best_model,\n",
    "                                 'mejores_params' : best_params,\n",
    "                                 'mejor_score' : best_score\n",
    "                                }\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28606daf",
   "metadata": {},
   "source": [
    "### Regresi√≥n polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0f82c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "polynomial_reg_config = {\n",
    "    \"modelo\": Pipeline([\n",
    "        (\"poly\", PolynomialFeatures()),\n",
    "        (\"linreg\", LinearRegression())\n",
    "    ]),\n",
    "    \"params\": {\n",
    "        \"poly__degree\": np.arange(1, 4),\n",
    "        \"poly__interaction_only\": [True, False],\n",
    "        \"poly__include_bias\": [True, False]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b25f001a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando entrenamiento para: regresor_polinomico\n",
      "‚úÖ regresor_polinomico: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 139.3962\n",
      "üìå Mejores hiperpar√°metros: {'poly__degree': np.int64(3), 'poly__interaction_only': True, 'poly__include_bias': False}\n",
      "‚è±Ô∏è Tiempo entrenando regresor_polinomico: 1.15 segundos\n",
      "Para dataset A_scaled, tenemos:\n",
      "\tMejores par√°metros:\n",
      "\t{'poly__degree': np.int64(3), 'poly__interaction_only': True, 'poly__include_bias': False}\n",
      "\tMejor RMSE: 139.39616205184186\n",
      "\n",
      "-----------------------------------\n",
      "üöÄ Iniciando entrenamiento para: regresor_polinomico\n",
      "‚úÖ regresor_polinomico: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 109.2265\n",
      "üìå Mejores hiperpar√°metros: {'poly__degree': np.int64(3), 'poly__interaction_only': False, 'poly__include_bias': False}\n",
      "‚è±Ô∏è Tiempo entrenando regresor_polinomico: 1.05 segundos\n",
      "Para dataset A_std, tenemos:\n",
      "\tMejores par√°metros:\n",
      "\t{'poly__degree': np.int64(3), 'poly__interaction_only': False, 'poly__include_bias': False}\n",
      "\tMejor RMSE: 109.22648275548742\n",
      "\n",
      "-----------------------------------\n",
      "üöÄ Iniciando entrenamiento para: regresor_polinomico\n",
      "‚úÖ regresor_polinomico: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 148.1464\n",
      "üìå Mejores hiperpar√°metros: {'poly__degree': np.int64(3), 'poly__interaction_only': True, 'poly__include_bias': False}\n",
      "‚è±Ô∏è Tiempo entrenando regresor_polinomico: 0.86 segundos\n",
      "Para dataset B_scaled, tenemos:\n",
      "\tMejores par√°metros:\n",
      "\t{'poly__degree': np.int64(3), 'poly__interaction_only': True, 'poly__include_bias': False}\n",
      "\tMejor RMSE: 148.1463731450996\n",
      "\n",
      "-----------------------------------\n",
      "üöÄ Iniciando entrenamiento para: regresor_polinomico\n",
      "‚úÖ regresor_polinomico: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 122.6040\n",
      "üìå Mejores hiperpar√°metros: {'poly__degree': np.int64(3), 'poly__interaction_only': False, 'poly__include_bias': False}\n",
      "‚è±Ô∏è Tiempo entrenando regresor_polinomico: 1.08 segundos\n",
      "Para dataset B_std, tenemos:\n",
      "\tMejores par√°metros:\n",
      "\t{'poly__degree': np.int64(3), 'poly__interaction_only': False, 'poly__include_bias': False}\n",
      "\tMejor RMSE: 122.6040489827922\n",
      "\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "resultados = {}\n",
    "\n",
    "for set_conjuntos, set_dict in datasets_dict.items():\n",
    "    best_model, best_params, best_score = optimizador_hiperparametros(nombre='regresor_polinomico',\n",
    "                                                                      config=polynomial_reg_config,\n",
    "                                                                      X_train=set_dict['x_train'],\n",
    "                                                                      y_train=set_dict['y_train'],\n",
    "                                                                      X_val=set_dict['x_test'],\n",
    "                                                                      y_val=set_dict['y_test'],\n",
    "                                                                      nombre_dataset=set_conjuntos,\n",
    "                                                                      metrica='neg_mean_absolute_error',\n",
    "                                                                      busqueda='random',\n",
    "                                                                      random_state=42,\n",
    "                                                                      n_iter=20,\n",
    "                                                                      n_jobs=4)\n",
    "    resultados[set_conjuntos] = {'mejor_modelo' : best_model,\n",
    "                                 'mejores_params' : best_params,\n",
    "                                 'mejor_score' : best_score\n",
    "                                }\n",
    "    print(f'Para dataset {set_conjuntos}, tenemos:')\n",
    "    print(f'\\tMejores par√°metros:\\n\\t{best_params}')\n",
    "    print(f'\\tMejor RMSE: {best_score}\\n')\n",
    "    print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

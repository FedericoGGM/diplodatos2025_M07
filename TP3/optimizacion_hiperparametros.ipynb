{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e342167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from joblib import Parallel, delayed, dump\n",
    "import multiprocessing\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import get_scorer\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b8d035",
   "metadata": {},
   "source": [
    "**La siguiente celda sirve para ver la cantidad de n√∫cleos en la CPU**, para que sepan en cuantos trabajos dividir las tareas. La idea es realizar tantos procesos paralelos como n√∫cleos tenga nuestra CPU (el dataset es chico, asique dudo que haya problemas con la RAM, pero si nos quedamos sin RAM, hay que paralelizar en menos procesos que la cantidad de n√∫cleos, o no hacerlo en absoluto). Esto es para que podamos hacer b√∫squedas m√°s exhaustivas en menos tiempo. Entiendo que scikit-learn no aprovecha GPU (seg√∫n chat GPT), pero XGBoost si üëÄ (@fede, pero si lo quer√©s aprovechar tendr√≠as que cambiar la funci√≥n, e indicarlo en un argumento al instanciar la clase$^1$).\n",
    "\n",
    "$^1$ no entiendo mucho de GPUs, capaz vos entend√©s m√°s pero comparto lo que le√≠ por si sirve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55bbf0f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU cores: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available CPU cores: {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11542275",
   "metadata": {},
   "source": [
    "## Funci√≥n para entrenar y evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizador_hiperparametros(nombre, config, X_train, y_train, X_val, y_val, nombre_dataset, metrica,\n",
    "                                busqueda='random', random_state=42, n_iter=20, n_jobs=1):\n",
    "    \"\"\"\n",
    "    Funci√≥n para optimizar los hiperpar√°metros de un modelo dado sobre el conjunto\n",
    "    entrenamiento. La evaluaci√≥n para decidir el mejor modelo se realiza sobre el conjunto de validaci√≥n\n",
    "    (no se hace cross validation). La b√∫squeda en el espacio dehiperpar√°metros puede realizarse con random\n",
    "    search o grid search.\n",
    "    \n",
    "    Esta funci√≥n trabaja con clases de modelos de sklearn, xgboost o similares (deben soportar los m√©todos\n",
    "    fit y predict).\n",
    "    \n",
    "    Se exporta el mejor modelo obtenido con el nombre en './{nombre}_{nombre_dataset}.joblib'.\n",
    "    \n",
    "    La funci√≥n devuelve el mejor modelo obtenido en la b√∫squeda realizada, sus hiperpar√°metros su score\n",
    "    obtenido.\n",
    "    \n",
    "    Se tiene opci√≥n de paralelizar los entrenamientos en los n√∫cleos de la CPU.\n",
    "    \n",
    "    Pa√°metros:\n",
    "    ----------\n",
    "        nombre : str\n",
    "            nombre con el que se identifica al modelo. Se utilizar√° en el nombre de la variables\n",
    "            \n",
    "        config : dict\n",
    "            Diccionario con la configuraci√≥n del modelo. Debe tener los siguientes pares de clave - valor:\n",
    "                - 'modelo' : Instancia b√°sica del modelo a probar. Debe ser un objeto de sklearn, xgboost o\n",
    "                alguna librer√≠a af√≠n.\n",
    "                'params' : diccionario. Sus claves son los nombres de los hiperpar√°metros (dados en la\n",
    "                documentaci√≥n de la clase de clasificador / regresor que se utilice) y sus valores\n",
    "                corresponden a listas con los rangos o valores a ensayar.\n",
    "                    \n",
    "        X_train : pandas.DataFrame\n",
    "            Dataframe del conjunto entrenamiento (sin variable objetivo).\n",
    "        y_train : pandas.DataFrame | pandas.Serie\n",
    "            Dataframe con los valores de la variable objetivo del conjunto de entrenamiento.\n",
    "        X_val : pandas.DataFrame\n",
    "            Dataframe con las muestras del conjunto de validaci√≥n (sin variable objetivo).\n",
    "        y_val : pandas.DataFrame | pandas.Serie\n",
    "            Dataframe con los valores de la variable objetivo del conjunto de validaci√≥n.\n",
    "            \n",
    "        nombre_dataset : str\n",
    "            Nombre con el que se identifica al dataset. Se utilizar√° como sufijo en el nombre final del modelo\n",
    "            para identificar con qu√© dataset se entren√≥.\n",
    "            \n",
    "        metrica : str\n",
    "            M√©trica a utilizar. Consultar opciones v√°lidas en\n",
    "            https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-string-names. Esta funci√≥n\n",
    "            utiliza m√©tricas negativas.\n",
    "            \n",
    "        busqueda : str, default='random'\n",
    "            La forma en que se realizar√° la b√∫squeda de hiperpar√°metros √≥ptimos en el espacio de\n",
    "            hiperpar√°metros. Opciones:\n",
    "                - 'random' : random search\n",
    "                - 'grid' : grid search.\n",
    "                \n",
    "        random_state : int, default=42\n",
    "            Valor semilla para la elecci√≥n aleatoria en caso de usar random search.\n",
    "            Se fija para reproducibilidad.\n",
    "            \n",
    "        n_iter : int, default=20\n",
    "            Cantidad m√°xima de combinaciones de valores de hiperpar√°metros a probar en caso de que se use random\n",
    "            search.\n",
    "            \n",
    "        n_jobs : int, default=1\n",
    "            Cantidad de trabajos a correr en paralelo.\n",
    "            \n",
    "    Devuelve:\n",
    "    ---------\n",
    "        best_model : sklearn | xgboost model\n",
    "            M√≥delo con hiperpar√°metros √≥ptimos seg√∫n b√∫squeda realizada.\n",
    "        \n",
    "        best_params : dict\n",
    "            Diccionario con pares clave - valor dados por los nombres de los hiperpar√°metros ingresados en\n",
    "            config['params'] y sus valores √≥ptimos.\n",
    "            \n",
    "        best_score : float\n",
    "            Score obtenido por el modelo √≥ptimo (valor de la m√©trica sobre X_test).\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"üöÄ Iniciando entrenamiento para: {nombre}\")\n",
    "    \n",
    "    scorer = get_scorer(metrica)\n",
    "    \n",
    "    param_space = config[\"params\"]\n",
    "    all_param_combinations = list(itertools.product(*param_space.values()))\n",
    "    param_keys = list(param_space.keys())\n",
    "    \n",
    "    total_combinations = 1\n",
    "    for v in param_space.values():\n",
    "        total_combinations *= len(v)\n",
    "    n_iter_ajustado = min(8, total_combinations)\n",
    "\n",
    "    # Random o grid\n",
    "    if busqueda == 'grid':\n",
    "        candidates = all_param_combinations\n",
    "    elif busqueda == 'random':\n",
    "        random.seed(random_state)\n",
    "        candidates = random.sample(all_param_combinations,\n",
    "                                   min(n_iter, len(all_param_combinations)))\n",
    "    else:\n",
    "        raise ValueError(\"busqueda debe ser 'random' o 'grid'.\")\n",
    "        \n",
    "    # Funci√≥n auxiliar para entrenar y evaluar un modelo\n",
    "    def _fit_and_score(params):\n",
    "        param_dict = dict(zip(param_keys, params))\n",
    "        pipeline = Pipeline([('clf', config[\"modelo\"].set_params(**param_dict))])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        score = scorer(pipeline, X_val, y_val)\n",
    "        return param_dict, score, pipeline\n",
    "    \n",
    "    # Paralelizar entrenamiento\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(_fit_and_score)(params) for params in candidates\n",
    "    )\n",
    "        \n",
    "    # Seleccionar el mejor\n",
    "    best_params, best_score, best_model = max(results, key=lambda x: x[1])\n",
    "    best_score = -best_score\n",
    "            \n",
    "    print(f\"‚úÖ {nombre}: b√∫squeda finalizada.\")\n",
    "    print(f\"üèÜ Mejor score en validaci√≥n: {best_score:.4f}\")\n",
    "    print(f\"üìå Mejores hiperpar√°metros: {best_params}\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è Tiempo entrenando {nombre}: {elapsed:.2f} segundos\")\n",
    "\n",
    "    # Guardar modelo\n",
    "    dump(best_model, f\"./models/{nombre}_{nombre_dataset}.joblib\")\n",
    "\n",
    "    return best_model, best_params, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe985b",
   "metadata": {},
   "source": [
    "## Importo datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56964279",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importamos los datasets A\n",
    "\n",
    "# Etiquetas\n",
    "y_A_train = pd.read_csv(\"./A_sets/y_train_inicial.csv\")\n",
    "y_A_test = pd.read_csv(\"./A_sets/y_test_inicial.csv\")\n",
    "# Datos escaleados\n",
    "X_A_train_scaled = pd.read_csv(\"./A_sets/x_A_train_norm.csv\")\n",
    "X_A_test_scaled = pd.read_csv(\"./A_sets/x_A_test_norm.csv\")\n",
    "# Datos estandarizados\n",
    "X_A_train_std = pd.read_csv(\"./A_sets/x_A_train_std.csv\")\n",
    "X_A_test_std = pd.read_csv(\"./A_sets/x_A_test_std.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c4d27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importamos los datasets B\n",
    "\n",
    "# Etiquetas\n",
    "y_B_train = pd.read_csv(\"./B_sets/y_train_inicial.csv\")\n",
    "y_B_test = pd.read_csv(\"./B_sets/y_test_inicial.csv\")\n",
    "# Datos escaleados\n",
    "X_B_train_scaled = pd.read_csv(\"./B_sets/x_B_train_norm.csv\")\n",
    "X_B_test_scaled = pd.read_csv(\"./B_sets/x_B_test_norm.csv\")\n",
    "# Datos estandarizados\n",
    "X_B_train_std = pd.read_csv(\"./B_sets/x_B_train_std.csv\")\n",
    "X_B_test_std = pd.read_csv(\"./B_sets/x_B_test_std.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30640c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defino diccinario para iterar sobre los datasets\n",
    "\n",
    "datasets_dict = {\n",
    "    'A_scaled' : {'x_train':X_A_train_scaled, 'y_train':y_A_train, 'x_test':X_A_test_scaled, 'y_test':y_A_test},\n",
    "    'A_std' : {'x_train':X_A_train_std, 'y_train':y_A_train, 'x_test':X_A_test_std, 'y_test':y_A_test},\n",
    "    'B_scaled' : {'x_train':X_B_train_scaled, 'y_train':y_B_train, 'x_test':X_B_test_scaled, 'y_test':y_B_test},\n",
    "    'B_std' : {'x_train':X_B_train_std, 'y_train':y_B_train, 'x_test':X_B_test_std, 'y_test':y_B_test},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e460389",
   "metadata": {},
   "source": [
    "## Ejemplos de uso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2602ab",
   "metadata": {},
   "source": [
    "### Regresi√≥n polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0f82c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "polynomial_reg_config = {\n",
    "    \"modelo\": Pipeline([\n",
    "        (\"poly\", PolynomialFeatures()),\n",
    "        (\"linreg\", LinearRegression())\n",
    "    ]),\n",
    "    \"params\": {\n",
    "        \"poly__degree\": np.arange(1, 3),\n",
    "        \"poly__interaction_only\": [True, False],\n",
    "        \"poly__include_bias\": [True, False],\n",
    "        \"linreg__positive\": [True, False]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b25f001a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Dataset A_scaled\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: Regresor_Polinomico\n",
      "‚úÖ Regresor_Polinomico: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 10.4708\n",
      "üìå Mejores hiperpar√°metros: {'poly__degree': np.int64(1), 'poly__interaction_only': True, 'poly__include_bias': False, 'linreg__positive': False}\n",
      "‚è±Ô∏è Tiempo entrenando Regresor_Polinomico: 0.59 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset A_std\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: Regresor_Polinomico\n",
      "‚úÖ Regresor_Polinomico: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 10.4708\n",
      "üìå Mejores hiperpar√°metros: {'poly__degree': np.int64(1), 'poly__interaction_only': True, 'poly__include_bias': True, 'linreg__positive': False}\n",
      "‚è±Ô∏è Tiempo entrenando Regresor_Polinomico: 0.62 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_scaled\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: Regresor_Polinomico\n",
      "‚úÖ Regresor_Polinomico: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 10.5060\n",
      "üìå Mejores hiperpar√°metros: {'poly__degree': np.int64(1), 'poly__interaction_only': True, 'poly__include_bias': False, 'linreg__positive': False}\n",
      "‚è±Ô∏è Tiempo entrenando Regresor_Polinomico: 0.51 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_std\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: Regresor_Polinomico\n",
      "‚úÖ Regresor_Polinomico: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 10.5060\n",
      "üìå Mejores hiperpar√°metros: {'poly__degree': np.int64(1), 'poly__interaction_only': True, 'poly__include_bias': True, 'linreg__positive': False}\n",
      "‚è±Ô∏è Tiempo entrenando Regresor_Polinomico: 0.55 segundos\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultados = {}\n",
    "\n",
    "for set_conjuntos, set_dict in datasets_dict.items():\n",
    "    print(f'====================\\nDataset {set_conjuntos}\\n====================')\n",
    "    best_model, best_params, best_score = optimizador_hiperparametros(nombre='Regresor_Polinomico',\n",
    "                                                                      config=polynomial_reg_config,\n",
    "                                                                      X_train=set_dict['x_train'],\n",
    "                                                                      y_train=set_dict['y_train'],\n",
    "                                                                      X_val=set_dict['x_test'],\n",
    "                                                                      y_val=set_dict['y_test'],\n",
    "                                                                      nombre_dataset=set_conjuntos,\n",
    "                                                                      metrica='neg_root_mean_squared_error',\n",
    "                                                                      busqueda='random',\n",
    "                                                                      random_state=42,\n",
    "                                                                      n_iter=20,\n",
    "                                                                      n_jobs=4)\n",
    "    resultados[set_conjuntos] = {'mejor_modelo' : best_model,\n",
    "                                 'mejores_params' : best_params,\n",
    "                                 'mejor_score' : best_score\n",
    "                                }\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699dbaaa",
   "metadata": {},
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "beda0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "\n",
    "lasso_config = {\n",
    "    \"modelo\": Pipeline([\n",
    "        (\"poly\", PolynomialFeatures()),\n",
    "        (\"linreg\", Lasso(random_state=42))\n",
    "    ]),\n",
    "    \"params\": {\n",
    "        \"poly__degree\": np.arange(1, 6),\n",
    "        \"poly__interaction_only\": [True, False],\n",
    "        \"poly__include_bias\": [True, False],\n",
    "        \"linreg__positive\": [True, False],\n",
    "        \"linreg__alpha\": np.logspace(-4, 4, num=9, base=10)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "808c747f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Dataset A_scaled\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: LASSO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.727e+04, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.138e+04, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.015e+03, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.535e+03, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.709e+04, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.669e+04, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.667e+02, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LASSO: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 10.4202\n",
      "üìå Mejores hiperpar√°metros: {'poly__degree': np.int64(1), 'poly__interaction_only': False, 'poly__include_bias': True, 'linreg__positive': False, 'linreg__alpha': np.float64(0.01)}\n",
      "‚è±Ô∏è Tiempo entrenando LASSO: 376.89 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset A_std\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: LASSO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.366e+01, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.428e+03, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m set_conjuntos, set_dict \u001b[38;5;129;01min\u001b[39;00m datasets_dict.items():\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m====================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mset_conjuntos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m====================\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     best_model, best_params, best_score = \u001b[43moptimizador_hiperparametros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnombre\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLASSO\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlasso_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mx_train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43my_train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mx_test\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43my_test\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mnombre_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mset_conjuntos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mmetrica\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mneg_root_mean_squared_error\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mbusqueda\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrandom\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     resultados[set_conjuntos] = {\u001b[33m'\u001b[39m\u001b[33mmejor_modelo\u001b[39m\u001b[33m'\u001b[39m : best_model,\n\u001b[32m     18\u001b[39m                                  \u001b[33m'\u001b[39m\u001b[33mmejores_params\u001b[39m\u001b[33m'\u001b[39m : best_params,\n\u001b[32m     19\u001b[39m                                  \u001b[33m'\u001b[39m\u001b[33mmejor_score\u001b[39m\u001b[33m'\u001b[39m : best_score\n\u001b[32m     20\u001b[39m                                 }\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36moptimizador_hiperparametros\u001b[39m\u001b[34m(nombre, config, X_train, y_train, X_val, y_val, nombre_dataset, metrica, busqueda, random_state, n_iter, n_jobs)\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m param_dict, score, pipeline\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Paralelizar entrenamiento\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# Seleccionar el mejor\u001b[39;00m\n\u001b[32m    117\u001b[39m best_params, best_score, best_model = \u001b[38;5;28mmax\u001b[39m(results, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "resultados = {}\n",
    "\n",
    "for set_conjuntos, set_dict in datasets_dict.items():\n",
    "    print(f'====================\\nDataset {set_conjuntos}\\n====================')\n",
    "    best_model, best_params, best_score = optimizador_hiperparametros(nombre='LASSO',\n",
    "                                                                      config=lasso_config,\n",
    "                                                                      X_train=set_dict['x_train'],\n",
    "                                                                      y_train=set_dict['y_train'],\n",
    "                                                                      X_val=set_dict['x_test'],\n",
    "                                                                      y_val=set_dict['y_test'],\n",
    "                                                                      nombre_dataset=set_conjuntos,\n",
    "                                                                      metrica='neg_root_mean_squared_error',\n",
    "                                                                      busqueda='random',\n",
    "                                                                      random_state=42,\n",
    "                                                                      n_iter=20,\n",
    "                                                                      n_jobs=4)\n",
    "    resultados[set_conjuntos] = {'mejor_modelo' : best_model,\n",
    "                                 'mejores_params' : best_params,\n",
    "                                 'mejor_score' : best_score\n",
    "                                }\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86da4b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89dc8a9",
   "metadata": {},
   "source": [
    "#### Uso con XGBoost (espacio reducido, pruebas r√°pidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ec145a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_reducido_config = {\n",
    "    \"modelo\": XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbosity=0\n",
    "    ),\n",
    "    \"params\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"max_depth\": [3, 5],\n",
    "        \"learning_rate\": [0.05, 0.1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d59b5985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Dataset A_scaled\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: XGBoost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XGBoost: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 6.8887\n",
      "üìå Mejores hiperpar√°metros: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1}\n",
      "‚è±Ô∏è Tiempo entrenando XGBoost: 6.22 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset A_std\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: XGBoost\n",
      "‚úÖ XGBoost: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 6.8887\n",
      "üìå Mejores hiperpar√°metros: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1}\n",
      "‚è±Ô∏è Tiempo entrenando XGBoost: 1.69 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_scaled\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: XGBoost\n",
      "‚úÖ XGBoost: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 7.1646\n",
      "üìå Mejores hiperpar√°metros: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "‚è±Ô∏è Tiempo entrenando XGBoost: 1.69 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_std\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: XGBoost\n",
      "‚úÖ XGBoost: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 7.1646\n",
      "üìå Mejores hiperpar√°metros: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "‚è±Ô∏è Tiempo entrenando XGBoost: 1.54 segundos\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultados = {}\n",
    "\n",
    "for set_conjuntos, set_dict in datasets_dict.items():\n",
    "    print(f'====================\\nDataset {set_conjuntos}\\n====================')\n",
    "    best_model, best_params, best_score = optimizador_hiperparametros(nombre='XGBoost',\n",
    "                                                                      config=xgboost_reducido_config,\n",
    "                                                                      X_train=set_dict['x_train'],\n",
    "                                                                      y_train=set_dict['y_train'],\n",
    "                                                                      X_val=set_dict['x_test'],\n",
    "                                                                      y_val=set_dict['y_test'],\n",
    "                                                                      nombre_dataset=set_conjuntos,\n",
    "                                                                      metrica='neg_root_mean_squared_error',\n",
    "                                                                      busqueda='random',\n",
    "                                                                      random_state=42,\n",
    "                                                                      n_iter=20,\n",
    "                                                                      n_jobs=4)\n",
    "    resultados[set_conjuntos] = {'mejor_modelo': best_model,\n",
    "                                 'mejores_params': best_params,\n",
    "                                 'mejor_score': best_score\n",
    "                                }\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556f3ef",
   "metadata": {},
   "source": [
    "#### Uso con XGBoost (espacio completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4edba13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgboost_config = {\n",
    "    \"modelo\": XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "        n_jobs=1,  # el paralelismo lo controla la funci√≥n externa\n",
    "        verbosity=0\n",
    "    ),\n",
    "    \"params\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_depth\": [3, 5, 7, 9],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "        \"subsample\": [0.6, 0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "        \"reg_alpha\": [0, 0.1, 1, 10],   # L1 regularization\n",
    "        \"reg_lambda\": [1, 10, 50]       # L2 regularization\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2aad9f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Dataset A_scaled\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: XGBoost\n",
      "‚úÖ XGBoost: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 6.3140\n",
      "üìå Mejores hiperpar√°metros: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0, 'reg_alpha': 0.1, 'reg_lambda': 1}\n",
      "‚è±Ô∏è Tiempo entrenando XGBoost: 8.45 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset A_std\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: XGBoost\n",
      "‚úÖ XGBoost: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 6.3140\n",
      "üìå Mejores hiperpar√°metros: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0, 'reg_alpha': 0.1, 'reg_lambda': 1}\n",
      "‚è±Ô∏è Tiempo entrenando XGBoost: 8.58 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_scaled\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: XGBoost\n",
      "‚úÖ XGBoost: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 6.3687\n",
      "üìå Mejores hiperpar√°metros: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0, 'reg_alpha': 0.1, 'reg_lambda': 10}\n",
      "‚è±Ô∏è Tiempo entrenando XGBoost: 8.36 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_std\n",
      "====================\n",
      "üöÄ Iniciando entrenamiento para: XGBoost\n",
      "‚úÖ XGBoost: b√∫squeda finalizada.\n",
      "üèÜ Mejor score en validaci√≥n: 6.3687\n",
      "üìå Mejores hiperpar√°metros: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0, 'reg_alpha': 0.1, 'reg_lambda': 10}\n",
      "‚è±Ô∏è Tiempo entrenando XGBoost: 8.43 segundos\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultados = {}\n",
    "\n",
    "for set_conjuntos, set_dict in datasets_dict.items():\n",
    "    print(f'====================\\nDataset {set_conjuntos}\\n====================')\n",
    "    best_model, best_params, best_score = optimizador_hiperparametros(nombre='XGBoost',\n",
    "                                                                      config=xgboost_config,\n",
    "                                                                      X_train=set_dict['x_train'],\n",
    "                                                                      y_train=set_dict['y_train'],\n",
    "                                                                      X_val=set_dict['x_test'],\n",
    "                                                                      y_val=set_dict['y_test'],\n",
    "                                                                      nombre_dataset=set_conjuntos,\n",
    "                                                                      metrica='neg_root_mean_squared_error',\n",
    "                                                                      busqueda='random',\n",
    "                                                                      random_state=42,\n",
    "                                                                      n_iter=20,\n",
    "                                                                      n_jobs=4)\n",
    "    resultados[set_conjuntos] = {'mejor_modelo': best_model,\n",
    "                                 'mejores_params': best_params,\n",
    "                                 'mejor_score': best_score\n",
    "                                }\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diplodatos_M07_G02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

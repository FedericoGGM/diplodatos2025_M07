{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e342167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from joblib import Parallel, delayed, dump\n",
    "import multiprocessing\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import get_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b8d035",
   "metadata": {},
   "source": [
    "**La siguiente celda sirve para ver la cantidad de nÃºcleos en la CPU**, para que sepan en cuantos trabajos dividir las tareas. La idea es realizar tantos procesos paralelos como nÃºcleos tenga nuestra CPU (el dataset es chico, asique dudo que haya problemas con la RAM, pero si nos quedamos sin RAM, hay que paralelizar en menos procesos que la cantidad de nÃºcleos, o no hacerlo en absoluto). Esto es para que podamos hacer bÃºsquedas mÃ¡s exhaustivas en menos tiempo. Entiendo que scikit-learn no aprovecha GPU (segÃºn chat GPT), pero XGBoost si ğŸ‘€ (@fede, pero si lo querÃ©s aprovechar tendrÃ­as que cambiar la funciÃ³n, e indicarlo en un argumento al instanciar la clase$^1$).\n",
    "\n",
    "$^1$ no entiendo mucho de GPUs, capaz vos entendÃ©s mÃ¡s pero comparto lo que leÃ­ por si sirve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55bbf0f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU cores: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available CPU cores: {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11542275",
   "metadata": {},
   "source": [
    "## FunciÃ³n para entrenar y evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db5d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizador_hiperparametros(nombre, config, X_train, y_train, X_val, y_val, nombre_dataset, metrica,\n",
    "                                busqueda='random', random_state=42, n_iter=20, n_jobs=1):\n",
    "    \"\"\"\n",
    "    FunciÃ³n para optimizar los hiperparÃ¡metros de un modelo dado sobre el conjunto\n",
    "    entrenamiento. La evaluaciÃ³n para decidir el mejor modelo se realiza sobre el conjunto de validaciÃ³n\n",
    "    (no se hace cross validation). La bÃºsqueda en el espacio dehiperparÃ¡metros puede realizarse con random\n",
    "    search o grid search.\n",
    "    \n",
    "    Esta funciÃ³n trabaja con clases de modelos de sklearn, xgboost o similares (deben soportar los mÃ©todos\n",
    "    fit y predict).\n",
    "    \n",
    "    Se exporta el mejor modelo obtenido con el nombre en './{nombre}_{nombre_dataset}.joblib'.\n",
    "    \n",
    "    La funciÃ³n devuelve el mejor modelo obtenido en la bÃºsqueda realizada, sus hiperparÃ¡metros su score\n",
    "    obtenido.\n",
    "    \n",
    "    Se tiene opciÃ³n de paralelizar los entrenamientos en los nÃºcleos de la CPU.\n",
    "    \n",
    "    PaÃ¡metros:\n",
    "    ----------\n",
    "        nombre : str\n",
    "            nombre con el que se identifica al modelo. Se utilizarÃ¡ en el nombre de la variables\n",
    "            \n",
    "        config : dict\n",
    "            Diccionario con la configuraciÃ³n del modelo. Debe tener los siguientes pares de clave - valor:\n",
    "                - 'modelo' : Instancia bÃ¡sica del modelo a probar. Debe ser un objeto de sklearn, xgboost o\n",
    "                alguna librerÃ­a afÃ­n.\n",
    "                'params' : diccionario. Sus claves son los nombres de los hiperparÃ¡metros (dados en la\n",
    "                documentaciÃ³n de la clase de clasificador / regresor que se utilice) y sus valores\n",
    "                corresponden a listas con los rangos o valores a ensayar.\n",
    "                    \n",
    "        X_train : pandas.DataFrame\n",
    "            Dataframe del conjunto entrenamiento (sin variable objetivo).\n",
    "        y_train : pandas.DataFrame | pandas.Serie\n",
    "            Dataframe con los valores de la variable objetivo del conjunto de entrenamiento.\n",
    "        X_val : pandas.DataFrame\n",
    "            Dataframe con las muestras del conjunto de validaciÃ³n (sin variable objetivo).\n",
    "        y_val : pandas.DataFrame | pandas.Serie\n",
    "            Dataframe con los valores de la variable objetivo del conjunto de validaciÃ³n.\n",
    "            \n",
    "        nombre_dataset : str\n",
    "            Nombre con el que se identifica al dataset. Se utilizarÃ¡ como sufijo en el nombre final del modelo\n",
    "            para identificar con quÃ© dataset se entrenÃ³.\n",
    "            \n",
    "        metrica : str\n",
    "            MÃ©trica a utilizar. Consultar opciones vÃ¡lidas en\n",
    "            https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-string-names. Esta funciÃ³n\n",
    "            utiliza mÃ©tricas negativas.\n",
    "            \n",
    "        busqueda : str, default='random'\n",
    "            La forma en que se realizarÃ¡ la bÃºsqueda de hiperparÃ¡metros Ã³ptimos en el espacio de\n",
    "            hiperparÃ¡metros. Opciones:\n",
    "                - 'random' : random search\n",
    "                - 'grid' : grid search.\n",
    "                \n",
    "        random_state : int, default=42\n",
    "            Valor semilla para la elecciÃ³n aleatoria en caso de usar random search.\n",
    "            Se fija para reproducibilidad.\n",
    "            \n",
    "        n_iter : int, default=20\n",
    "            Cantidad mÃ¡xima de combinaciones de valores de hiperparÃ¡metros a probar en caso de que se use random\n",
    "            search.\n",
    "            \n",
    "        n_jobs : int, default=1\n",
    "            Cantidad de trabajos a correr en paralelo.\n",
    "            \n",
    "    Devuelve:\n",
    "    ---------\n",
    "        best_model : sklearn | xgboost model\n",
    "            MÃ³delo con hiperparÃ¡metros Ã³ptimos segÃºn bÃºsqueda realizada.\n",
    "        \n",
    "        best_params : dict\n",
    "            Diccionario con pares clave - valor dados por los nombres de los hiperparÃ¡metros ingresados en\n",
    "            config['params'] y sus valores Ã³ptimos.\n",
    "            \n",
    "        best_score : float\n",
    "            Score obtenido por el modelo Ã³ptimo (valor de la mÃ©trica sobre X_test).\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"ğŸš€ Iniciando entrenamiento para: {nombre}\")\n",
    "    \n",
    "    scorer = get_scorer(metrica)\n",
    "    \n",
    "    param_space = config[\"params\"]\n",
    "    all_param_combinations = list(itertools.product(*param_space.values()))\n",
    "    param_keys = list(param_space.keys())\n",
    "    \n",
    "    total_combinations = 1\n",
    "    for v in param_space.values():\n",
    "        total_combinations *= len(v)\n",
    "    n_iter_ajustado = min(8, total_combinations)\n",
    "\n",
    "    # Random o grid\n",
    "    if busqueda == 'grid':\n",
    "        candidates = all_param_combinations\n",
    "    elif busqueda == 'random':\n",
    "        random.seed(random_state)\n",
    "        candidates = random.sample(all_param_combinations,\n",
    "                                   min(n_iter, len(all_param_combinations)))\n",
    "    else:\n",
    "        raise ValueError(\"busqueda debe ser 'random' o 'grid'.\")\n",
    "        \n",
    "    # FunciÃ³n auxiliar para entrenar y evaluar un modelo\n",
    "    def _fit_and_score(params):\n",
    "        param_dict = dict(zip(param_keys, params))\n",
    "        pipeline = Pipeline([('clf', config[\"modelo\"].set_params(**param_dict))])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        score = scorer(pipeline, X_val, y_val)\n",
    "        return param_dict, score, pipeline\n",
    "    \n",
    "    # Paralelizar entrenamiento\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(_fit_and_score)(params) for params in candidates\n",
    "    )\n",
    "        \n",
    "    # Seleccionar el mejor\n",
    "    best_params, best_score, best_model = max(results, key=lambda x: x[1])\n",
    "    best_score = -best_score\n",
    "            \n",
    "    print(f\"âœ… {nombre}: bÃºsqueda finalizada.\")\n",
    "    print(f\"ğŸ† Mejor score en validaciÃ³n: {best_score:.4f}\")\n",
    "    print(f\"ğŸ“Œ Mejores hiperparÃ¡metros: {best_params}\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"â±ï¸ Tiempo entrenando {nombre}: {elapsed:.2f} segundos\")\n",
    "\n",
    "    # Guardar modelo\n",
    "    dump(best_model, f\"./models/{nombre}_{nombre_dataset}.joblib\")\n",
    "\n",
    "    return best_model, best_params, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe985b",
   "metadata": {},
   "source": [
    "## Importo datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56964279",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importamos los datasets A\n",
    "\n",
    "# Etiquetas\n",
    "y_A_train = pd.read_csv(\"./A_sets/y_train_inicial.csv\")\n",
    "y_A_test = pd.read_csv(\"./A_sets/y_test_inicial.csv\")\n",
    "# Datos escaleados\n",
    "X_A_train_scaled = pd.read_csv(\"./A_sets/x_A_train_norm.csv\")\n",
    "X_A_test_scaled = pd.read_csv(\"./A_sets/x_A_test_norm.csv\")\n",
    "# Datos estandarizados\n",
    "X_A_train_std = pd.read_csv(\"./A_sets/x_A_train_std.csv\")\n",
    "X_A_test_std = pd.read_csv(\"./A_sets/x_A_test_std.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4d27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importamos los datasets B\n",
    "\n",
    "# Etiquetas\n",
    "y_B_train = pd.read_csv(\"./B_sets/y_train_inicial.csv\")\n",
    "y_B_test = pd.read_csv(\"./B_sets/y_test_inicial.csv\")\n",
    "# Datos escaleados\n",
    "X_B_train_scaled = pd.read_csv(\"./B_sets/x_B_train_norm.csv\")\n",
    "X_B_test_scaled = pd.read_csv(\"./B_sets/x_B_test_norm.csv\")\n",
    "# Datos estandarizados\n",
    "X_B_train_std = pd.read_csv(\"./B_sets/x_B_train_std.csv\")\n",
    "X_B_test_std = pd.read_csv(\"./B_sets/x_B_test_std.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30640c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defino diccinario para iterar sobre los datasets\n",
    "\n",
    "datasets_dict = {\n",
    "    'A_scaled' : {'x_train':X_A_train_scaled, 'y_train':y_A_train, 'x_test':X_A_test_scaled, 'y_test':y_A_test},\n",
    "    'A_std' : {'x_train':X_A_train_std, 'y_train':y_A_train, 'x_test':X_A_test_std, 'y_test':y_A_test},\n",
    "    'B_scaled' : {'x_train':X_B_train_scaled, 'y_train':y_B_train, 'x_test':X_B_test_scaled, 'y_test':y_B_test},\n",
    "    'B_std' : {'x_train':X_B_train_std, 'y_train':y_B_train, 'x_test':X_B_test_std, 'y_test':y_B_test},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e460389",
   "metadata": {},
   "source": [
    "---\n",
    "## Ejemplos de uso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2602ab",
   "metadata": {},
   "source": [
    "### RegresiÃ³n polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0f82c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "polynomial_reg_config = {\n",
    "    \"modelo\": Pipeline([\n",
    "        (\"poly\", PolynomialFeatures()),\n",
    "        (\"linreg\", LinearRegression())\n",
    "    ]),\n",
    "    \"params\": {\n",
    "        \"poly__degree\": np.arange(1, 3),\n",
    "        \"poly__interaction_only\": [True, False],\n",
    "        \"poly__include_bias\": [True, False],\n",
    "        \"linreg__positive\": [True, False]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b25f001a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Dataset A_scaled\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: Regresor_Polinomico\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Regresor_Polinomico: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 10.4708\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'poly__degree': np.int64(1), 'poly__interaction_only': True, 'poly__include_bias': False, 'linreg__positive': False}\n",
      "â±ï¸ Tiempo entrenando Regresor_Polinomico: 6.26 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset A_std\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: Regresor_Polinomico\n",
      "âœ… Regresor_Polinomico: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 10.4708\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'poly__degree': np.int64(1), 'poly__interaction_only': True, 'poly__include_bias': True, 'linreg__positive': False}\n",
      "â±ï¸ Tiempo entrenando Regresor_Polinomico: 0.81 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_scaled\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: Regresor_Polinomico\n",
      "âœ… Regresor_Polinomico: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 10.5060\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'poly__degree': np.int64(1), 'poly__interaction_only': True, 'poly__include_bias': False, 'linreg__positive': False}\n",
      "â±ï¸ Tiempo entrenando Regresor_Polinomico: 0.51 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_std\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: Regresor_Polinomico\n",
      "âœ… Regresor_Polinomico: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 10.5060\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'poly__degree': np.int64(1), 'poly__interaction_only': True, 'poly__include_bias': True, 'linreg__positive': False}\n",
      "â±ï¸ Tiempo entrenando Regresor_Polinomico: 0.65 segundos\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultados = {}\n",
    "\n",
    "for set_conjuntos, set_dict in datasets_dict.items():\n",
    "    print(f'====================\\nDataset {set_conjuntos}\\n====================')\n",
    "    best_model, best_params, best_score = optimizador_hiperparametros(nombre='Regresor_Polinomico',\n",
    "                                                                      config=polynomial_reg_config,\n",
    "                                                                      X_train=set_dict['x_train'],\n",
    "                                                                      y_train=set_dict['y_train'],\n",
    "                                                                      X_val=set_dict['x_test'],\n",
    "                                                                      y_val=set_dict['y_test'],\n",
    "                                                                      nombre_dataset=set_conjuntos,\n",
    "                                                                      metrica='neg_root_mean_squared_error',\n",
    "                                                                      busqueda='random',\n",
    "                                                                      random_state=42,\n",
    "                                                                      n_iter=20,\n",
    "                                                                      n_jobs=4)\n",
    "    resultados[set_conjuntos] = {'mejor_modelo' : best_model,\n",
    "                                 'mejores_params' : best_params,\n",
    "                                 'mejor_score' : best_score\n",
    "                                }\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699dbaaa",
   "metadata": {},
   "source": [
    "---\n",
    "### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "beda0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "\n",
    "lasso_config = {\n",
    "    \"modelo\": Pipeline([\n",
    "        (\"poly\", PolynomialFeatures()),\n",
    "        (\"linreg\", Lasso(random_state=42))\n",
    "    ]),\n",
    "    \"params\": {\n",
    "        \"poly__degree\": np.arange(1, 6),\n",
    "        \"poly__interaction_only\": [True, False],\n",
    "        \"poly__include_bias\": [True, False],\n",
    "        \"linreg__positive\": [True, False],\n",
    "        \"linreg__alpha\": np.logspace(-4, 4, num=9, base=10)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "808c747f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Dataset A_scaled\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: LASSO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.727e+04, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.138e+04, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.015e+03, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.535e+03, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.709e+04, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.669e+04, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.667e+02, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LASSO: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 10.4202\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'poly__degree': np.int64(1), 'poly__interaction_only': False, 'poly__include_bias': True, 'linreg__positive': False, 'linreg__alpha': np.float64(0.01)}\n",
      "â±ï¸ Tiempo entrenando LASSO: 376.89 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset A_std\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: LASSO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.366e+01, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/fgutierrez/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.428e+03, tolerance: 4.981e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m set_conjuntos, set_dict \u001b[38;5;129;01min\u001b[39;00m datasets_dict.items():\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m====================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mset_conjuntos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m====================\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     best_model, best_params, best_score = \u001b[43moptimizador_hiperparametros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnombre\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLASSO\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlasso_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mx_train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43my_train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mx_test\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43my_test\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mnombre_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mset_conjuntos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mmetrica\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mneg_root_mean_squared_error\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mbusqueda\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrandom\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                                                                      \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     resultados[set_conjuntos] = {\u001b[33m'\u001b[39m\u001b[33mmejor_modelo\u001b[39m\u001b[33m'\u001b[39m : best_model,\n\u001b[32m     18\u001b[39m                                  \u001b[33m'\u001b[39m\u001b[33mmejores_params\u001b[39m\u001b[33m'\u001b[39m : best_params,\n\u001b[32m     19\u001b[39m                                  \u001b[33m'\u001b[39m\u001b[33mmejor_score\u001b[39m\u001b[33m'\u001b[39m : best_score\n\u001b[32m     20\u001b[39m                                 }\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36moptimizador_hiperparametros\u001b[39m\u001b[34m(nombre, config, X_train, y_train, X_val, y_val, nombre_dataset, metrica, busqueda, random_state, n_iter, n_jobs)\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m param_dict, score, pipeline\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Paralelizar entrenamiento\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# Seleccionar el mejor\u001b[39;00m\n\u001b[32m    117\u001b[39m best_params, best_score, best_model = \u001b[38;5;28mmax\u001b[39m(results, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/diplodatos_M07_G02/lib/python3.11/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "resultados = {}\n",
    "\n",
    "for set_conjuntos, set_dict in datasets_dict.items():\n",
    "    print(f'====================\\nDataset {set_conjuntos}\\n====================')\n",
    "    best_model, best_params, best_score = optimizador_hiperparametros(nombre='LASSO',\n",
    "                                                                      config=lasso_config,\n",
    "                                                                      X_train=set_dict['x_train'],\n",
    "                                                                      y_train=set_dict['y_train'],\n",
    "                                                                      X_val=set_dict['x_test'],\n",
    "                                                                      y_val=set_dict['y_test'],\n",
    "                                                                      nombre_dataset=set_conjuntos,\n",
    "                                                                      metrica='neg_root_mean_squared_error',\n",
    "                                                                      busqueda='random',\n",
    "                                                                      random_state=42,\n",
    "                                                                      n_iter=20,\n",
    "                                                                      n_jobs=4)\n",
    "    resultados[set_conjuntos] = {'mejor_modelo' : best_model,\n",
    "                                 'mejores_params' : best_params,\n",
    "                                 'mejor_score' : best_score\n",
    "                                }\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86da4b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89dc8a9",
   "metadata": {},
   "source": [
    "#### Uso con XGBoost (espacio reducido, pruebas rÃ¡pidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ec145a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_reducido_config = {\n",
    "    \"modelo\": XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbosity=0\n",
    "    ),\n",
    "    \"params\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"max_depth\": [3, 5],\n",
    "        \"learning_rate\": [0.05, 0.1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d59b5985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Dataset A_scaled\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: XGBoost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… XGBoost: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 6.8887\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1}\n",
      "â±ï¸ Tiempo entrenando XGBoost: 6.22 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset A_std\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: XGBoost\n",
      "âœ… XGBoost: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 6.8887\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1}\n",
      "â±ï¸ Tiempo entrenando XGBoost: 1.69 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_scaled\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: XGBoost\n",
      "âœ… XGBoost: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 7.1646\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "â±ï¸ Tiempo entrenando XGBoost: 1.69 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_std\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: XGBoost\n",
      "âœ… XGBoost: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 7.1646\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "â±ï¸ Tiempo entrenando XGBoost: 1.54 segundos\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultados = {}\n",
    "\n",
    "for set_conjuntos, set_dict in datasets_dict.items():\n",
    "    print(f'====================\\nDataset {set_conjuntos}\\n====================')\n",
    "    best_model, best_params, best_score = optimizador_hiperparametros(nombre='XGBoost',\n",
    "                                                                      config=xgboost_reducido_config,\n",
    "                                                                      X_train=set_dict['x_train'],\n",
    "                                                                      y_train=set_dict['y_train'],\n",
    "                                                                      X_val=set_dict['x_test'],\n",
    "                                                                      y_val=set_dict['y_test'],\n",
    "                                                                      nombre_dataset=set_conjuntos,\n",
    "                                                                      metrica='neg_root_mean_squared_error',\n",
    "                                                                      busqueda='random',\n",
    "                                                                      random_state=42,\n",
    "                                                                      n_iter=20,\n",
    "                                                                      n_jobs=4)\n",
    "    resultados[set_conjuntos] = {'mejor_modelo': best_model,\n",
    "                                 'mejores_params': best_params,\n",
    "                                 'mejor_score': best_score\n",
    "                                }\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556f3ef",
   "metadata": {},
   "source": [
    "#### Uso con XGBoost (espacio completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4edba13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgboost_config = {\n",
    "    \"modelo\": XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "        n_jobs=1,  # el paralelismo lo controla la funciÃ³n externa\n",
    "        verbosity=0\n",
    "    ),\n",
    "    \"params\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_depth\": [3, 5, 7, 9],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "        \"subsample\": [0.6, 0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "        \"reg_alpha\": [0, 0.1, 1, 10],   # L1 regularization\n",
    "        \"reg_lambda\": [1, 10, 50]       # L2 regularization\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2aad9f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Dataset A_scaled\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: XGBoost\n",
      "âœ… XGBoost: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 6.3140\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0, 'reg_alpha': 0.1, 'reg_lambda': 1}\n",
      "â±ï¸ Tiempo entrenando XGBoost: 8.45 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset A_std\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: XGBoost\n",
      "âœ… XGBoost: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 6.3140\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0, 'reg_alpha': 0.1, 'reg_lambda': 1}\n",
      "â±ï¸ Tiempo entrenando XGBoost: 8.58 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_scaled\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: XGBoost\n",
      "âœ… XGBoost: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 6.3687\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0, 'reg_alpha': 0.1, 'reg_lambda': 10}\n",
      "â±ï¸ Tiempo entrenando XGBoost: 8.36 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_std\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: XGBoost\n",
      "âœ… XGBoost: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 6.3687\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0, 'reg_alpha': 0.1, 'reg_lambda': 10}\n",
      "â±ï¸ Tiempo entrenando XGBoost: 8.43 segundos\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultados = {}\n",
    "\n",
    "for set_conjuntos, set_dict in datasets_dict.items():\n",
    "    print(f'====================\\nDataset {set_conjuntos}\\n====================')\n",
    "    best_model, best_params, best_score = optimizador_hiperparametros(nombre='XGBoost',\n",
    "                                                                      config=xgboost_config,\n",
    "                                                                      X_train=set_dict['x_train'],\n",
    "                                                                      y_train=set_dict['y_train'],\n",
    "                                                                      X_val=set_dict['x_test'],\n",
    "                                                                      y_val=set_dict['y_test'],\n",
    "                                                                      nombre_dataset=set_conjuntos,\n",
    "                                                                      metrica='neg_root_mean_squared_error',\n",
    "                                                                      busqueda='random',\n",
    "                                                                      random_state=42,\n",
    "                                                                      n_iter=20,\n",
    "                                                                      n_jobs=4)\n",
    "    resultados[set_conjuntos] = {'mejor_modelo': best_model,\n",
    "                                 'mejores_params': best_params,\n",
    "                                 'mejor_score': best_score\n",
    "                                }\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150908e4",
   "metadata": {},
   "source": [
    "---\n",
    "### Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a6c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "red_neuronal_config = {\n",
    "    \"modelo\": MLPRegressor(random_state=42, max_iter=1000),\n",
    "    \"params\": {\n",
    "        \"hidden_layer_sizes\": [(32,), (64,), (64, 32), (64, 32, 16), (128, 64, 32, 16), (128, 64, 32)],\n",
    "        \"activation\": [\"relu\"],\n",
    "        \"solver\": [\"adam\"],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d4c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_nn = {}\n",
    "\n",
    "for set_conjuntos, set_dict in datasets_dict.items():\n",
    "    print(f'====================\\nDataset {set_conjuntos}\\n====================')\n",
    "    # Convertimos y a 1D para evitar el warning\n",
    "    y_train = set_dict['y_train'].values.ravel()\n",
    "    y_test = set_dict['y_test'].values.ravel()\n",
    "    best_model_nn, best_params_nn, best_score_nn = optimizador_hiperparametros(nombre='Neural_Network',\n",
    "                                                                      config=red_neuronal_config,\n",
    "                                                                      X_train=set_dict['x_train'],\n",
    "                                                                      y_train=set_dict['y_train'],\n",
    "                                                                      X_val=set_dict['x_test'],\n",
    "                                                                      y_val=set_dict['y_test'],\n",
    "                                                                      nombre_dataset=set_conjuntos,\n",
    "                                                                      metrica='neg_root_mean_squared_error',\n",
    "                                                                      busqueda='grid',\n",
    "                                                                      random_state=42,\n",
    "                                                                      n_iter=20,\n",
    "                                                                      n_jobs=multiprocessing.cpu_count())\n",
    "    resultados_nn[set_conjuntos] = {'mejor_modelo' : best_model_nn,\n",
    "                                 'mejores_params' : best_params_nn,\n",
    "                                 'mejor_score' : best_score_nn\n",
    "                                }\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55aabda",
   "metadata": {},
   "source": [
    "---\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1415b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_config = {\n",
    "    \"modelo\": RandomForestRegressor(random_state=42),\n",
    "    \"params\": {\n",
    "        \"n_estimators\": [100, 500, 800],\n",
    "        \"max_depth\": [None, 5, 10],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e94d17cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Dataset A_scaled\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: RandomForest\n",
      "âœ… RandomForest: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 6.9764\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'n_estimators': 800, 'max_depth': None, 'min_samples_split': 2}\n",
      "â±ï¸ Tiempo entrenando RandomForest: 51.00 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset A_std\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: RandomForest\n",
      "âœ… RandomForest: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 6.9902\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'n_estimators': 800, 'max_depth': None, 'min_samples_split': 2}\n",
      "â±ï¸ Tiempo entrenando RandomForest: 50.11 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_scaled\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: RandomForest\n",
      "âœ… RandomForest: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 6.9717\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'n_estimators': 800, 'max_depth': None, 'min_samples_split': 2}\n",
      "â±ï¸ Tiempo entrenando RandomForest: 49.28 segundos\n",
      "\n",
      "\n",
      "====================\n",
      "Dataset B_std\n",
      "====================\n",
      "ğŸš€ Iniciando entrenamiento para: RandomForest\n",
      "âœ… RandomForest: bÃºsqueda finalizada.\n",
      "ğŸ† Mejor score en validaciÃ³n: 6.9578\n",
      "ğŸ“Œ Mejores hiperparÃ¡metros: {'n_estimators': 800, 'max_depth': None, 'min_samples_split': 2}\n",
      "â±ï¸ Tiempo entrenando RandomForest: 49.20 segundos\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultados = {}\n",
    "\n",
    "for set_conjuntos, set_dict in datasets_dict.items():\n",
    "    print(f'====================\\nDataset {set_conjuntos}\\n====================')\n",
    "    best_model, best_params, best_score = optimizador_hiperparametros(\n",
    "                                              nombre='RandomForest',\n",
    "                                              config=rf_config,\n",
    "                                              X_train=set_dict['x_train'],\n",
    "                                              y_train=set_dict['y_train'].values.ravel(),\n",
    "                                              X_val=set_dict['x_test'],\n",
    "                                              y_val=set_dict['y_test'].values.ravel(),\n",
    "                                              nombre_dataset=set_conjuntos,\n",
    "                                              metrica='neg_root_mean_squared_error',\n",
    "                                              busqueda='grid',   # grid para probar todas las combinaciones\n",
    "                                              random_state=42,\n",
    "                                              n_iter=20,\n",
    "                                              n_jobs=4)\n",
    "    resultados[set_conjuntos] = {\n",
    "        'mejor_modelo' : best_model,\n",
    "        'mejores_params' : best_params,\n",
    "        'mejor_score' : best_score\n",
    "    }\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diplodatos_M07_G02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

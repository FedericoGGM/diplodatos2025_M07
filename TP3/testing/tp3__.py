# -*- coding: utf-8 -*-
"""TP3-_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Gge4Ch_HQK9xCaDXSljzikYAw2bcA3Z

# **El Impacto De Las Condiciones Ambientales En La Calidad Del Agua Del Río De La Plata**

**TP2 - TP3 **

Bibliografia:
-  Intendencia de Montevideo. (2022). Estudio de la calidad de agua del Río de la Plata: Sistema Oeste, agosto–diciembre 2021. Montevideo: Servicio de Evaluación de la Calidad y Control Ambiental. Recuperado de https://montevideo.gub.uy/sites/default/files/biblioteca/informeestudiodelacalidadaguadelriodelaplatasistemaoesteagosto-diciembre2021.pdf

- Intendencia de Montevideo. (2015). Estudio de línea de base del Río de la Plata [Informe técnico]. Montevideo: Intendencia de Montevideo, Departamento de Calidad Ambiental. Recuperado de
 https://www.montevideo.gub.uy/sites/default/files/biblioteca/informeestudiodelineadebaseriodelaplata2015final.pdf

###Libreria
"""

#Librerias

import pandas as pd
import unicodedata
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import folium
import ee
import geemap
import datetime
import re
import matplotlib.pyplot as plt
from tqdm import tqdm
from IPython.display import display, HTML
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from scipy.optimize import curve_fit
from sklearn.metrics import r2_score
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
from datetime import datetime
from folium import Map, Marker, LayerControl, TileLayer, features
from branca.element import MacroElement
from jinja2 import Template
from scipy.stats import pearsonr, spearmanr
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import make_scorer, r2_score, mean_squared_error
from datetime import timedelta
from tqdm.notebook import tqdm

"""####Dataset"""

# Cargar el dataset

url = "https://raw.githubusercontent.com/MaricelSantos/Mentoria--Diplodatos-2025/main/Conexiones_Transparentes.csv"
df = pd.read_csv(url)

"""#Variable objetivo

Se definió la variable clorofila_a_ug_l como objetivo del modelo, por ser un indicador ambiental clave vinculado a la calidad del agua. Esta variable es continua.  

**La medición de la clorofila en el agua es importante para evaluar la calidad del agua y la salud del ecosistema, ya que niveles altos de clorofila pueden indicar eutrofización y desequilibrios ecológicos.**

Otras variables complementarias fue campaña, fecha, turbidez del agua, temperatura

## *Resumen de descripción de la variable clorofila_a_ug_l*
"""

# Filtrar solo registros con clorofila válida, pero conservar todas las columnas
df_validos = df[df['clorofila_a_ug_l'].notna()].copy()

df_validos['clorofila'] = pd.to_numeric(df_validos['clorofila_a_ug_l'], errors='coerce')

# === 1) Trabajar sobre el texto original (antes de convertir a número) ===
s_raw = df['clorofila_a_ug_l'].astype('string')  # preserva NA de pandas

def _normalize(x: pd.Series) -> pd.Series:
    y = x.fillna('')
    # quitar tildes
    y = y.apply(lambda t: unicodedata.normalize('NFKD', t).encode('ascii', 'ignore').decode('ascii'))
    # minúscula, trim y colapsar espacios
    y = y.str.lower().str.strip().str.replace(r'\s+', ' ', regex=True)
    return y

s_norm = _normalize(s_raw)

# === 2) Máscaras por categoría ===
# < número (acepta coma o punto y espacios)
mask_lt = s_norm.str.contains(r'^\s*<\s*\d+(?:[.,]\d+)?\s*$', na=False)
# > número
mask_gt = s_norm.str.contains(r'^\s*>\s*\d+(?:[.,]\d+)?\s*$', na=False)
# "no se midio / no se midió" (ya normalizado sin tildes)
mask_no_midio = s_norm.str.contains(r'\bno\s*se\s*midio\b', na=False)
# Otros textos de faltante (si querés ampliar, agregá más términos dentro del grupo)
mask_sin_dato = s_norm.str_contains if hasattr(s_norm, 'str_contains') else s_norm.str.contains
mask_sin_dato = s_norm.str.contains(r'\b(?:sin dato|no medido|no medidos|no disponible|nd)\b', na=False)

# Subcategorías observadas en tus datos (para no mezclar todo como "mal cargado")
mask_inaccesible = s_norm.str.contains(r'\binaccesible\b', na=False)
mask_no_muestreo = s_norm.str.contains(r'\bno\s*(?:se\s*)?muestreo\b', na=False) | s_norm.str.contains(r'\bno\s*muestreo\b', na=False) | s_norm.str.contains(r'\bno\s*muestreo\b', na=False) | s_norm.str.contains(r'\bno\s*muestreo\b', na=False)
# mejor regex robusto para "no muestreó / no se muestreó"
mask_no_muestreo = s_norm.str.contains(r'\bno\s*(?:se\s*)?muestreo\b', na=False) | s_norm.str.contains(r'\bno\s*(?:se\s*)?m(ue|ue)streo\b', na=False) | s_norm.str.contains(r'\bno\s*(?:se\s*)?muestreo\b', na=False)
# Simplificamos con una expresión más general:
mask_no_muestreo = s_norm.str.contains(r'\bno\s*(?:se\s*)?muestr(e|eo|e[oó])\w*\b', na=False)

mask_en_obra = s_norm.str.contains(r'\ben obra\b', na=False)
mask_no_midieron_dia = s_norm.str.contains(r'\bno\s*midieron\s*este\s*dia\b', na=False)

# === 3) Conteos base ===
total_valores = len(s_raw)
valores_nulos = s_raw.isna().sum()
valores_con_lt = mask_lt.sum()
valores_con_gt = mask_gt.sum()
valores_no_se_midio = mask_no_midio.sum()
valores_otro_texto_faltante = mask_sin_dato.sum()

# Subcategorías
c_inaccesible = mask_inaccesible.sum()
c_no_muestreo = mask_no_muestreo.sum()
c_en_obra = mask_en_obra.sum()
c_no_midieron_dia = mask_no_midieron_dia.sum()

# Identificar números válidos (para aislar textos que no son número)
es_numero_valido = pd.to_numeric(s_norm.str.replace(',', '.', regex=False), errors='coerce').notna()

# Texto no vacío:
mask_texto = ~es_numero_valido & s_norm.ne('')

# Conjunto de categorías textuales ya identificadas
mask_categorizado = (
    mask_lt | mask_gt | mask_no_midio | mask_sin_dato |
    mask_inaccesible | mask_no_muestreo | mask_en_obra | mask_no_midieron_dia
)

# Resto de textos no categorizados (mal cargados)
valores_mal_cargados = (mask_texto & ~mask_categorizado).sum()

# === 4) Resumen ===
resumen = pd.DataFrame({
    'Descripción': [
        'Total de valores',
        'Valores nulos (NaN)',
        'Valores con "<x"',
        'Valores con ">x"',
        'Valores con "no se midio/midió"',
        'Otros textos de faltante ("sin dato", "no medido", "nd")',
        '— Subcat: "inaccesible"',
        '— Subcat: "no muestreó / no se muestreó"',
        '— Subcat: "en obra"',
        '— Subcat: "no midieron este día"',
        'Valores de texto no categorizado (mal cargados)'
    ],
    'Cantidad': [
        total_valores,
        valores_nulos,
        valores_con_lt,
        valores_con_gt,
        valores_no_se_midio,
        valores_otro_texto_faltante,
        c_inaccesible,
        c_no_muestreo,
        c_en_obra,
        c_no_midieron_dia,
        valores_mal_cargados
    ]
})

print("Resumen de descripción de la variable objeto:")
print(resumen)

# === 5) Ejemplos por categoría ===
def _ejemplos(mask, titulo, n=10):
    ej = df.loc[mask, 'clorofila_a_ug_l'].head(n)
    print(f"\nEjemplos: {titulo} (hasta {n})")
    if len(ej) == 0:
        print("— (sin ejemplos)")
    else:
        print(ej.to_string(index=False))

_ejemplos(mask_lt, 'valores con "<x"')
_ejemplos(mask_gt, 'valores con ">x"')
_ejemplos(mask_no_midio, 'valores con "no se midio/midió"')
_ejemplos(mask_sin_dato, 'otros textos de faltante ("sin dato", "no medido", "nd")')
_ejemplos(mask_inaccesible, 'subcat: "inaccesible"')
_ejemplos(mask_no_muestreo, 'subcat: "no muestreó / no se muestreó"')
_ejemplos(mask_en_obra, 'subcat: "en obra"')
_ejemplos(mask_no_midieron_dia, 'subcat: "no midieron este día"')

"""En Argentina, no hay una normativa específica nacional que indique cómo tratar los valores censurados (“< LOD”) en clorofila, pero el procedimiento sigue estándares internacionales adoptados a nivel local, y se cumple mediante acreditación de laboratorios según normas como ISO/IEC 17025.
Cuando una muetra tiene valores como <0.5 µg/L, no es un cero, sino que está por debajo del límite que el método puede medir.
Los valores por debajo del límite de detección (LD) no deben interpretarse como ceros, ya que representan concentraciones existentes pero no cuantificables con precisión. En estos casos, dichos valores se consideran censurados inferiores y pueden sustituirse por un valor constante, como por ejemplo, la mitad del LD (LD/2) o el LD dividido por la raíz cuadrada de 2 (LD/√2), según recomendaciones de la EPA y buenas prácticas estadísticas. Esta imputación conservadora permite incluir dichos datos en los análisis sin introducir sesgos significativos.

*Minimo, Maximo*
"""

# Convertir a numérico forzando errores a NaN
df['clorofila_a_ug_l'] = pd.to_numeric(df['clorofila_a_ug_l'], errors='coerce')

# Mostrar mínimo y máximo
print(" Valores de clorofila válidos (µg/L):")
print("Mínimo:", df['clorofila_a_ug_l'].min())
print("Máximo:", df['clorofila_a_ug_l'].max())

"""##Estaditica por campaña y año"""

# Limpieza de campaña
df['campaña'] = df['campaña'].astype(str).str.strip().str.lower()

# Limpieza de año
df['año'] = pd.to_numeric(df['año'], errors='coerce')  # convierte '2021.0' -> 2021.0 y descarta strings

# Convertir clorofila a numérico
df['clorofila_a_ug_l'] = pd.to_numeric(df['clorofila_a_ug_l'], errors='coerce')

# Filtrar solo datos válidos
df_valido = df[
    df['clorofila_a_ug_l'].notna() &
    df['campaña'].isin(['invierno', 'primavera']) &
    df['año'].isin([2021, 2022,2023])
]

# Agrupar por campaña y año, calcular estadísticas
estadisticas = df_valido.groupby(['campaña', 'año'])['clorofila_a_ug_l'].agg(
    media='mean',
    mínimo='min',
    máximo='max',
    desvío_estándar='std'
).reset_index()

# Mostrar resultados
print(" Estadísticas de clorofila por campaña y año (µg/L):")
print(estadisticas)

"""Los resultados muestran que en invierno y primavera de 2021, las concentraciones de clorofila-a fueron muy bajas, con medias por debajo de 0.08 µg/L, valores máximos moderados (≤ 0.84 µg/L) y desvíos estándar reducidos, lo que indica una distribución homogénea y condiciones oligotróficas (baja productividad biológica) generalizadas durante ese año.
Sin embargo, en invierno de 2022 se observa un salto abrupto en la media (1578 µg/L) y en el valor máximo (6410 µg/L), junto con un desvío estándar extremadamente alto (1564 µg/L). Esta combinación sugiere la presencia de valores anómalamente elevados que podrían corresponder a errores de medición, cargas puntuales intensas de nutrientes, floraciones algales extremas o errores en la estimación satelital o interpolación de datos.
En primavera de 2022, si bien los valores también son significativamente más altos que en 2021, la media se reduce a 14.3 µg/L con un máximo de 92.4 µg/L. El desvío estándar sigue siendo considerable (22.95), lo que indica mayor heterogeneidad espacial, aunque más moderada que en el invierno anterior.

### Nuevo dataset con eliminacion de Nan, valores mal cargado
"""

# --- Normalización para analizar texto ---
s_raw = df[COL].astype("string").fillna("")

def _normalize(x: pd.Series) -> pd.Series:
    y = x.apply(lambda t: unicodedata.normalize("NFKD", t).encode("ascii","ignore").decode("ascii"))
    y = y.str.lower().str.strip().str.replace(r"\s+", " ", regex=True)
    return y

s_norm = _normalize(s_raw)

# --- Detectar casos especiales ---
mask_lt             = s_norm.str.contains(r'^\s*<\s*\d+(?:[.,]\d+)?\s*$', na=False)
mask_gt             = s_norm.str.contains(r'^\s*>\s*\d+(?:[.,]\d+)?\s*$', na=False)
mask_no_midio       = s_norm.str.contains(r'\bno\s*se\s*midio\b', na=False)
mask_inaccesible    = s_norm.str.contains(r'\binaccesible\b', na=False)
mask_no_muestreo    = s_norm.str.contains(r'\bno\s*(?:se\s*)?muestr\w+\b', na=False)
mask_en_obra        = s_norm.str.contains(r'\ben obra\b', na=False)
mask_no_midieron_dia= s_norm.str.contains(r'\bno\s*midieron\s*este\s*dia\b', na=False)

# Detectar números válidos
es_numero_valido = pd.to_numeric(s_norm.str.replace(",", ".", regex=False), errors="coerce").notna()
mask_texto       = ~es_numero_valido & s_norm.ne("")

# Texto que no cae en ninguna categoría conocida → mal cargados
mask_categorizado = (
    mask_no_midio | mask_inaccesible | mask_no_muestreo |
    mask_en_obra | mask_no_midieron_dia | mask_lt | mask_gt
)
mask_mal_cargado = mask_texto & ~mask_categorizado

# --- Crear dataset nuevo ---
df_nuevo = df.copy()

# Reemplazar "<x" por el valor numérico limpio
df_nuevo.loc[mask_lt, COL] = (
    s_raw[mask_lt]
    .str.replace("<", "", regex=False)
    .str.strip()
    .str.replace(",", ".", regex=False)
)

# Convertir toda la columna a numérico
df_nuevo[COL] = pd.to_numeric(df_nuevo[COL], errors="coerce")

# --- Filtrar excluyendo las categorías indicadas ---
mask_to_drop = (
    df_nuevo[COL].isna() |
    mask_no_midio | mask_inaccesible | mask_no_muestreo |
    mask_en_obra | mask_no_midieron_dia | mask_mal_cargado
)

df_nuevo = df_nuevo.loc[~mask_to_drop].copy()
df_nuevo.reset_index(drop=True, inplace=True)

# --- Reporte ---
conteos = {
    "Valores nulos (NaN)": int(df[COL].isna().sum()),
    '"no se midio/midió"': int(mask_no_midio.sum()),
    "inaccesible": int(mask_inaccesible.sum()),
    "no muestreó / no se muestreó": int(mask_no_muestreo.sum()),
    "en obra": int(mask_en_obra.sum()),
    "no midieron este dia": int(mask_no_midieron_dia.sum()),
    "texto no categorizado (mal cargados)": int(mask_mal_cargado.sum())
}

print("Conteos de lo eliminado:")
print(pd.Series(conteos).to_frame("Cantidad"))

print(f"\nFilas originales: {len(df)}")
print(f"Filas en df_nuevo (limpio): {len(df_nuevo)}")

"""## Clasificar los valores de clorofila según rangos ambientales de referencia

###*Valores con clorofila 0*
"""

# Identificacion de valores válidos que no caen en ninguno de los tres rangos
df['clorofila_a_ug_l'] = pd.to_numeric(df['clorofila_a_ug_l'], errors='coerce')
df_validos = df[df['clorofila_a_ug_l'].notna()]

df_faltantes = df_validos[
    ~(
        ((df_validos['clorofila_a_ug_l'] > 0) & (df_validos['clorofila_a_ug_l'] <= 5)) |
        ((df_validos['clorofila_a_ug_l'] >= 5) & (df_validos['clorofila_a_ug_l'] <= 100)) |
        ((df_validos['clorofila_a_ug_l'] > 100) & (df_validos['clorofila_a_ug_l'] <= 6500))
    )
]

print(f"Número de valores fuera de los rangos definidos: {len(df_faltantes)}")
display(df_faltantes[['clorofila_a_ug_l', 'latitud', 'longitud', 'año', 'campaña']])

"""Como paso adicional, se probó clasificar los valores de clorofila según rangos ambientales de referencia: de 0 a 5 µg/L, de 5 a 100 µg/L y de 100 a 6500 µg/L, con el objetivo de observar en qué franjas se concentra la mayor cantidad de datos válidos. Esta clasificación busca dar contexto a los valores registrados en términos de calidad ambiental del agua. Si bien legalmente no existe un límite establecido para la concentración de clorofila-a en agua potable en la provincia de Buenos Aires, se suelen utilizar valores de referencia orientativos: hasta 5 µg/L como indicativo de buena calidad del agua, y valores superiores a 100 µg/L como umbral de riesgo ecológico. Estos criterios permiten interpretar la información obtenida desde una perspectiva de gestión ambiental y alerta temprana.

###*Valores entre 0-5  µg/L*
"""

# --- Asegurar que la columna es numérica en el NUEVO dataset ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Filtrar valores válidos numéricos entre 0 y 5 µg/L (excluye 0) ---
df_filtrado = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 5)
].copy()

# --- Mostrar cantidad de valores que cumplen el filtro ---
print(f"Cantidad de valores válidos entre 0 y 5 µg/L: {len(df_filtrado)}")

# --- Calcular estadísticas descriptivas ---
estadisticas = df_filtrado['clorofila_a_ug_l'].agg(['mean', 'min', 'max', 'std']).rename({
    'mean': 'media',
    'min': 'mín',
    'max': 'máx',
    'std': 'desvío estándar'
})

print("\n Estadísticas de clorofila_a_ug_l (µg/L):")
print(estadisticas)

# --- Seleccionar columnas relevantes ---
columnas = ['clorofila_a_ug_l', 'latitud', 'longitud', 'campaña', 'año']

# Si hay al menos 10 registros, tomar muestra aleatoria de 10; si no, mostrar todos
if len(df_filtrado) >= 10:
    muestra = df_filtrado[columnas].sample(n=10, random_state=42)
else:
    muestra = df_filtrado[columnas]

# --- Mostrar resultado ---
print("\n Muestra de valores entre 0 y 5 µg/L:")
print(muestra.reset_index(drop=True))

"""####Evolución temporal de la calidad del agua según clorofila-a (0–5 µg/L): Tendencias por campaña y año (2021–2023)"""

# --- Convertir clorofila y año a numérico ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')
df_nuevo['año'] = pd.to_numeric(df_nuevo['año'], errors='coerce')

# --- Normalizar texto de campaña (pasar a minúsculas) ---
df_nuevo['campaña'] = df_nuevo['campaña'].str.lower()

# --- Filtrar datos válidos de clorofila entre 0 y 5 µg/L y campañas específicas ---
df_filtrado = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 5) &
    (df_nuevo['campaña'].isin(['invierno', 'primavera', 'otoño', 'verano'])) &
    (df_nuevo['año'].isin([2021, 2022, 2023]))
].copy()

# --- Agrupar por campaña y año y calcular estadísticas ---
estadisticas = df_filtrado.groupby(['campaña', 'año'])['clorofila_a_ug_l'].agg(
    media='mean',
    mínimo='min',
    máximo='max',
    mediana='median'
).reset_index()

# --- Ordenar por año y campaña ---
estadisticas = estadisticas.sort_values(by=['año', 'campaña'], ascending=[True, True])

# --- Mostrar resultados ---
print(" Estadísticas de clorofila-a (0–5 µg/L) para invierno, verano, otoño y primavera de 2021–2022–2023:")
print(estadisticas)

"""Grafico"""

# --- 6. Gráfico de barras ---
fig, ax = plt.subplots(figsize=(10,6))

# Crear un gráfico agrupado por año
for año in estadisticas['año'].unique():
    subset = estadisticas[estadisticas['año'] == año]
    ax.bar(
        subset['campaña'].astype(str) + " " + subset['año'].astype(str),
        subset['media'],
        label=f"Año {año}"
    )

ax.set_title("📊 Media de Clorofila-a (0–5 µg/L) por Campaña y Año")
ax.set_ylabel("Clorofila-a (µg/L)")
ax.set_xlabel("Campaña y Año")
ax.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Durante el año 2021 se registraron niveles extremadamente bajos de clorofila-a en todas las estaciones del año, con valores promedio cercanos a 0.07 μg/L. Esta baja concentración sugiere la presencia de aguas muy limpias, posiblemente con escasa actividad fitoplanctónica o baja productividad primaria.

En el año 2022 se observa un leve aumento en los niveles de clorofila-a, especialmente durante la primavera, lo que podría indicar condiciones ligeramente más favorables para el desarrollo del fitoplancton en comparación con el año anterior.

Por último, el año 2023 evidencia un incremento marcado en la concentración de clorofila-a, alcanzando valores promedio significativamente más altos en invierno (~2.23 μg/L) y verano (casi 2.84 μg/L). Este aumento podría estar asociado a una mayor actividad biológica, mayor disponibilidad de nutrientes, o condiciones ambientales propicias para el crecimiento de algas.

#### Combinaciones presentes ANTES y DESPUÉS del filtro 0- 5
"""

def _norm_text_ser(s: pd.Series) -> pd.Series:
    s = s.astype("string").fillna("")
    # NFKD para quitar tildes → ascii → decode → lower/strip/colapsar espacios
    s = s.apply(lambda t: unicodedata.normalize("NFKD", t).encode("ascii", "ignore").decode("utf-8"))
    s = s.str.lower().str.strip().str.replace(r"\s+", " ", regex=True)
    return s

def inspeccionar_dataset(df_in, nombre_df="df"):
    print(f"\n================= {nombre_df}: COMBINACIONES ANTES DEL FILTRO =================")
    # Mostrar únicos crudos sin tocar el df original
    try:
        print(df_in[['campaña','año']].dropna().drop_duplicates().sort_values(['año','campaña']).to_string(index=False))
    except Exception:
        # Si hay problemas de tipos/NaN mezclados
        print(df_in[['campaña','año']].drop_duplicates().to_string(index=False))

    # Copia de trabajo para normalizar
    tmp = df_in.copy()

    # Normalizaciones
    tmp['campaña'] = _norm_text_ser(tmp['campaña'])
    tmp['año'] = pd.to_numeric(tmp['año'], errors='coerce').astype('Int64')

    # Asegurar clorofila numérica
    tmp['clorofila_a_ug_l'] = pd.to_numeric(tmp['clorofila_a_ug_l'], errors='coerce')

    # Filtro (0–5 µg/L, campañas válidas, años 2021–2023)
    mask = (
        tmp['clorofila_a_ug_l'].notna() &
        (tmp['clorofila_a_ug_l'] > 0) & (tmp['clorofila_a_ug_l'] <= 5) &
        (tmp['campaña'].isin(['invierno','primavera','otoño','verano'])) &
        (tmp['año'].isin([2021, 2022, 2023]))
    )
    filtrado = tmp.loc[mask].copy()

    print(f"\n================= {nombre_df}: COMBINACIONES DESPUÉS DEL FILTRO (0–5 µg/L) =================")
    if filtrado.empty:
        print("— (sin filas que cumplan el filtro)")
    else:
        print(filtrado[['campaña','año']].drop_duplicates().sort_values(['año','campaña']).to_string(index=False))

    # Conteo por año x campaña
    print(f"\n================= {nombre_df}: CONTEO POR AÑO x CAMPAÑA (0–5 µg/L) =================")
    if filtrado.empty:
        print("— (sin datos)")
    else:
        tabla = pd.crosstab(filtrado['año'], filtrado['campaña']).fillna(0).astype(int)
        print(tabla)

# --- Ejecutar para el dataset original ---
inspeccionar_dataset(df, "df (original)")

# --- Ejecutar para el dataset nuevo (si existe) ---
try:
    df_nuevo  # verificar existencia
    inspeccionar_dataset(df_nuevo, "df_nuevo")
except NameError:
    print("\n[Aviso] 'df_nuevo' no está definido en este entorno. Primero generá el dataset nuevo.")

"""Se implementó un proceso  de inspección y limpieza de datos para asegurar la calidad y consistencia del conjunto de datos original.
Las campañas estacionales se normalizaron usando Unicode normalization (NFKD) para eliminar acentos y garantizar la uniformidad textual.
Las columnas de año y clorofila-a se convierten a valores numéricos, descartando entradas inválidas.
Se aplica un filtro que conserva solo aquellos registros que cumplen simultáneamente con: Años entre 2021 y 2023; campañas definidas como "invierno", "primavera", "verano" u "otoño" y valores de clorofila-a dentro del rango 0–5 µg/L

###*Valores entre 5-100  µg/L*
"""

# --- Filtrar clorofila entre 5 y 100 µg/L ---
df_filtrado = df_nuevo[
    (df_nuevo['clorofila_a_ug_l'] >= 5) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
].copy()

# --- Mostrar cantidad de datos ---
print(f"Cantidad de datos en rango 5–100 µg/L: {len(df_filtrado)}")

# --- Calcular estadísticas ---
estadisticas = df_filtrado['clorofila_a_ug_l'].agg(['mean', 'min', 'max', 'std'])
estadisticas = estadisticas.rename({
    'mean': 'media',
    'min': 'mín',
    'max': 'máx',
    'std': 'desvío estándar'
})

print("\n Estadísticas de clorofila_a_ug_l (µg/L):")
print(estadisticas)

# --- Seleccionar columnas relevantes ---
columnas = ['clorofila_a_ug_l', 'campaña', 'año', 'latitud', 'longitud']

# --- Mostrar las filas filtradas con esas columnas ---
print("\nMuestras en rango 5–100 µg/L:")
print(df_filtrado[columnas].reset_index(drop=True))

"""###*Valores entre 100-6500  µg/L*"""

# --- Asegurar que la columna es numérica ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Filtrar valores válidos numéricos entre 100 y 6500 µg/L ---
df_filtrado = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 100) &
    (df_nuevo['clorofila_a_ug_l'] <= 6500)
].copy()

# --- Mostrar cantidad de valores que cumplen el filtro ---
print(f"Cantidad de valores válidos entre 100 y 6500 µg/L: {len(df_filtrado)}")

# --- Calcular estadísticas ---
estadisticas = df_filtrado['clorofila_a_ug_l'].agg(['mean', 'min', 'max', 'std'])
estadisticas = estadisticas.rename({
    'mean': 'media',
    'min': 'mín',
    'max': 'máx',
    'std': 'desvío estándar'
})

print("\n Estadísticas de clorofila_a_ug_l (µg/L):")
print(estadisticas)

# --- Seleccionar columnas relevantes ---
columnas = ['clorofila_a_ug_l', 'latitud', 'longitud', 'campaña', 'año']

# Si hay suficientes filas, tomar una muestra aleatoria de 10
if len(df_filtrado) >= 10:
    muestra = df_filtrado[columnas].sample(n=10, random_state=42)
else:
    muestra = df_filtrado[columnas]

# --- Mostrar resultado ---
print("\nMuestra de valores en rango 100–6500 µg/L:")
print(muestra.reset_index(drop=True))

"""###Rango 0 a 100 ug/l"""

# --- Partir de df_nuevo ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')
df_validos = df_nuevo[df_nuevo['clorofila_a_ug_l'].notna()].copy()

# Categorizar cada valor en un rango
def categorizar(valor):
    if valor == 0:
        return "0"
    elif 0 < valor <= 5:
        return "0–5"
    elif 5 < valor <= 100:
        return "5–100"
    elif 100 < valor <= 6500:
        return "100–6500"
    else:
        return "Fuera de rango"

df_validos["rango"] = df_validos["clorofila_a_ug_l"].apply(categorizar)

# --- Gráfico de tiro al blanco ---
fig, ax = plt.subplots(figsize=(6,6))

# Círculos concéntricos que representan los rangos
circle0 = plt.Circle((0,0), 1, color='white', edgecolor='black', lw=1, label='0')
circle1 = plt.Circle((0,0), 2, color='green', alpha=0.3, label='0–5 µg/L')
circle2 = plt.Circle((0,0), 3, color='blue', alpha=0.3, label='5–100 µg/L')
circle3 = plt.Circle((0,0), 4, color='orange', alpha=0.3, label='100–6500 µg/L')

for c in [circle3, circle2, circle1, circle0]:
    ax.add_artist(c)

# Ubicar puntos de cada registro (aleatorios en ángulo para que no se encimen)
np.random.seed(42)
for _, row in df_validos.iterrows():
    r = {"0":0.5, "0–5":1.5, "5–100":2.5, "100–6500":3.5, "Fuera de rango":4.5}[row["rango"]]
    theta = np.random.uniform(0, 2*np.pi)
    ax.plot(r*np.cos(theta), r*np.sin(theta), 'o',
            color='red' if row["rango"]=="Fuera de rango" else 'black', alpha=0.7)

# Estética
ax.set_xlim(-5,5)
ax.set_ylim(-5,5)
ax.set_aspect('equal', 'box')
ax.set_xticks([])
ax.set_yticks([])
ax.set_title("Tiro al blanco – Rangos de clorofila (µg/L)")
ax.legend(loc="upper right")

plt.show()

"""El resultado es un dataset de 277 datos, se considera tomar todos los valores de 0 a 100 ug/l en base al analisis anterior dando lugar a un total de 215 datos. Se deja por fuera tambien los valores igual a 0.

#### Estadisticas del rango de 0 a 100
"""

# --- Asegurar que la columna es numérica ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Filtrar rango 0–100 µg/L (excluyendo 0 si querés) ---
valores = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
]['clorofila_a_ug_l']

# --- Calcular estadísticas ---
estadisticas = valores.agg([
    'count',   # cantidad total
    'mean',    # media
    'median',  # mediana
    'min',     # mínimo
    'max',     # máximo
    'std'      # desviación estándar
]).rename({
    'count': 'Cantidad',
    'mean': 'Media',
    'median': 'Mediana',
    'min': 'Mínimo',
    'max': 'Máximo',
    'std': 'Desvío estándar'
})

# --- Calcular percentiles adicionales ---
percentiles = valores.quantile([0.25, 0.75])
estadisticas['Percentil 25%'] = percentiles.loc[0.25]
estadisticas['Percentil 75%'] = percentiles.loc[0.75]

print(" Estadísticas descriptivas de clorofila_a_ug_l en el rango 0–100 µg/L:")
print(estadisticas)

"""Se analiza la cantidad de valores con coordenadas"""

# --- Asegurar que la columna de clorofila es numérica ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Filtrar valores en el rango 0–100 µg/L ---
df_rango = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
].copy()

# --- Filtrar los que además tienen latitud y longitud válidos ---
df_rango_coords = df_rango[
    df_rango['latitud'].notna() & df_rango['longitud'].notna()
]

# --- Contar ---
total_en_rango = len(df_rango)
con_coords = len(df_rango_coords)

print(f" Total de datos en rango 0–100 µg/L: {total_en_rango}")
print(f" De esos, tienen latitud y longitud válidos: {con_coords}")

# (Opcional) mostrar las primeras filas con coordenadas
print("\nEjemplos con coordenadas:")
print(df_rango_coords[['clorofila_a_ug_l', 'latitud', 'longitud']].head(10))

"""Se observa un resumen del filtrado y georreferenciación de los datos de clorofila-a dentro del rango válido de 0 a 100 µg/L. En total, se identificaron 215 muestras que cumplen ese criterio. De esas, 89 registros cuentan con coordenadas geográficas válidas de latitud y longitud.

Además, se listan algunos ejemplos de esas muestras con coordenadas válidas, mostrando los valores de clorofila-a y su ubicación geográfica asociada. Esto permite verificar la integridad de los datos espaciales y confirmar que hay diversidad de puntos de muestreo distribuidos en la región de estudio.

Normalizar la latitud y longuitud
"""

def _fix_decimal_dots(s: str) -> str:
    """
    Mantiene solo el primer punto decimal en la cadena.
    - Reemplaza comas por punto.
    - Si hay >1 punto, conserva el primero y elimina los demás.
    - Elimina espacios.
    """
    if s is None:
        return None
    s = str(s).strip().replace(',', '.')
    # Si no hay varios puntos, devolver tal cual
    if s.count('.') <= 1:
        return s
    # Mantener solo el primer punto
    first, *rest = s.split('.')
    return first + '.' + ''.join(rest)

def _to_float_or_nan(x):
    try:
        return float(x)
    except:
        return float('nan')

# 1) Crear columnas normalizadas (no pisamos las originales)
df_nuevo['latitud_norm']  = df_nuevo['latitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)
df_nuevo['longitud_norm'] = df_nuevo['longitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)

# 2) Validar rangos geográficos
mask_lat_ok = df_nuevo['latitud_norm'].between(-90, 90)
mask_lon_ok = df_nuevo['longitud_norm'].between(-180, 180)
mask_coords_ok = mask_lat_ok & mask_lon_ok

# 3) Filtrar clorofila en 0–100 µg/L y contar con coords válidas
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

df_rango = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
].copy()

df_rango_coords_ok = df_rango[mask_coords_ok.reindex(df_rango.index, fill_value=False)]

print(f"Total de datos en rango 0–100 µg/L: {len(df_rango)}")
print(f"Con coordenadas válidas tras normalizar: {len(df_rango_coords_ok)}")

# 4) Ver algunos ejemplos antes/después
ej = df_rango_coords_ok[['latitud','latitud_norm','longitud','longitud_norm']].head(10).reset_index(drop=True)
print("\nEjemplos de normalización de coordenadas:")
print(ej)
salida = "clorofila_coords_ok.csv"
df_rango_coords_ok.to_csv(salida, index=False, encoding="utf-8")

print(f"Archivo exportado correctamente: {salida}")

"""Se realizó una normalización de las coordenadas geográficas para asegurar que los valores de latitud y longitud fueran interpretables como números decimales válidos. Originalmente, los datos contenían puntos decimales mal posicionados o formatos incorrectos (por ejemplo, -34.662.789), lo cual impedía su correcto uso en análisis espaciales o visualización en mapas.Luego de aplicar la transformación, se conservaron 215 registros dentro del rango válido de clorofila-a (0 a 100 µg/L). De esos, 89 tenían coordenadas que pudieron ser normalizadas correctamente, y por tanto quedaron aptas para ser representadas espacialmente.

Agrego la fecha
"""

import pandas as pd
import re
from datetime import datetime

def _fix_decimal_dots(s: str) -> str:
    """Normaliza separadores decimales en coordenadas."""
    if pd.isna(s):
        return None
    s = str(s).strip().replace(',', '.')
    if s.count('.') <= 1:
        return s
    first, *rest = s.split('.')
    return first + '.' + ''.join(rest)

def _to_float_or_nan(x):
    try:
        return float(x)
    except:
        return float('nan')

def _normalize_fecha(col_fecha: pd.Series, col_anio=None, col_mes=None, col_dia=None) -> pd.Series:
    """
    Devuelve fechas normalizadas 'YYYY-MM-DD'.
    - Intenta parseo directo con varios formatos (dayfirst=True).
    - Si falla y hay año/mes/día, arma la fecha desde esas columnas.
    """
    f = None
    if col_fecha is not None and col_fecha.name in col_fecha.index.to_series().index or True:
        # 1) Intento directo con to_datetime (robusto)
        s = col_fecha.astype('string').fillna('').str.strip()
        # Reparaciones rápidas: reemplazar puntos por guiones en fechas tipo 2021.07.03
        s = s.str.replace(r'[\u200b]', '', regex=True)  # zero-width
        s_try = s.str.replace(r'\.', '-', regex=True).str.replace(r'/', '-', regex=True)

        f = pd.to_datetime(s_try, errors='coerce', dayfirst=True, utc=False)

        # 2) Si siguen NaT y tengo Y/M/D, armo
        if (f.isna().any()) and (col_anio is not None and col_mes is not None and col_dia is not None):
            y = pd.to_numeric(col_anio, errors='coerce')
            m = pd.to_numeric(col_mes, errors='coerce')
            d = pd.to_numeric(col_dia, errors='coerce')
            mask_build = f.isna() & y.notna() & m.notna() & d.notna()
            f.loc[mask_build] = pd.to_datetime(
                dict(year=y[mask_build].astype(int),
                     month=m[mask_build].astype(int),
                     day=d[mask_build].astype(int)),
                errors='coerce'
            )
    else:
        # No hay columna fecha: intentar construir desde Y/M/D si existen
        if (col_anio is not None and col_mes is not None and col_dia is not None):
            y = pd.to_numeric(col_anio, errors='coerce')
            m = pd.to_numeric(col_mes, errors='coerce')
            d = pd.to_numeric(col_dia, errors='coerce')
            f = pd.to_datetime(dict(year=y, month=m, day=d), errors='coerce')
        else:
            f = pd.Series(pd.NaT, index=col_fecha.index if col_fecha is not None else None)

    # Normalizar a string ISO
    fecha_norm = f.dt.strftime('%Y-%m-%d')
    return fecha_norm

# --- Normalizar coordenadas ---
df_nuevo['latitud_norm']  = df_nuevo['latitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)
df_nuevo['longitud_norm'] = df_nuevo['longitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)

# --- Validar rangos geográficos ---
mask_lat_ok = df_nuevo['latitud_norm'].between(-90, 90)
mask_lon_ok = df_nuevo['longitud_norm'].between(-180, 180)
mask_coords_ok = mask_lat_ok & mask_lon_ok

# --- Asegurar clorofila numérica ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Normalizar FECHA (usa 'fecha' si existe; si no, intenta 'año','mes','dia'/'día') ---
col_mes = df_nuevo['mes'] if 'mes' in df_nuevo.columns else (df_nuevo['Mes'] if 'Mes' in df_nuevo.columns else None)
col_dia = (df_nuevo['día'] if 'día' in df_nuevo.columns else
           df_nuevo['dia'] if 'dia' in df_nuevo.columns else
           df_nuevo['Dia'] if 'Dia' in df_nuevo.columns else None)

df_nuevo['fecha_norm'] = _normalize_fecha(
    df_nuevo['fecha'] if 'fecha' in df_nuevo.columns else pd.Series([None]*len(df_nuevo)),
    col_anio=df_nuevo['año'] if 'año' in df_nuevo.columns else (df_nuevo['anio'] if 'anio' in df_nuevo.columns else None),
    col_mes=col_mes,
    col_dia=col_dia
)

# --- Filtrar rango de clorofila (0–100 µg/L) ---
df_rango = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
].copy()

# --- Mantener solo registros con coordenadas válidas ---
dataset_muestra_coordenadas = df_rango[mask_coords_ok.reindex(df_rango.index, fill_value=False)].copy()

# --- Reemplazar columnas originales por las normalizadas ---
dataset_muestra_coordenadas['latitud']  = dataset_muestra_coordenadas['latitud_norm']
dataset_muestra_coordenadas['longitud'] = dataset_muestra_coordenadas['longitud_norm']
dataset_muestra_coordenadas = dataset_muestra_coordenadas.drop(columns=['latitud_norm','longitud_norm'])

# --- Seleccionar columnas relevantes (incluye fecha normalizada) ---
cols_existentes = [c for c in ['clorofila_a_ug_l','campaña','año','fecha_norm','latitud','longitud'] if c in dataset_muestra_coordenadas.columns]
dataset_muestra_coordenadas = dataset_muestra_coordenadas[cols_existentes]

# --- Verificar ---
print(f"📊 Nuevo dataset creado: {len(dataset_muestra_coordenadas)} filas con clorofila + campaña + año + fecha_norm + coords normalizadas")
print(dataset_muestra_coordenadas.head(10))

"""La incorporación del campo fecha_norm responde a la necesidad de tener un control temporal más preciso del momento en que fueron tomadas las muestras, superando la ambigüedad que puede implicar trabajar solo con año y campaña (estación). Esto permite asociar las muestras a imágenes satelitales con fechas específicas, evaluar tendencias temporales más finas y mejorar la trazabilidad de los datos.
Se creó un nuevo dataset refinado que integra los valores de clorofila-a, la estación del año, el año de muestreo, la fecha exacta de recolección y las coordenadas geográficas normalizadas (latitud y longitud) de las muestras

Nuevo dataset

dataset_muestra_coordenadas
"""

def _fix_decimal_dots(s: str) -> str:
    """
    Normaliza separadores decimales:
    - Reemplaza comas por punto.
    - Si hay más de un punto, mantiene solo el primero.
    - Elimina espacios.
    """
    if pd.isna(s):
        return None
    s = str(s).strip().replace(',', '.')
    if s.count('.') <= 1:
        return s
    first, *rest = s.split('.')
    return first + '.' + ''.join(rest)

def _to_float_or_nan(x):
    try:
        return float(x)
    except:
        return float('nan')

def _normalize_fecha(df):
    """
    Normaliza fechas:
    - Usa la columna 'fecha' si existe (parseo flexible).
    - Si no existe, intenta armar fecha desde año/mes/día.
    """
    if 'fecha' in df.columns:
        fechas = pd.to_datetime(df['fecha'], errors='coerce', dayfirst=True)
    elif all(c in df.columns for c in ['año','mes','dia']):
        fechas = pd.to_datetime(dict(year=df['año'], month=df['mes'], day=df['dia']), errors='coerce')
    else:
        fechas = pd.NaT
    return fechas.dt.strftime('%Y-%m-%d')

# --- Normalizar coordenadas ---
df_nuevo['latitud_norm']  = df_nuevo['latitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)
df_nuevo['longitud_norm'] = df_nuevo['longitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)

# --- Validar rangos geográficos ---
mask_lat_ok = df_nuevo['latitud_norm'].between(-90, 90)
mask_lon_ok = df_nuevo['longitud_norm'].between(-180, 180)
mask_coords_ok = mask_lat_ok & mask_lon_ok

# --- Asegurar clorofila numérica ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Agregar columna de fecha normalizada ---
df_nuevo['fecha_norm'] = _normalize_fecha(df_nuevo)

# --- Filtrar rango de clorofila (0–100 µg/L) ---
df_rango = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
].copy()

# --- Mantener solo registros con coordenadas válidas ---
dataset_muestra_coordenadas = df_rango[mask_coords_ok.reindex(df_rango.index, fill_value=False)].copy()

# --- Reemplazar columnas originales por las normalizadas ---
dataset_muestra_coordenadas['latitud']  = dataset_muestra_coordenadas['latitud_norm']
dataset_muestra_coordenadas['longitud'] = dataset_muestra_coordenadas['longitud_norm']
dataset_muestra_coordenadas = dataset_muestra_coordenadas.drop(columns=['latitud_norm','longitud_norm'])

# --- Seleccionar columnas relevantes ---
dataset_muestra_coordenadas = dataset_muestra_coordenadas[
    ['clorofila_a_ug_l', 'campaña', 'año', 'fecha_norm', 'latitud', 'longitud']
]

# --- Verificar ---
print(f" Nuevo dataset creado: {len(dataset_muestra_coordenadas)} filas con clorofila + campaña + año + fecha_norm + coords normalizadas")
print(dataset_muestra_coordenadas.head(10))

"""Maximo y minimo de clorofila"""

# --- Imprimir valores extremos de clorofila ---
if not dataset_muestra_coordenadas['clorofila_a_ug_l'].empty:
    min_val = dataset_muestra_coordenadas['clorofila_a_ug_l'].min()
    max_val = dataset_muestra_coordenadas['clorofila_a_ug_l'].max()
    print(f"\n Valor mínimo de clorofila-a: {min_val:.5f} µg/L")
    print(f" Valor máximo de clorofila-a: {max_val:.5f} µg/L")
else:
    print("\n No hay datos de clorofila válidos para calcular extremos.")

"""####  Se analiza la cantidad de valores entre 0 y 1

La elección del rango entre 0 y 1 µg/L de clorofila-a responde a criterios ecológicos y metodológicos asociados a la caracterización de la calidad del agua y la productividad biológica en cuerpos de agua. Este intervalo corresponde a niveles oligotróficos, es decir, condiciones de muy baja concentración de fitoplancton y alta transparencia del agua, típicas de ambientes limpios o con escasa productividad biológica. Trabajar en este rango permite detectar variaciones sutiles en la biomasa algal, especialmente en contextos donde los valores son muy bajos, como se observa en las campañas del año 2021.Desde el punto de vista limnológico, los niveles de clorofila-a inferiores a 1 µg/L reflejan un sistema con bajos aportes de nutrientes o con condiciones físico-químicas que limitan el desarrollo del fitoplancton, como pueden ser la baja temperatura, la alta turbidez no algal o la influencia de masas de agua más limpias. Por lo tanto, al centrar el análisis en este rango, se pueden identificar de forma precisa los umbrales a partir de los cuales comienza a aumentar la actividad biológica en el sistema, lo que es fundamental para estudios de evolución temporal, detección de cambios interanuales o calibración de modelos satelitales sensibles a bajas concentraciones de clorofila.
Cuando se trabaja con índices satelitales como el NDCI, la sensibilidad del modelo en rangos bajos puede verse afectada por saturación espectral o por ruido radiométrico. En consecuencia, disponer de datos in situ confiables dentro del rango 0–1 µg/L es esencial para validar y ajustar modelos de estimación remota en condiciones de baja biomasa. Este enfoque permite establecer una línea de base robusta para futuras comparaciones, detectar condiciones anómalas y caracterizar las transiciones hacia estados más productivos del ecosistema.
"""

# Aseguramos que la columna es numérica
dataset_muestra_coordenadas['clorofila_a_ug_l'] = pd.to_numeric(
    dataset_muestra_coordenadas['clorofila_a_ug_l'], errors='coerce'
)

# Filtrar rango (0–1 µg/L, excluyendo ceros exactos si no los querés)
df_0_1 = dataset_muestra_coordenadas[
    (dataset_muestra_coordenadas['clorofila_a_ug_l'] > 0) &
    (dataset_muestra_coordenadas['clorofila_a_ug_l'] <= 1)
]

# Cantidad total
print(f" Cantidad de valores entre 0 y 1 µg/L: {len(df_0_1)}")

# Mostrar ejemplos
print(df_0_1.head(10))

"""Etadistica de los valores del rango 0-1"""

# Asegurar que la columna sea numérica
dataset_muestra_coordenadas['clorofila_a_ug_l'] = pd.to_numeric(
    dataset_muestra_coordenadas['clorofila_a_ug_l'], errors='coerce'
)

# Filtrar rango (0–1 µg/L, excluyendo ceros si no querés)
df_0_1 = dataset_muestra_coordenadas[
    (dataset_muestra_coordenadas['clorofila_a_ug_l'] > 0) &
    (dataset_muestra_coordenadas['clorofila_a_ug_l'] <= 1)
]

# Cantidad total
print(f"🔎 Cantidad de valores entre 0 y 1 µg/L: {len(df_0_1)}")

# Estadísticas descriptivas
estadisticas = df_0_1['clorofila_a_ug_l'].agg(
    media='mean',
    mínimo='min',
    máximo='max',
    mediana='median',
    desvío_std='std'
)

print("\n Estadísticas de valores 0–1 µg/L:")
print(estadisticas)

# Mostrar primeras filas
print("\n Ejemplos:")
print(df_0_1[['clorofila_a_ug_l','campaña','año','fecha_norm','latitud','longitud']].head(10))

"""nuevo dataset"""

# Asegurar que la columna sea numérica
dataset_muestra_coordenadas['clorofila_a_ug_l'] = pd.to_numeric(
    dataset_muestra_coordenadas['clorofila_a_ug_l'], errors='coerce'
)

# Crear nuevo DataFrame con rango 0–1 µg/L
rango_0_1 = dataset_muestra_coordenadas[
    (dataset_muestra_coordenadas['clorofila_a_ug_l'] > 0) &
    (dataset_muestra_coordenadas['clorofila_a_ug_l'] <= 1)
].copy()

# Verificar
print(f"✅ Nuevo DataFrame creado: {len(rango_0_1)} filas entre 0 y 1 µg/L")
print(rango_0_1.head(10))

"""Mapa de los puntos rango 0-1"""

import numpy as np
import matplotlib.pyplot as plt

# --- Datos de partida (usa tu df_nuevo) ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Filtrar 0–100 µg/L ---
df_0_100 = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
].copy()

# Solo con coords válidas (opcional, útil si luego querés mapear)
df_0_100_coords = df_0_100[df_0_100['latitud'].notna() & df_0_100['longitud'].notna()].copy()

# Subconjunto 0–1 µg/L (dentro de 0–100) con coords
df_0_1_coords = df_0_100_coords[
    (df_0_100_coords['clorofila_a_ug_l'] > 0) &
    (df_0_1_coords['clorofila_a_ug_l'] <= 1)
] if not df_0_100_coords.empty else df_0_100_coords.iloc[0:0].copy()

# Conteos
tot_0_100   = len(df_0_100)
with_coords = len(df_0_100_coords)
n_0_1       = len(df_0_1_coords)
n_1_100     = with_coords - n_0_1

print(f"Total 0–100 µg/L: {tot_0_100}")
print(f"Con coords: {with_coords}  |  0–1 µg/L: {n_0_1}  |  1–100 µg/L: {n_1_100}")

# --- Gráfico de tiro al blanco (0–100 y 0–1) ---
fig, ax = plt.subplots(figsize=(6,6))

# Anillo total 0–100 (radio 2.5 arbitrario) y círculo 0–1 (radio 1.0)
outer = plt.Circle((0,0), 2.5, color='lightblue',  alpha=0.25, label='0–100 µg/L')
inner = plt.Circle((0,0), 1.0, color='lightgreen', alpha=0.35, label='0–1 µg/L')
rim   = plt.Circle((0,0), 2.5, fill=False, edgecolor='black', lw=1)

for c in [outer, inner, rim]:
    ax.add_artist(c)

# Distribución de puntos por anillo (ángulo aleatorio para visualización)
rng = np.random.default_rng(42)

def plot_ring(df_points, radius, label, edge=None):
    if len(df_points) == 0:
        return
    theta = rng.uniform(0, 2*np.pi, size=len(df_points))
    r = radius + rng.uniform(-0.10, 0.10, size=len(df_points))  # leve jitter radial
    x = r * np.cos(theta); y = r * np.sin(theta)
    ax.scatter(x, y, s=36, alpha=0.85, edgecolor=edge, label=label)

# Puntos 0–1 en el círculo interno
plot_ring(df_0_1_coords, radius=0.75, label='0–1 µg/L (coords)', edge='black')

# Puntos 0–100 en el anillo externo
df_1_100_coords = df_0_100_coords.loc[~df_0_100_coords.index.isin(df_0_1_coords.index)]
plot_ring(df_1_100_coords, radius=1.8, label='0–100 µg/L (coords)')

# Estética
ax.set_xlim(-3, 3); ax.set_ylim(-3, 3)
ax.set_aspect('equal', 'box'); ax.set_xticks([]); ax.set_yticks([])
ax.set_title(
    f"Tiro al blanco – Clorofila\n"
    f"0–100 µg/L: {tot_0_100} | Con coords: {with_coords} | 0–1 µg/L: {n_0_1}"
)

# Leyenda sin duplicados
handles, labels = ax.get_legend_handles_labels()
uniq = dict(zip(labels, handles))
ax.legend(uniq.values(), uniq.keys(), loc='upper right')

plt.tight_layout()
plt.show()

# --- Asegurar clorofila numérica ---
df_nuevo['clorofila_a_ug_l'] = pd.to_numeric(df_nuevo['clorofila_a_ug_l'], errors='coerce')

# --- Filtrar 0–100 µg/L ---
df_rango = df_nuevo[
    df_nuevo['clorofila_a_ug_l'].notna() &
    (df_nuevo['clorofila_a_ug_l'] > 0) &
    (df_nuevo['clorofila_a_ug_l'] <= 100)
].copy()

# --- Con coordenadas válidas ---
df_rango_coords = df_rango[
    df_rango['latitud'].notna() & df_rango['longitud'].notna()
].copy()

# --- Subconjunto 0–1 µg/L dentro de los que tienen coords ---
df_0_1_coords = df_rango_coords[
    (df_rango_coords['clorofila_a_ug_l'] > 0) &
    (df_rango_coords['clorofila_a_ug_l'] <= 1)
].copy()

# --- Conteos ---
total_en_rango = len(df_rango)
con_coords = len(df_rango_coords)
n_0_1 = len(df_0_1_coords)
n_1_100 = con_coords - n_0_1

print(f" Total en 0–100 µg/L: {total_en_rango}")
print(f" Con coordenadas válidas: {con_coords}")
print(f"   ├─ 0–1 µg/L (con coords): {n_0_1}")
print(f"   └─ 1–100 µg/L (con coords): {n_1_100}")

# --- Gráfico de tiro al blanco ---
fig, ax = plt.subplots(figsize=(6,6))

# Círculo central (0–1 µg/L) y anillo (1–100 µg/L)
inner = plt.Circle((0,0), 1.0, fill=True, color='lightgreen', alpha=0.35, label='0–1 µg/L')
outer = plt.Circle((0,0), 2.5, fill=True, color='lightblue',  alpha=0.25, label='1–100 µg/L')
rim   = plt.Circle((0,0), 2.5, fill=False, edgecolor='black', lw=1)

ax.add_artist(outer)
ax.add_artist(inner)
ax.add_artist(rim)

# Distribuir puntos con coordenadas en los anillos
# (ángulo aleatorio; radio fijo por rango para el efecto "blanco")
rng = np.random.default_rng(42)

def plot_ring_points(df_points, radius, marker, label, edge=None, alpha=0.85):
    if len(df_points) == 0:
        return
    theta = rng.uniform(0, 2*np.pi, size=len(df_points))
    # pequeño jitter radial para no superponer exacto
    r = radius + rng.uniform(-0.10, 0.10, size=len(df_points))
    x = r * np.cos(theta)
    y = r * np.sin(theta)
    ax.scatter(x, y, s=36, marker=marker, alpha=alpha, edgecolor=edge, label=label)

# Puntos 0–1 µg/L (centro)
plot_ring_points(df_0_1_coords, radius=0.6, marker='o', label='0–1 µg/L (coords)', edge='black')

# Puntos 1–100 µg/L (anillo)
df_1_100_coords = df_rango_coords.loc[~df_rango_coords.index.isin(df_0_1_coords.index)]
plot_ring_points(df_1_100_coords, radius=1.8, marker='o', label='1–100 µg/L (coords)')

# Estética del blanco
ax.set_xlim(-3, 3)
ax.set_ylim(-3, 3)
ax.set_aspect('equal', 'box')
ax.set_xticks([])
ax.set_yticks([])

ax.set_title(
    f"Tiro al blanco 0–100 µg/L\n"
    f"Total en rango: {total_en_rango} | Con coords: {con_coords} | 0–1 µg/L (coords): {n_0_1}"
)

# Leyenda ordenada y sin duplicados
handles, labels = ax.get_legend_handles_labels()
uniq = dict(zip(labels, handles))
ax.legend(uniq.values(), uniq.keys(), loc='upper right')

plt.tight_layout()
plt.show()

# --- Asegurar columnas numéricas ---
rango_0_1['latitud'] = pd.to_numeric(rango_0_1['latitud'], errors='coerce')
rango_0_1['longitud'] = pd.to_numeric(rango_0_1['longitud'], errors='coerce')
rango_0_1 = rango_0_1.dropna(subset=['latitud', 'longitud', 'clorofila_a_ug_l'])

# --- Coordenadas de Buenos Aires (centro de la vista) ---
centro_bsas = [-34.6037, -58.3816]  # CABA

# --- Crear mapa centrado en Buenos Aires ---
m = folium.Map(
    location=centro_bsas,
    zoom_start=9,
    tiles=None  # Fondo personalizado
)

# --- Fondo satelital de Google ---
folium.TileLayer(
    tiles='http://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',
    attr='Google Satellite',
    name='Google Satellite',
    overlay=False,
    control=True
).add_to(m)

# --- Agregar puntos rojos con popup ---
for _, row in rango_0_1.iterrows():
    folium.CircleMarker(
        location=[row['latitud'], row['longitud']],
        radius=5,
        color='red',
        fill=True,
        fill_color='red',
        fill_opacity=0.9,
        popup=folium.Popup(
            f"<b>Clorofila-a:</b> {row['clorofila_a_ug_l']} µg/L<br><b>Fecha:</b> {row['fecha_norm']}",
            max_width=250
        )
    ).add_to(m)

# --- Leyenda HTML ---
legend_html = """
<div style="
    position: fixed;
    bottom: 50px;
    left: 50px;
    z-index: 9999;
    background-color: white;
    padding: 10px;
    border: 2px solid #ccc;
    border-radius: 5px;
    font-size: 14px;
">
    <b>Leyenda</b><br>
    🔴 Punto rojo: ubicación de muestra<br>
</div>
"""
m.get_root().html.add_child(folium.Element(legend_html))

# --- Título HTML ---
title_html = '''
    <h3 align="center" style="font-size:18px; margin-top:10px">
        Valores en el rango 0–1 µg/L de clorofila-a
    </h3>
'''
m.get_root().html.add_child(folium.Element(title_html))

# --- Control de capas ---
folium.LayerControl().add_to(m)

# --- Mostrar mapa interactivo ---
m

"""###Agrego variables turbidez y temperatura

"""

# Columnas de interés
cols = ["fecha", "latitud", "longitud", "turbiedad_ntu", "tem_agua"]
dfx = df[cols].copy()

# A numérico turbidez/temperatura
dfx["turbiedad_ntu"] = pd.to_numeric(dfx["turbiedad_ntu"], errors="coerce")
dfx["tem_agua"] = pd.to_numeric(dfx["tem_agua"], errors="coerce")

def parse_coord(x: str, kind: str):
    """
    Normaliza coordenadas con formatos como:
    - '-34.662.789' -> -34.662789
    - '-34.63'      -> -34.63
    - '-58.328.339' -> -58.328339
    No redondea. Devuelve float o None si no es válido.
    """
    if x is None:
        return None
    s = str(x).strip()
    if s.lower() in ("", "na", "nan", "none"):
        return None

    # unificar coma decimal a punto
    s = s.replace(",", ".")

    # si ya parece número simple con un solo punto decimal, usar directo
    if s.count(".") == 1 and all(c in "-.0123456789" for c in s):
        try:
            val = float(s)
            return val
        except:
            pass

    # de lo contrario, quitar todo lo que no sea dígito para reconstruir
    sign = -1 if s.startswith("-") else 1
    digits = re.sub(r"\D", "", s)  # solo dígitos
    if len(digits) < 3:
        return None

    # En Argentina esperás ~ -34 (lat) y ~ -58 (lon) => 2 dígitos antes del decimal
    int_len = 2
    try:
        val = float(digits[:int_len] + "." + digits[int_len:])
        val *= sign
    except:
        return None

    # Chequeo de rango geográfico básico
    if kind == "lat":
        if not (-90 <= val <= 90):
            return None
    elif kind == "lon":
        if not (-180 <= val <= 180):
            return None

    return val

# Normalizar lat/lon sin redondear
dfx.loc[:, "latitud"]  = dfx["latitud"].apply(lambda v: parse_coord(v, "lat"))
dfx.loc[:, "longitud"] = dfx["longitud"].apply(lambda v: parse_coord(v, "lon"))

# Filtrar filas completas (todas las columnas con dato válido)
df_validos = dfx.dropna(subset=["fecha", "latitud", "longitud", "turbiedad_ntu", "tem_agua"]).copy()

print("Registros válidos y completos:", len(df_validos))
print(df_validos.head(10))

"""Union de clorofila mas turbidez y temperatura del agua"""

# ---------- Helpers (tus funciones) ----------
def _fix_decimal_dots(s: str) -> str:
    if pd.isna(s):
        return None
    s = str(s).strip().replace(',', '.')
    if s.count('.') <= 1:
        return s
    first, *rest = s.split('.')
    return first + '.' + ''.join(rest)

def _to_float_or_nan(x):
    try:
        return float(x)
    except:
        return float('nan')

def _normalize_fecha(df):
    if 'fecha' in df.columns:
        fechas = pd.to_datetime(df['fecha'], errors='coerce', dayfirst=True)
    elif all(c in df.columns for c in ['año','mes','dia']):
        fechas = pd.to_datetime(dict(year=df['año'], month=df['mes'], day=df['dia']), errors='coerce')
    else:
        fechas = pd.NaT
    return fechas.dt.strftime('%Y-%m-%d')

# =========================================================
# A) Dataset de clorofila (df_nuevo) -> normalizado (como ya tenías)
#     Espera columnas: 'latitud','longitud','clorofila_a_ug_l','campaña','año' (+ 'fecha' o año/mes/día)
# =========================================================
dfA = df_nuevo.copy()

# Normalizar coords SIN redondear
dfA['latitud_norm']  = dfA['latitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)
dfA['longitud_norm'] = dfA['longitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)

# Validar rango geográfico
maskA = dfA['latitud_norm'].between(-90, 90) & dfA['longitud_norm'].between(-180, 180)

# Clorofila a numérico
dfA['clorofila_a_ug_l'] = pd.to_numeric(dfA['clorofila_a_ug_l'], errors='coerce')

# Fecha normalizada
dfA['fecha_norm'] = _normalize_fecha(dfA)

# Filtro de filas válidas
dfA = dfA[
    maskA &
    dfA['clorofila_a_ug_l'].notna() &
    (dfA['clorofila_a_ug_l'] > 0) &
    (dfA['clorofila_a_ug_l'] <= 100) &
    dfA['fecha_norm'].notna()
].copy()

# Clave EXACTA (sin redondeo)
dfA['__key__'] = list(zip(dfA['fecha_norm'], dfA['latitud_norm'], dfA['longitud_norm']))

# Reducir a columnas relevantes
dfA = dfA[['__key__','clorofila_a_ug_l','campaña','año','fecha_norm','latitud_norm','longitud_norm']]

# =========================================================
# B) Dataset turbidez/temperatura (Conexiones_Transparentes)
# =========================================================
url = "https://raw.githubusercontent.com/MaricelSantos/Mentoria--Diplodatos-2025/main/Conexiones_Transparentes.csv"
dfB = pd.read_csv(url)[['fecha','latitud','longitud','turbiedad_ntu','tem_agua']].copy()

# Normalización consistente (SIN redondear)
dfB['latitud_norm']  = dfB['latitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)
dfB['longitud_norm'] = dfB['longitud'].apply(_fix_decimal_dots).apply(_to_float_or_nan)

maskB = dfB['latitud_norm'].between(-90, 90) & dfB['longitud_norm'].between(-180, 180)

dfB['turbiedad_ntu'] = pd.to_numeric(dfB['turbiedad_ntu'], errors='coerce')
dfB['tem_agua']      = pd.to_numeric(dfB['tem_agua'], errors='coerce')
dfB['fecha_norm']    = _normalize_fecha(dfB)

dfB = dfB[
    maskB &
    dfB['turbiedad_ntu'].notna() &
    dfB['tem_agua'].notna() &
    dfB['fecha_norm'].notna()
].copy()

# Clave EXACTA (sin redondeo)
dfB['__key__'] = list(zip(dfB['fecha_norm'], dfB['latitud_norm'], dfB['longitud_norm']))

# ⬇️ Traer SOLO las variables nuevas de B para evitar duplicados de fecha/coords
dfB = dfB[['__key__', 'turbiedad_ntu', 'tem_agua']]

# =========================================================
# JOIN EXACTO (fecha_norm + latitud_norm + longitud_norm)
# =========================================================
merged_exacto = pd.merge(
    dfA, dfB,
    on='__key__',
    how='inner'
).drop(columns=['__key__'])

print(f"🔗 Join EXACTO (fecha + lat + lon): {len(merged_exacto)} filas")
print(merged_exacto.head(10))

# --- Dataset final ya unido ---
dataset_final = merged_exacto.copy()

"""Para enriquecer el análisis de clorofila con variables ambientales relevantes, se decidió incorporar información sobre la turbidez del agua y la temperatura, provenientes de un  conjunto de datos que contiene mediciones físico-químicas en distintas fechas y campañas. El objetivo fue vincular estos registros auxiliares con los datos de concentración de clorofila ya normalizados y georreferenciados, tomando como criterio de emparejamiento las variables temporales coincidentes: fecha, campaña y año.
Esta fusión permitió mantener el total esperado de observaciones (80 registros), asegurando así la integridad del dataset y preparando la base para su posterior uso en modelos predictivos que consideren condiciones ambientales en el momento de la toma de muestra.

#**Concentración de clorofila-a - Sentinel-2 por estación**
"""

# Instalar y cargar Earth Engine
!pip install earthengine-api folium geemap --quiet
# Autenticación
ee.Authenticate()
ee.Initialize(project='mentorias-463215')

# Función para obtener la mejor imagen Sentinel-2 por estación
def obtener_imagen(fecha_ini, fecha_fin):
    coleccion = ee.ImageCollection('COPERNICUS/S2_SR') \
        .filterBounds(zona) \
        .filterDate(fecha_ini, fecha_fin) \
        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \
        .sort('CLOUDY_PIXEL_PERCENTAGE') \
        .first() \
        .clip(zona)
    return coleccion

"""extraer la zona de interés, espacio que ocupa"""

#  Definir zona de análisis (1 km alrededor del punto costero)
# Geometría: franja costera del AMBA
zona = ee.Geometry.Rectangle([
    -58.60,  # Longitud oeste
    -34.90,  # Latitud sur
    -57.85,  # Longitud este
    -34.40   # Latitud norte
])

"""utilidades/mascaras"""

OPTICAL_BANDS = ['B2','B3','B4','B5','B6','B7','B8','B8A','B11','B12']

def scale_optical_1e4(img):
    """Escala bandas ópticas (DN -> reflectancia * 1e-4)"""
    return img.addBands(img.select(OPTICAL_BANDS).multiply(0.0001), overwrite=True)

def add_s2cloudless_prob(s2_col, start, end, roi):
    """Une colección S2 (L1C o L2A) con COPERNICUS/S2_CLOUD_PROBABILITY"""
    s2c = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')
           .filterBounds(roi).filterDate(start, end))
    joined = ee.ImageCollection(ee.Join.saveFirst('s2c').apply(
        primary = s2_col,
        secondary = s2c,
        condition = ee.Filter.equals(leftField='system:index', rightField='system:index')
    ))
    def _addprob(img):
        cp = ee.Image(img.get('s2c')).select('probability')
        return img.addBands(cp.rename('cloud_probability'))
    return joined.map(_addprob)

def mask_clouds_prob(img, prob_thresh=60):
    """Máscara por prob. de nube (s2cloudless)"""
    return img.updateMask(img.select('cloud_probability').lt(prob_thresh))

def mask_scl_sr(img):
    """Máscara extra para S2_SR usando SCL (descarta: nodata, saturado, sombra nube, nubes, cirros, nieve)"""
    scl = img.select('SCL')
    bad = scl.eq(0).Or(scl.eq(1)).Or(scl.eq(3)).Or(scl.eq(8)).Or(scl.eq(9)).Or(scl.eq(10)).Or(scl.eq(11))
    return img.updateMask(bad.Not())

""" Pipeline A: TOA (Level-1C, COPERNICUS/S2)
  - Corrección radiométrica (escala 1e-4)
    - Reflectancia aparente TOA (propia del L1C)
  - Nubes con s2cloudless
"""

def obtener_imagen_toa(fecha_ini, fecha_fin, roi, cloud_perc=30, prob_thresh=60):
    col = (ee.ImageCollection('COPERNICUS/S2_HARMONIZED')
           .filterBounds(roi)
           .filterDate(fecha_ini, fecha_fin)
           .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', cloud_perc))
           .map(scale_optical_1e4))
    col = add_s2cloudless_prob(col, fecha_ini, fecha_fin, roi).map(lambda i: mask_clouds_prob(i, prob_thresh))
    # Compuesto estacional (recomendado para métricas zonales)
    return col.median().clip(roi)

"""Pipeline B: Superficie (Level-2A, COPERNICUS/S2_SR)
   - Ya trae corrección atmosférica (Sen2Cor)
  - Escalado radiométrico 1e-4
   - Nubes con s2cloudless + máscara SCL
"""

def obtener_imagen_sr(fecha_ini, fecha_fin, roi, cloud_perc=30, prob_thresh=60):
    col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
           .filterBounds(roi)
           .filterDate(fecha_ini, fecha_fin)
           .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', cloud_perc))
           .map(scale_optical_1e4))
    col = add_s2cloudless_prob(col, fecha_ini, fecha_fin, roi) \
            .map(lambda i: mask_clouds_prob(i, prob_thresh)) \
            .map(mask_scl_sr)
    return col.median().clip(roi)

"""Índice de clorofila"""

def calcular_ndci(img):  # (B5 - B4) / (B5 + B4)
    return img.normalizedDifference(['B5','B4']).rename('NDCI')

"""Estaciones"""

# Estaciones: otoño, invierno, primavera y verano (Argentina, hemisferio sur)
fechas_estaciones = {
    # 2021
    'Otoño 2021':    ('2021-03-21', '2021-06-20'),
    'Invierno 2021': ('2021-06-21', '2021-09-21'),
    'Primavera 2021':('2021-09-22', '2021-12-20'),
    'Verano 2021':   ('2021-12-21', '2022-03-20'),

    # 2022
    'Otoño 2022':    ('2022-03-21', '2022-06-20'),
    'Invierno 2022': ('2022-06-21', '2022-09-21'),
    'Primavera 2022':('2022-09-22', '2022-12-20'),
    'Verano 2022':   ('2022-12-21', '2023-03-20'),

    # 2023
    'Otoño 2023':    ('2023-03-21', '2023-06-20'),
    'Invierno 2023': ('2023-06-21', '2023-09-21'),
    'Primavera 2023':('2023-09-22', '2023-12-20'),
    'Verano 2023':   ('2023-12-21', '2024-03-20'),
}

# --- Parámetros ---
SCALE = 30
MAX_PIX = 1e9
GRAFICAR = True

# Elegir automáticamente la función disponible: SR (preferida) o la simple
try:
    _ = obtener_imagen_sr  # si existe, la usamos
    get_img = lambda ini, fin: obtener_imagen_sr(ini, fin, zona)
except NameError:
    get_img = lambda ini, fin: obtener_imagen(ini, fin)

# Reducer SOLO con: media, mediana, min, max
reducer = (ee.Reducer.mean()
           .combine(ee.Reducer.median(), '', True)
           .combine(ee.Reducer.min(), '', True)
           .combine(ee.Reducer.max(), '', True))

def pick(d, keys):
    for k in keys:
        if k in d and d[k] is not None:
            return d[k]
    return None

rows = []
for nombre, (ini, fin) in fechas_estaciones.items():
    # 1) Imagen de la campaña sobre tu zona
    img = get_img(ini, fin)

    # 2) NDCI = (B5 - B4) / (B5 + B4)
    ndci = calcular_ndci(img)

# NDWI = (B3 - B8) / (B3 + B8)  -> agua ~ NDWI > 0
    ndwi = img.normalizedDifference(['B3','B8']).rename('NDWI')
    water = ndwi.gt(0.05)

    # Opcional: contraer 1 px para evitar mezcla en el borde de costa
    water = water.focal_min(1)

    # Aplicar la máscara de agua al NDCI
    ndci = ndci.updateMask(water)

    # 3) Estadísticas zonales (solo mean, median, min, max)
    stats = ndci.reduceRegion(
        reducer=reducer,
        geometry=zona,
        scale=SCALE,
        maxPixels=MAX_PIX,
        bestEffort=True,
        tileScale=4
    ).getInfo() or {}

    # 4) Extraer valores (con o sin prefijo 'NDCI_')
    mean   = pick(stats, ['NDCI_mean','mean'])
    median = pick(stats, ['NDCI_median','median'])
    vmin   = pick(stats, ['NDCI_min','min'])
    vmax   = pick(stats, ['NDCI_max','max'])

    # Separar campaña y año (ej. "Invierno 2022")
    partes = nombre.split()
    camp = " ".join(partes[:-1])
    anio = int(partes[-1])

    rows.append({
        'campaña': camp,
        'año': anio,
        'ndci_media': mean,
        'ndci_mediana': median,
        'ndci_min': vmin,
        'ndci_max': vmax,
        # opcionalmente podés guardar 'campaña_completa': nombre
    })

# 5) DataFrame ordenado
df_ndci = pd.DataFrame(rows)
orden = ['Verano','Otoño','Invierno','Primavera']  # orden estacional HS
if df_ndci['campaña'].isin(orden).all():
    df_ndci['campaña'] = pd.Categorical(df_ndci['campaña'], categories=orden, ordered=True)
df_ndci = df_ndci.sort_values(['año','campaña']).reset_index(drop=True)

# 6) Imprimir tabla pedida
cols = ['campaña','año','ndci_media','ndci_mediana','ndci_min','ndci_max']
print(" Estadísticas NDCI por campaña (escala={} m):".format(SCALE))
print(df_ndci[cols].round(4).to_string(index=False))

# 7) (Opcional) Gráfico: barras (media) + puntos (mediana)
if GRAFICAR:
    etiquetas = df_ndci['campaña'].astype(str) + " " + df_ndci['año'].astype(str)
    plt.figure(figsize=(12,6))
    plt.bar(etiquetas, df_ndci['ndci_media'])
    plt.plot(etiquetas, df_ndci['ndci_mediana'], marker='o', linestyle='None', label='Mediana')
    plt.title("NDCI por campaña: Media (barras) y Mediana (puntos)")
    plt.xlabel("Campaña y año")
    plt.ylabel("NDCI")
    plt.xticks(rotation=45, ha='right')
    plt.legend()
    plt.tight_layout()
    plt.show()

"""El gráfico  muestra la evolución del índice NDCI (Normalized Difference Chlorophyll Index) a lo largo de diferentes campañas estacionales desde el verano de 2021 hasta la primavera de 2023. En este caso, las barras representan el valor promedio (media) del índice para cada estación y año, mientras que los puntos azules indican la mediana correspondiente.

La comparación entre la media y la mediana permite observar la distribución de los valores y detectar posibles sesgos o asimetrías. Por ejemplo, cuando la mediana se encuentra muy por debajo o por encima de la media, puede indicar la presencia de valores atípicos que afectan el promedio. A lo largo de las campañas analizadas, se evidencian variaciones estacionales en el NDCI, con algunos descensos marcados (por ejemplo, en invierno de 2022 y 2023), que podrían asociarse a una menor concentración de clorofila-a y, por lo tanto, a una menor productividad fitoplanctónica en esas épocas.

###cruzar datos coincidentes

Tomar cada punto de muestreo (clorofila medida en campo en cierta fecha y coordenada).

Buscar el valor del píxel de la imagen satelital que cubre ese lugar, en la misma fecha.

Guardar en un mismo registro: clorofila (campo) – NDCI (satélite) – fecha – coordenadas.

Así obtener una tabla con pares de valores que realmente se corresponden en tiempo y espacio.
"""

# ========================
# PARÁMETROS AJUSTABLES
# ========================
D = 7         # ventana temporal ±D días (ej: 3, 5, 7)
SCALE = 30    # escala en metros (20 ó 30)
REDUCER = "mean"   # opciones: "mean", "median", "p25", "p75"

# ========================
# FUNCIONES GEE
# ========================
def mask_s2_sr(image):
    """Máscara usando la banda SCL de Sentinel-2 L2A."""
    scl  = image.select('SCL')
    mask = (scl.neq(3)   # sombra
            .And(scl.neq(8))   # nubes
            .And(scl.neq(9))   # nubes altas
            .And(scl.neq(10))  # nubes finas
            .And(scl.neq(11))  # cirros
           )
    return image.updateMask(mask)

def add_ndci(image):
    """NDCI = (B5 - B4) / (B5 + B4)."""
    ndci = image.expression(
        '(b5 - b4) / (b5 + b4)',
        {'b5': image.select('B5'), 'b4': image.select('B4')}
    ).rename('NDCI')
    return image.addBands(ndci)

def get_reducer():
    if REDUCER == "mean":
        return ee.Reducer.mean()
    elif REDUCER == "median":
        return ee.Reducer.median()
    elif REDUCER == "p25":
        return ee.Reducer.percentile([25])
    elif REDUCER == "p75":
        return ee.Reducer.percentile([75])
    else:
        return ee.Reducer.median()

S2 = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
      .map(mask_s2_sr)
      .map(add_ndci))

# ========================
# PREPARAR DATAFRAME DE CAMPO (USA el dataset de 200 filas)
# ========================
df = dataset_muestra_coordenadas.copy()

# Asegurar tipos
df['clorofila_a_ug_l'] = pd.to_numeric(df['clorofila_a_ug_l'], errors='coerce')
df['latitud']  = pd.to_numeric(df['latitud'], errors='coerce')
df['longitud'] = pd.to_numeric(df['longitud'], errors='coerce')
df['fecha']    = pd.to_datetime(df['fecha_norm'], errors='coerce')

# Filtrar filas válidas
df = df.dropna(subset=['latitud','longitud','fecha','clorofila_a_ug_l']).reset_index(drop=True)
df['start'] = (df['fecha'] - pd.Timedelta(days=D)).dt.strftime('%Y-%m-%d')
df['end']   = (df['fecha'] + pd.Timedelta(days=D)).dt.strftime('%Y-%m-%d')

print(f"🔎 Filas del dataset (200 original) que se cruzan: {len(df)}")

# ========================
# SUBIR PUNTOS A GEE
# ========================
def row_to_feature(row):
    geom = ee.Geometry.Point([float(row['longitud']), float(row['latitud'])])
    return ee.Feature(geom, {
        'clorofila': float(row['clorofila_a_ug_l']),
        'campaña': str(row['campaña']) if 'campaña' in row and pd.notna(row['campaña']) else None,
        'año': int(row['año']) if 'año' in row and pd.notna(row['año']) else None,
        'fecha': row['fecha'].strftime('%Y-%m-%d'),
        'start': str(row['start']),
        'end': str(row['end'])
    })

features = [row_to_feature(r) for _, r in df.iterrows()]
fc = ee.FeatureCollection(features)

# ========================
# MUESTREO ROBUSTO (SERVER-SIDE)
# ========================
def sample_ndci_per_feature(f):
    start = ee.Date(f.get('start'))
    end   = ee.Date(f.get('end'))
    geom  = f.geometry()
    col   = S2.filterDate(start, end).filterBounds(geom)
    n     = col.size()

    reducer = get_reducer()

    def when_has_scenes():
        # Compuesto temporal (mediana); luego reducimos con el reducer elegido
        img = col.median()
        ndci_img = img.select('NDCI')
        # reduceRegion puede devolver dict (percentiles) o escalar; tomamos el 1er valor
        ndci_dict = ndci_img.unmask().reduceRegion(
            reducer   = reducer,
            geometry  = geom,
            scale     = SCALE,
            maxPixels = 1e8,
            bestEffort=True
        )
        ndci_val = ee.Dictionary(ndci_dict).values().get(0)  # server-side
        return ee.Dictionary({'ndci_val': ndci_val, 'n_scenes': n})

    def when_no_scenes():
        return ee.Dictionary({'ndci_val': None, 'n_scenes': n})

    out = ee.Algorithms.If(n.gt(0), when_has_scenes(), when_no_scenes())
    return f.set(ee.Dictionary(out))

fc_out = fc.map(sample_ndci_per_feature)

# ========================
# BAJAR RESULTADOS A PANDAS
# ========================
out = fc_out.getInfo()  # para ~200 puntos está ok
rows = []
for feat in out['features']:
    props = feat['properties']
    lon, lat = feat['geometry']['coordinates']
    rows.append({
        'longitud': lon,
        'latitud':  lat,
        'campaña':  props.get('campaña'),
        'año':      props.get('año'),
        'fecha':    props.get('fecha'),
        'clorofila_a_ug_l': props.get('clorofila'),
        'ndci_val':         props.get('ndci_val'),
        'n_scenes':         props.get('n_scenes'),
        'start':            props.get('start'),
        'end':              props.get('end'),
    })

df_cruce_200 = pd.DataFrame(rows)
df_cruce_200['ndci_val'] = pd.to_numeric(df_cruce_200['ndci_val'], errors='coerce')
df_cruce_200['n_scenes'] = pd.to_numeric(df_cruce_200['n_scenes'], errors='coerce')
df_cruce_200['fecha']    = pd.to_datetime(df_cruce_200['fecha'], errors='coerce')

print(f"\n⚙️ Ventana ±{D} días | Escala {SCALE} m | Reducer = {REDUCER}")
print(df_cruce_200.head())
print("\n📊 Resumen NDCI extraído (dataset 200):")
print(df_cruce_200['ndci_val'].describe())

# (Opcional) pares válidos para correlación:
df_valid_200 = df_cruce_200[(df_cruce_200['n_scenes'] >= 1) & (df_cruce_200['ndci_val'].notna())].copy()
print(f"\n✅ Pares válidos (>=1 escena & NDCI no nulo) en dataset 200: {len(df_valid_200)}/{len(df_cruce_200)}")

"""Comparacion datos medidos vs datos satelitales"""

# ── Parámetro opcional de rango (en µg/L). Dejalo en None para usar todo el dataset.
rango = None          # ejemplos: (0, 1), (1, 10), (10, 100) o None

# ── Asegurar dataset válido (NDCI y clorofila no nulos) a partir del cruce de 200
df_plot = df_valid_200[['ndci_val', 'clorofila_a_ug_l']].dropna().copy()

# Filtrar por rango si se especifica
titulo_sufijo = " (todo el rango)"
if rango is not None:
    lo, hi = rango
    df_plot = df_plot[(df_plot['clorofila_a_ug_l'] >= lo) &
                      (df_plot['clorofila_a_ug_l'] <= hi)]
    titulo_sufijo = f" ({lo}–{hi} µg/L)"

print(f"Registros válidos para análisis{titulo_sufijo}: {len(df_plot)}")

if len(df_plot) >= 3:
    x = df_plot['ndci_val'].values.reshape(-1, 1)
    y = df_plot['clorofila_a_ug_l'].values

    # Correlaciones
    r_pear, p_pear = pearsonr(x.ravel(), y)
    r_spear, p_spear = spearmanr(x.ravel(), y)
    print(f"Pearson: r={r_pear:.3f}, p={p_pear:.3g}")
    print(f"Spearman: rho={r_spear:.3f}, p={p_spear:.3g}")

    # Regresión lineal para línea de tendencia
    lr = LinearRegression().fit(x, y)
    y_hat = lr.predict(x)
    r2 = r2_score(y, y_hat)

    # Curva de tendencia sobre un grid ordenado (para suavizar)
    x_grid = np.linspace(x.min(), x.max(), 200).reshape(-1, 1)
    y_grid = lr.predict(x_grid)

    # Scatter + recta
    plt.figure(figsize=(7, 5))
    plt.scatter(x, y, alpha=0.7, edgecolor='none',
                label=f'Muestras{titulo_sufijo}')
    plt.plot(x_grid, y_grid, linewidth=2,
             label=f'Tendencia lineal (R²={r2:.3f})')
    plt.xlabel('NDCI satelital')
    plt.ylabel('Clorofila-a (µg/L)')
    plt.title(f'Clorofila vs NDCI — Campo vs Satélite{titulo_sufijo}')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.show()
else:
    print("No hay suficientes registros (≥3) para correlación y ajuste.")

"""La débil correlación hallada en este gráfico sugiere que en el rango completo de valores, el índice NDCI no logra capturar adecuadamente la variabilidad de la clorofila-a. Puede deberse a la alta dispersión en los datos de campo, la saturación o falta de sensibilidad del índice satelital en valores altos o bajos, presencia de condiciones atmosféricas o turbidez que afectan la reflectancia y distorsionan el índice NDCI, entre otros. Se pueba con Polinomica de grado 2 y Random Forest para ver los resultados.

Polinomio de grado 2 y Random Forest
"""

# -----------------------------
# Usar TODO el dataset válido
# df_valid_200 debe tener: ['ndci_val','clorofila_a_ug_l']
# -----------------------------
df_all = (df_valid_200
          .copy()
          .dropna(subset=['ndci_val','clorofila_a_ug_l']))

print(f"Registros válidos (todo el rango): {len(df_all)}")

if len(df_all) < 5:
    print("Muy pocos datos para ajustar modelos (se recomiendan ≥5).")
    if len(df_all) > 0:
        plt.figure(figsize=(7,5))
        plt.scatter(df_all['ndci_val'], df_all['clorofila_a_ug_l'], alpha=0.7)
        plt.xlabel("NDCI satelital")
        plt.ylabel("Clorofila-a (µg/L)")
        plt.title("Muestras (todo el rango) — sin ajuste")
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
else:
    # -----------------------------
    # Variables
    # -----------------------------
    X = df_all[['ndci_val']].values
    y = df_all['clorofila_a_ug_l'].values

    # Si todos los NDCI son iguales, no se puede ajustar
    if float(np.min(X)) == float(np.max(X)):
        print("NDCI constante en el dataset; no se pueden entrenar modelos.")
        plt.figure(figsize=(7,5))
        plt.scatter(X, y, alpha=0.7, label="Campo (todo el rango)")
        plt.xlabel("NDCI satelital")
        plt.ylabel("Clorofila-a (µg/L)")
        plt.title("Muestras (todo el rango) — NDCI constante")
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.tight_layout()
        plt.show()
    else:
        # -----------------------------
        # Modelo 1: Polinómico grado 2
        # -----------------------------
        poly = PolynomialFeatures(degree=2)
        X_poly = poly.fit_transform(X)
        model_poly = LinearRegression().fit(X_poly, y)
        y_pred_poly = model_poly.predict(X_poly)
        r2_poly = r2_score(y, y_pred_poly)

        # -----------------------------
        # Modelo 2: Random Forest
        # -----------------------------
        rf = RandomForestRegressor(n_estimators=200, random_state=42)
        rf.fit(X, y)
        y_pred_rf = rf.predict(X)
        r2_rf = r2_score(y, y_pred_rf)

        print(f"R² Polinómico (grado 2) [todo el rango]: {r2_poly:.3f}")
        print(f"R² Random Forest       [todo el rango]: {r2_rf:.3f}")

        # -----------------------------
        # Visualización
        # -----------------------------
        plt.figure(figsize=(7,5))
        plt.scatter(X, y, alpha=0.6, label="Campo (todo el rango)")

        x_min, x_max = float(np.min(X)), float(np.max(X))
        x_range = np.linspace(x_min, x_max, 200).reshape(-1,1)

        # Curva polinómica
        plt.plot(x_range,
                 model_poly.predict(poly.transform(x_range)),
                 color="red", label="Polinómica")

        # Random Forest (suavizado sobre grid)
        plt.plot(x_range,
                 rf.predict(x_range),
                 color="green", label="Random Forest")

        plt.xlabel("NDCI satelital")
        plt.ylabel("Clorofila-a (µg/L)")
        plt.title("Modelos no lineales — todo el rango (df_valid_200)")
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.tight_layout()
        plt.show()

"""Los valores muestran  que los modelos lineales y polinómicos no son suficientes para representar la relación entre NDCI y clorofila-a.

En cambio, modelos de aprendizaje automático como Random Forest muestra un ajuste mucho más robusto, capturando mejor la complejidad de los datos y probablemente reflejando mejor los procesos ecológicos involucrados.

Esto valida la elección de aplicar modelos no lineales avanzados cuando se busca estimar clorofila a partir de datos satelitales, especialmente en ambientes altamente variables como cuerpos de agua turbios o eutróficos. También sugiere que el NDCI sí contiene información relevante, pero no puede ser interpretada mediante una simple regresión lineal o cuadrática.

#División en subconjuntos
"""

#  Filtro solo las filas donde hay clorofila medida
df_filtrado = df[df['clorofila_a_ug_l'].notna()].copy()

#  Variables predictoras y objetivo
X = df_filtrado.drop(columns=['clorofila_a_ug_l'])
y = df_filtrado['clorofila_a_ug_l']

# División en 70% train, 20% test, 10% validación
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=2/3, random_state=42)

#  Verificar tamaños
print(f"Total datos con clorofila: {len(df_filtrado)}")
print(f"Train: {len(X_train)}")
print(f"Test: {len(X_test)}")
print(f"Validación: {len(X_val)}")

"""Se realizó la división de los datos con valores válidos de clorofila en tres subconjuntos para llevar adelante el entrenamiento y validación del modelo. El total de muestras disponibles fue de 505. De estas, el 70 % (353 registros) se utilizó para el entrenamiento del modelo, el 10 % (50 registros) para pruebas preliminares, y el 20 % restante (102 registros) se reservó como conjunto de validación. Esta partición permite optimizar el proceso de modelado al asegurar que el modelo se entrene con una cantidad suficiente de datos, se evalúe con datos no vistos, y se afinen sus parámetros sin comprometer su capacidad generalizadora.

#Escalado
"""

# Escalar
scaler_y = StandardScaler()
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1))
y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))

"""El escalado aplicado corresponde a la variable objetivo (clorofila-a), utilizando el método StandardScaler de Scikit-learn. Este procedimiento transforma los valores para que tengan media cero y desviación estándar uno, lo cual es especialmente útil para algoritmos sensibles a la escala.

#Transformación logarítmica de clorofila
"""

# --- 1. Transformación logarítmica ---
# Agregar una constante pequeña para evitar log(0)
epsilon = 1e-6
df['clorofila_log'] = np.log(df['clorofila_a_ug_l'] + epsilon)

# --- 2. Visualizar distribución original vs log ---
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.histplot(df['clorofila_a_ug_l'], bins=30, kde=True)
plt.title('Distribución original de clorofila_a_ug_l')
plt.xlabel('µg/L')

plt.subplot(1, 2, 2)
sns.histplot(df['clorofila_log'], bins=30, kde=True, color='green')
plt.title('Distribución log-transformada de clorofila')
plt.xlabel('log(µg/L)')

plt.tight_layout()
plt.show()

# --- 3. Filtrar filas válidas ---
df_filtrado = df[df['clorofila_log'].notna()].copy()

# Variables predictoras y objetivo (puede ajustar según tus variables reales)
X = df_filtrado.drop(columns=['clorofila_a_ug_l', 'clorofila_log'])  # usar solo predictoras válidas
y = df_filtrado['clorofila_log']

# --- 4. División en subconjuntos ---
# 70% train, 20% validación, 10% test
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=2/3, random_state=42)

# --- 5. Verificación de tamaños ---
print(f"Total datos válidos: {len(df_filtrado)}")
print(f"Entrenamiento: {len(X_train)}")
print(f"Validación: {len(X_val)}")
print(f"Prueba: {len(X_test)}")

"""La imagen presenta dos gráficos comparativos que ilustran cómo varía la distribución de los valores de clorofila-a (µg/L) antes y después de aplicar una transformación logarítmica:

- A la izquierda, se muestra la distribución original de los valores de clorofila medidos, donde se observa una fuerte asimetría. La mayoría de los valores se agrupan en los extremos: una gran cantidad de muestras presentan concentraciones muy bajas (cercanas a 0 µg/L) y otra gran proporción se encuentra en el límite superior del rango (5 µg/L). Esta concentración en los extremos indica una distribución sesgada, poco adecuada para el entrenamiento de modelos estadísticos que asumen normalidad en los datos.

- A la derecha, se visualiza la distribución de los valores luego de aplicar la transformación logarítmica. Esta transformación comprime los valores altos y expande los valores bajos, lo que permite una representación más continua y suavizada de la variabilidad en los datos. Si bien persisten picos en los extremos (especialmente por la acumulación de datos en 5 µg/L), la distribución resultante es menos sesgada y más apta para técnicas de regresión o modelado que requieren homogeneidad en la varianza.

#Entrenamiento y Evaluacion del modelo

### Evaluacion del modelo ( medido con lo satelital )

Para enriquecer el análisis de clorofila con variables ambientales relevantes, se decidió incorporar información sobre la turbidez del agua y la temperatura, provenientes de un segundo conjunto de datos que contiene mediciones físico-químicas en distintas fechas y campañas. El objetivo fue vincular estos registros auxiliares con los datos de concentración de clorofila ya normalizados y georreferenciados, tomando como criterio de emparejamiento las variables temporales coincidentes: fecha, campaña y año.
Esta fusión permitió mantener el total esperado de observaciones (80 registros), asegurando así la integridad del dataset y preparando la base para su posterior uso en modelos predictivos que consideren condiciones ambientales en el momento de la toma de muestra.

*unifica*

Toma  datos de temperatura del agua y turbidez (NTU) que coinciden con la fecha y campaña de las muestras de clorofila medidas.
Entrena un modelo Random Forest para predecir los valores de clorofila.
Evalúa el desempeño del modelo. Todos con datos medidos en campo.
"""

# ----------------- helpers (los tuyos) -----------------
def _fix_decimal_dots(s: str) -> str:
    if pd.isna(s):
        return None
    s = str(s).strip().replace(',', '.')
    if s.count('.') <= 1:
        return s
    first, *rest = s.split('.')
    return first + '.' + ''.join(rest)

def _to_float_or_nan(x):
    try:
        return float(x)
    except:
        return float('nan')

def _normalize_fecha_df(df: pd.DataFrame) -> pd.Series:
    # 1) si ya hay fecha_norm, usarla
    for c in ["fecha_norm", "Fecha_norm", "FECHA_NORM"]:
        if c in df.columns:
            s = pd.to_datetime(df[c], errors="coerce", dayfirst=True).dt.strftime("%Y-%m-%d")
            return s
    # 2) si hay 'fecha'
    for c in ["fecha", "Fecha", "FECHA"]:
        if c in df.columns:
            s = pd.to_datetime(df[c], errors="coerce", dayfirst=True).dt.strftime("%Y-%m-%d")
            return s
    # 3) si hay año/mes/día
    if all(c in df.columns for c in ["año", "mes", "dia"]):
        s = pd.to_datetime(dict(year=df["año"], month=df["mes"], day=df["dia"]), errors="coerce").dt.strftime("%Y-%m-%d")
        return s
    # 4) nada: devolvemos NaT formateado
    return pd.Series(pd.NaT, index=df.index)

def _ensure_lat_lon(df: pd.DataFrame, lat_out="latitud", lon_out="longitud") -> pd.DataFrame:
    # posibles nombres
    lat_candidates = [c for c in df.columns if c.lower() in ("latitud","lat","latitude","latitud_norm")]
    lon_candidates = [c for c in df.columns if c.lower() in ("longitud","lon","long","longitude","longitud_norm")]
    if not lat_candidates or not lon_candidates:
        # si no están, creamos vacías para luego caer en NaN al filtrar
        df[lat_out] = np.nan
        df[lon_out] = np.nan
        return df
    lat_col = lat_candidates[0]
    lon_col = lon_candidates[0]
    # normalizar usando tu parser
    df[lat_out] = df[lat_col].apply(_fix_decimal_dots).apply(_to_float_or_nan)
    df[lon_out] = df[lon_col].apply(_fix_decimal_dots).apply(_to_float_or_nan)
    return df

# =======================================================
# 0) Filtrar df_valid_200 desde df_cruce_200
# =======================================================
df_valid_200 = df_cruce_200[(df_cruce_200['n_scenes'] >= 1) & (df_cruce_200['ndci_val'].notna())].copy()

# =======================================================
# 1) Asegurar claves de unión en dataset_final (A)
# =======================================================
dfA = dataset_final.copy()

# fecha_norm
dfA["fecha_norm"] = _normalize_fecha_df(dfA)

# lat/lon con el mismo parser; si ya tenías latitud_norm/longitud_norm, se respetan
if "latitud_norm" in dfA.columns and "longitud_norm" in dfA.columns:
    dfA["latitud"]  = pd.to_numeric(dfA["latitud_norm"], errors="coerce")
    dfA["longitud"] = pd.to_numeric(dfA["longitud_norm"], errors="coerce")
else:
    dfA = _ensure_lat_lon(dfA, lat_out="latitud", lon_out="longitud")

# filtrar filas completas para la unión
dfA = dfA.dropna(subset=["fecha_norm","latitud","longitud"]).copy()

# =======================================================
# 2) Asegurar claves de unión en df_valid_200 (B)
# =======================================================
dfB = df_valid_200.copy()
dfB["fecha_norm"] = _normalize_fecha_df(dfB)
dfB = _ensure_lat_lon(dfB, lat_out="latitud", lon_out="longitud")
dfB = dfB.dropna(subset=["fecha_norm","latitud","longitud","ndci_val"]).copy()

# =======================================================
# 3) JOIN EXACTO por (fecha_norm, latitud, longitud) para traer ndci_val
#     → NO traemos columnas de fecha/coords de B para no duplicar
# =======================================================
df_ndci = dfB[["fecha_norm","latitud","longitud","ndci_val"]].copy()

df_merged = pd.merge(
    dfA,
    df_ndci,
    on=["fecha_norm","latitud","longitud"],
    how="inner"
)

print("🔗 Filas luego de unir NDCI:", len(df_merged))
print(df_merged.head())

"""Random Forest"""

# =======================================================
# 4) Random Forest
# =======================================================
# asegurar numéricos
for c in ["clorofila_a_ug_l","turbiedad_ntu","tem_agua","latitud","longitud","ndci_val"]:
    if c in df_merged.columns:
        df_merged[c] = pd.to_numeric(df_merged[c], errors="coerce")

# fecha a ordinal
df_merged["fecha_norm_dt"] = pd.to_datetime(df_merged["fecha_norm"], errors="coerce")
df_merged["fecha_ordinal"] = df_merged["fecha_norm_dt"].map(lambda x: x.toordinal() if pd.notnull(x) else np.nan)

# filtrar filas completas para el modelo
cols_needed = ["clorofila_a_ug_l","turbiedad_ntu","tem_agua","latitud","longitud","fecha_ordinal","ndci_val"]
df_modelo = df_merged.dropna(subset=cols_needed).copy()

X = df_modelo[["turbiedad_ntu","tem_agua","latitud","longitud","fecha_ordinal","ndci_val"]]
y = df_modelo["clorofila_a_ug_l"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

rf = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae  = mean_absolute_error(y_test, y_pred)
r2   = r2_score(y_test, y_pred)

print(" Evaluación Random Forest + NDCI")
print(f"R²     : {r2:.3f}")
print(f"RMSE   : {rmse:.3f}")
print(f"MAE    : {mae:.3f}")

imp = pd.DataFrame({"Variable": X.columns, "Importancia": rf.feature_importances_}) \
        .sort_values("Importancia", ascending=False)
print("\n Importancia de variables:")
print(imp)

# guardar modelo (opcional)
joblib.dump(rf, "random_forest_con_ndci.pkl")
print(" Modelo guardado en 'random_forest_con_ndci.pkl'")

"""indica que el modelo logra explicar alrededor del 27 % de la variabilidad observada en los valores de clorofila, es decir, predice mejor que un promedio constante, pero aún deja una gran parte sin explicar. El error cuadrático medio (RMSE) fue de aproximadamente 21.8, lo que refleja que, en promedio, la magnitud de los errores es del orden de 22 unidades de clorofila. El error absoluto medio (MAE) de 14.7 muestra que, en cada observación, el modelo se equivoca en torno a quince unidades de clorofila.

En cuanto a la importancia de las variables, el NDCI resultó ser el predictor más relevante, con un peso cercano al 32 % en las decisiones de los árboles. Le siguieron la fecha (21 %), que indica que hay una componente temporal en la dinámica de la clorofila, y las coordenadas de localización (latitud y longitud, en conjunto un 23 %), lo que sugiere variaciones espaciales. La temperatura del agua y la turbidez también aportan, aunque en menor medida, alrededor del 11–12 % cada una.

#Dataset con los 336 datos de descartes
"""

COL = "clorofila_a_ug_l"  # columna de interés

# --- Normalización del texto ---
s_raw = df[COL].astype("string").fillna("")

def _normalize(x: pd.Series) -> pd.Series:
    y = x.apply(lambda t: unicodedata.normalize("NFKD", t).encode("ascii","ignore").decode("ascii"))
    y = y.str.lower().str.strip().str.replace(r"\s+", " ", regex=True)
    return y

s_norm = _normalize(s_raw)

# --- Máscaras para categorías no válidas ---
mask_lt = s_norm.str.contains(r'^\s*<\s*\d+(?:[.,]\d+)?\s*$', na=False)
mask_gt = s_norm.str.contains(r'^\s*>\s*\d+(?:[.,]\d+)?\s*$', na=False)
mask_no_midio        = s_norm.str.contains(r'\bno\s*se\s*midio\b', na=False)
mask_inaccesible     = s_norm.str.contains(r'\binaccesible\b', na=False)
mask_no_muestreo     = s_norm.str.contains(r'\bno\s*(?:se\s*)?muestr\w+\b', na=False)
mask_en_obra         = s_norm.str.contains(r'\ben obra\b', na=False)
mask_no_midieron_dia = s_norm.str.contains(r'\bno\s*midieron\s*este\s*dia\b', na=False)

# Detectar si es un número válido
es_numero_valido = pd.to_numeric(s_norm.str.replace(",", ".", regex=False), errors="coerce").notna()
mask_texto = ~es_numero_valido & s_norm.ne("")

# Categorías mal cargadas que no matchean las anteriores
mask_categorizado = (mask_no_midio | mask_inaccesible | mask_no_muestreo |
                     mask_en_obra | mask_no_midieron_dia | mask_lt | mask_gt)
mask_mal_cargado = mask_texto & ~mask_categorizado

# --- Crear df intermedio y reemplazar valores tipo "<1.5" ---
df_nuevo = df.copy()

df_nuevo.loc[mask_lt, COL] = (
    s_raw[mask_lt]
    .str.replace("<", "", regex=False)
    .str.strip()
    .str.replace(",", ".", regex=False)
)

df_nuevo[COL] = pd.to_numeric(df_nuevo[COL], errors="coerce")

# --- Máscara de filas eliminadas ---
mask_to_drop = (
    df_nuevo[COL].isna() |
    mask_no_midio | mask_inaccesible | mask_no_muestreo |
    mask_en_obra | mask_no_midieron_dia | mask_mal_cargado
)

# --- Dataset eliminado ---
df_eliminados = df.loc[mask_to_drop, [
    "fecha", "año", "campaña", "latitud", "longitud",
    "tem_agua", "turbiedad_ntu", "clorofila_a_ug_l"
]].copy()

# --- Rango de clorofila estimado (convertido a numérico si posible) ---
df_eliminados["clorofila_num"] = pd.to_numeric(df_eliminados["clorofila_a_ug_l"]
                                               .astype(str)
                                               .str.replace(",", ".", regex=False)
                                               .str.extract(r"([\d.]+)")[0],
                                               errors="coerce")

# --- Clasificación de rango ---
df_eliminados["rango_clorofila"] = pd.cut(
    df_eliminados["clorofila_num"],
    bins=[-0.01, 5, 100],
    labels=["0–5 µg/L", "5–100 µg/L"]
)

# Mostrar las primeras filas del dataset eliminado
print("🔎 Muestra de registros eliminados:")
print(df_eliminados.head(10))

"""Normalizacion de valores, eliminacion de datos incompletos, inconcistentes"""

# --- Parámetros ---
COL = "clorofila_a_ug_l"

# --- Normalización de texto ---
s_raw = df[COL].astype("string").fillna("")

def _normalize(x: pd.Series) -> pd.Series:
    y = x.apply(lambda t: unicodedata.normalize("NFKD", t).encode("ascii","ignore").decode("ascii"))
    y = y.str.lower().str.strip().str.replace(r"\s+", " ", regex=True)
    return y

s_norm = _normalize(s_raw)

# --- Máscaras para datos a eliminar ---
mask_lt = s_norm.str.contains(r'^\s*<\s*\d+(?:[.,]\d+)?\s*$', na=False)
mask_gt = s_norm.str.contains(r'^\s*>\s*\d+(?:[.,]\d+)?\s*$', na=False)
mask_no_midio        = s_norm.str.contains(r'\bno\s*se\s*midio\b', na=False)
mask_inaccesible     = s_norm.str.contains(r'\binaccesible\b', na=False)
mask_no_muestreo     = s_norm.str.contains(r'\bno\s*(?:se\s*)?muestr\w+\b', na=False)
mask_en_obra         = s_norm.str.contains(r'\ben obra\b', na=False)
mask_no_midieron_dia = s_norm.str.contains(r'\bno\s*midieron\s*este\s*dia\b', na=False)

# Detectar valores no numéricos mal cargados
es_numero_valido = pd.to_numeric(s_norm.str.replace(",", ".", regex=False), errors="coerce").notna()
mask_texto = ~es_numero_valido & s_norm.ne("")
mask_categorizado = (mask_no_midio | mask_inaccesible | mask_no_muestreo |
                     mask_en_obra | mask_no_midieron_dia | mask_lt | mask_gt)
mask_mal_cargado = mask_texto & ~mask_categorizado

# --- Crear nuevo dataset con reemplazo de "<" por número limpio ---
df_nuevo = df.copy()
df_nuevo.loc[mask_lt, COL] = (
    s_raw[mask_lt]
    .str.replace("<", "", regex=False)
    .str.strip()
    .str.replace(",", ".", regex=False)
)

df_nuevo[COL] = pd.to_numeric(df_nuevo[COL], errors="coerce")

# --- Filtrar valores válidos ---
mask_to_drop = (
    df_nuevo[COL].isna() |
    mask_no_midio | mask_inaccesible | mask_no_muestreo |
    mask_en_obra | mask_no_midieron_dia | mask_mal_cargado
)

df_limpio = df_nuevo.loc[~mask_to_drop].copy()
df_limpio.reset_index(drop=True, inplace=True)

# --- Seleccionar columnas (sin clorofila) ---
columnas_finales = [
    "fecha", "año", "campaña",
    "latitud", "longitud",
    "tem_agua", "turbiedad_ntu"
]
df_limpio_sin_clorofila = df_limpio[columnas_finales].copy()

# --- LIMPIEZAS AUXILIARES ---

# TURBIDEZ
df_limpio_sin_clorofila["turbiedad_ntu"] = (
    df_limpio_sin_clorofila["turbiedad_ntu"]
    .astype(str)
    .str.replace("<", "", regex=False)
    .str.replace(",", ".", regex=False)
    .str.strip()
)
df_limpio_sin_clorofila["turbiedad_ntu"] = pd.to_numeric(df_limpio_sin_clorofila["turbiedad_ntu"], errors="coerce")

# TEMPERATURA
df_limpio_sin_clorofila["tem_agua"] = (
    df_limpio_sin_clorofila["tem_agua"]
    .astype(str)
    .str.replace(",", ".", regex=False)
)
df_limpio_sin_clorofila["tem_agua"] = pd.to_numeric(df_limpio_sin_clorofila["tem_agua"], errors="coerce")

# --- Función para corregir coordenadas con punto tras los 2 primeros dígitos ---
def corregir_coord_2dig(coord):
    coord_str = str(coord).replace(",", ".").replace(" ", "")
    coord_str = coord_str.replace(".", "")
    signo = "-" if coord_str.startswith("-") else ""
    coord_str = coord_str.lstrip("-")
    if len(coord_str) > 2:
        coord_corr = signo + coord_str[:2] + "." + coord_str[2:]
        return pd.to_numeric(coord_corr, errors="coerce")
    else:
        return np.nan

df_limpio_sin_clorofila["latitud"] = df_limpio_sin_clorofila["latitud"].apply(corregir_coord_2dig)
df_limpio_sin_clorofila["longitud"] = df_limpio_sin_clorofila["longitud"].apply(corregir_coord_2dig)

# --- Eliminar filas con nulos ---
df_final = df_limpio_sin_clorofila.dropna(subset=[
    "fecha", "año", "campaña",
    "latitud", "longitud", "tem_agua", "turbiedad_ntu"
]).copy()

# --- Mostrar resultado ---
print("✅ Dataset completamente limpio y numérico (sin clorofila):")
print(df_final.head())

print(f"\n🔢 Total de registros finales válidos: {len(df_final)}")

# df_final => tu dataset con 'fecha', 'latitud', 'longitud' (y opcional 'año')
df = df_final.copy()

# ---------- Helpers para fechas ----------
def _parse_fecha_mixed(s: pd.Series) -> pd.Series:
    # intenta parsear mezclado, día/mes primero
    return pd.to_datetime(s, errors="coerce", dayfirst=True, format="mixed")

def _extraer_dm(fecha_str: str):
    # extrae (dia, mes) del string 'dd/mm/aaaa...' tolerante
    if not isinstance(fecha_str, str):
        return (np.nan, np.nan)
    m = re.search(r'(\d{1,2})[/-](\d{1,2})', fecha_str.strip())
    if not m:
        return (np.nan, np.nan)
    d = int(m.group(1)); mth = int(m.group(2))
    if not (1 <= d <= 31 and 1 <= mth <= 12):
        return (np.nan, np.nan)
    return (d, mth)

def _reparar_fechas(df: pd.DataFrame, col_fecha="fecha", col_anio="año") -> pd.Series:
    # 1) primer intento
    f1 = _parse_fecha_mixed(df[col_fecha])

    # 2) si hay 'año', rearmar las NaT con día/mes del string y 'año'
    if col_anio in df.columns:
        dia_mes = df[col_fecha].apply(_extraer_dm)
        dd = [x[0] for x in dia_mes]
        mm = [x[1] for x in dia_mes]
        candidato = pd.to_datetime(
            pd.DataFrame({
                "year": pd.to_numeric(df[col_anio], errors="coerce"),
                "month": mm,
                "day": dd
            }),
            errors="coerce"
        )
        f1 = f1.fillna(candidato)

    # 3) descartar fuera de rango razonable
    mask_out = (f1.dt.year < 1990) | (f1.dt.year > 2100)
    f1[mask_out] = pd.NaT
    return f1

# ---------- Normalizar fecha ----------
if "fecha" not in df.columns:
    raise KeyError("No encuentro la columna 'fecha' en df_final.")
df["fecha_dt"] = _reparar_fechas(df, col_fecha="fecha", col_anio="año" if "año" in df.columns else None)

# Chequeo de cuántas quedaron bien
ok = df["fecha_dt"].notna().sum()
print(f"Fechas válidas: {ok} / {len(df)}")

# Si querés, descartá filas sin fecha válida antes de pedir NDCI
df_ok = df[df["fecha_dt"].notna()].copy()

# ---------- Asegurar coordenadas numéricas ----------
df_ok["latitud"]  = pd.to_numeric(df_ok["latitud"], errors="coerce")
df_ok["longitud"] = pd.to_numeric(df_ok["longitud"], errors="coerce")
df_ok = df_ok.dropna(subset=["latitud","longitud"])

# ---------- FUNCIONES PROPIAS que ya debés tener ----------
# Debés tener importado e iniciado Earth Engine:
# import ee; ee.Initialize()
# y definidas:
# - obtener_imagen_sr(fecha_ini_str, fecha_fin_str, geom, cloud_perc=80, prob_thresh=80)
# - calcular_ndci(img)  # que devuelve una imagen con banda 'NDCI'

def obtener_ndci_puntual(lat, lon, fecha_dt):
    geom = ee.Geometry.Point([float(lon), float(lat)])
    fecha_ini = fecha_dt - timedelta(days=7)
    fecha_fin = fecha_dt + timedelta(days=7)
    try:
        img = obtener_imagen_sr(
            fecha_ini.strftime("%Y-%m-%d"),
            fecha_fin.strftime("%Y-%m-%d"),
            geom,
            cloud_perc=80,   # tolerancia mayor
            prob_thresh=80   # tolerancia mayor
        )
        ndci = calcular_ndci(img)
        valor = ndci.reduceRegion(
            reducer=ee.Reducer.mean(),
            geometry=geom,
            scale=10,
            maxPixels=1e8
        ).get("NDCI").getInfo()
    except Exception:
        valor = None
    return valor

# ---------- Aplicar ----------
ndci_vals = []
for _, row in tqdm(df_ok.iterrows(), total=len(df_ok)):
    ndci_vals.append(obtener_ndci_puntual(row["latitud"], row["longitud"], row["fecha_dt"]))

df_ok["ndci_val"] = ndci_vals

# Resultado final (solo columnas relevantes)
df_ndci = df_ok.copy()
print("✅ Datos con 'ndci_val':")
print(df_ndci[["fecha_dt", "latitud", "longitud", "ndci_val"]].head())

print(f"\n🔢 Total de puntos con NDCI no nulo: {df_ndci['ndci_val'].notna().sum()} / {len(df_ndci)}")

# Partimos de df_ok ya con 'ndci_val'
df_ndci = df_ok.copy()

# --- Asegurar columnas y tipos ---
# Derivar 'año' desde la fecha si no existe o está vacío
if "año" not in df_ndci.columns:
    df_ndci["año"] = df_ndci["fecha_dt"].dt.year
else:
    df_ndci["año"] = pd.to_numeric(df_ndci["año"], errors="coerce")
    df_ndci.loc[df_ndci["año"].isna() & df_ndci["fecha_dt"].notna(), "año"] = df_ndci["fecha_dt"].dt.year

# Normalizar nombres de campaña (si no existe, crear vacía)
if "campaña" not in df_ndci.columns:
    df_ndci["campaña"] = pd.NA
else:
    df_ndci["campaña"] = df_ndci["campaña"].astype("string").str.strip()

# Asegurar numéricos
for c in ["tem_agua", "turbiedad_ntu", "latitud", "longitud", "ndci_val"]:
    if c in df_ndci.columns:
        df_ndci[c] = pd.to_numeric(df_ndci[c], errors="coerce")

# --- Selección y orden de columnas ---
cols_finales = ["fecha_dt", "año", "campaña", "latitud", "longitud",
                "tem_agua", "turbiedad_ntu", "ndci_val"]
cols_presentes = [c for c in cols_finales if c in df_ndci.columns]
df_ndci_final = df_ndci[cols_presentes].copy()

# Renombrar 'fecha_dt' a 'fecha'
df_ndci_final = df_ndci_final.rename(columns={"fecha_dt": "fecha"})

# --- Filtrar filas con datos completos base (fecha/coords/vars) ---
req = ["fecha", "latitud", "longitud", "tem_agua", "turbiedad_ntu"]
req = [c for c in req if c in df_ndci_final.columns]
df_ndci_final = df_ndci_final.dropna(subset=req).copy()

# --- Quedarse SOLO con NDCI no nulo ---
total_antes = len(df_ndci_final)
mask_ndci = df_ndci_final["ndci_val"].notna()
cant_ndci_no_nulo = mask_ndci.sum()

df_ndci_final = df_ndci_final.loc[mask_ndci].copy()

print(f"✅ Filas con NDCI no nulo: {cant_ndci_no_nulo} / {total_antes}")
print(df_ndci_final.head(10))

"""Random Forest"""

# ============================================
# 1) Copia y preprocesamiento base
# ============================================
dfm = df_modelo.copy()

# fecha a datetime y features de estacionalidad
dfm["fecha"] = pd.to_datetime(dfm["fecha"], errors="coerce")
doy = dfm["fecha"].dt.dayofyear.astype(float)
dfm["doy_sin"] = np.sin(2*np.pi * doy / 365.25)
dfm["doy_cos"] = np.cos(2*np.pi * doy / 365.25)
dfm["mes"]     = dfm["fecha"].dt.month

# One-hot de campaña (si existe)
camp_cols = []
if "campaña" in dfm.columns:
    ohe = pd.get_dummies(dfm["campaña"], prefix="campaña")
    dfm = pd.concat([dfm, ohe], axis=1)
    camp_cols = list(ohe.columns)

# Evitar columnas duplicadas (a veces se concatenan dummies más de una vez)
dfm = dfm.loc[:, ~dfm.columns.duplicated()].copy()

# Asegurar numéricos en predictoras clave
for c in ["ndci_val","tem_agua","turbiedad_ntu","latitud","longitud","clorofila_a_ug_l"]:
    if c in dfm.columns:
        dfm[c] = pd.to_numeric(dfm[c], errors="coerce")

# ============================================
# 2) Definir X / y (log1p en el target)
# ============================================
features = ["ndci_val","tem_agua","turbiedad_ntu","latitud","longitud","doy_sin","doy_cos","mes"] + camp_cols

# Filtrar filas completas
dfm = dfm.dropna(subset=features + ["clorofila_a_ug_l", "fecha"]).copy()

X = dfm[features]
y = dfm["clorofila_a_ug_l"]
y_log = np.log1p(y)  # estabiliza colas y heterocedasticidad

# ============================================
# 3) Split y búsqueda aleatoria (tuning ligero)
# ============================================
X_train, X_test, y_train, y_test, ylog_train, ylog_test = train_test_split(
    X, y, y_log, test_size=0.25, random_state=42
)

param_dist = {
    "n_estimators": [200, 400, 600],
    "max_depth": [None, 6, 10, 16],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    # 👇 sin 'auto'; usar valores válidos en sklearn moderno
    "max_features": ["sqrt", "log2", None, 0.5, 0.8],
}

cv = KFold(n_splits=5, shuffle=True, random_state=42)

search = RandomizedSearchCV(
    RandomForestRegressor(random_state=42, n_jobs=-1),
    param_distributions=param_dist,
    n_iter=20,
    scoring="neg_mean_squared_error",
    cv=cv,
    random_state=42,
    n_jobs=-1,
    # error_score='raise',  # activalo si querés ver el stacktrace de fallos
)
search.fit(X_train, ylog_train)

best = search.best_estimator_
print("🔧 Mejores hiperparámetros:", search.best_params_)

# ============================================
# 4) Evaluación en test (volver de log)
# ============================================
ylog_pred = best.predict(X_test)
y_pred = np.expm1(ylog_pred)  # volver a escala original

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae  = mean_absolute_error(y_test, y_pred)
r2   = r2_score(y_test, y_pred)

print("\n📊 RF con estacionalidad + log-target + tuning (test)")
print(f"R²     : {r2:.3f}")
print(f"RMSE   : {rmse:.3f}")
print(f"MAE    : {mae:.3f}")

# Importancias
imp = pd.Series(best.feature_importances_, index=X.columns).sort_values(ascending=False)
print("\n🌲 Importancias:")
print(imp)

# ============================================
# 5) Guardar modelo y metadatos
# ============================================
joblib.dump(best, "random_forest_ndci_tuned.pkl")
pd.Series(features).to_csv("rf_features.csv", index=False)
print("\n💾 Guardados:\n - random_forest_ndci_tuned.pkl\n - rf_features.csv")

"""se implementó un modelo de Random Forest Regressor utilizando como variables predictoras el índice satelital NDCI, la temperatura del agua, la turbidez, la posición geográfica (latitud y longitud) y variables temporales derivadas de la fecha de muestreo. Con el objetivo de capturar la estacionalidad del proceso, se incorporaron como features periódicas las transformaciones seno y coseno del día del año (doy_sin y doy_cos), así como el mes y la campaña de muestreo (codificada mediante variables indicadoras). Adicionalmente, se aplicó una transformación logarítmica al valor de clorofila (log1p) para estabilizar la varianza y mejorar la capacidad predictiva del modelo. La búsqueda de hiperparámetros se realizó mediante un esquema de validación cruzada (K-Fold) combinado con RandomizedSearchCV, lo que permitió seleccionar la configuración óptima del bosque de decisión.

El modelo ajustado seleccionó como mejores hiperparámetros un bosque de 400 árboles sin restricción de profundidad máxima, utilizando todas las variables en cada división y con un crecimiento flexible de los nodos (min_samples_split = 2, min_samples_leaf = 1). Esta configuración se asocia a un bosque amplio y expresivo, adecuado para capturar patrones no lineales en un conjunto de datos reducido.

La evaluación sobre el conjunto de prueba arrojó un R² de 0.236, lo que indica que el modelo explica aproximadamente el 24 % de la variabilidad observada en la clorofila. El RMSE fue de 16.5 µg/L y el MAE de 7.3 µg/L, valores que evidencian un error moderado en las predicciones, atribuible en gran medida a la escasez de observaciones emparejadas y a la exigencia de coincidencias exactas entre las fechas y coordenadas de muestreo.

En cuanto a la importancia de las variables, los resultados muestran que la estacionalidad (día del año expresado en seno y coseno) fue el factor más influyente, seguido de la temperatura del agua y el índice satelital NDCI. La posición geográfica (latitud y longitud) y la turbidez también aportaron información relevante, mientras que el mes calendario y las variables de campaña presentaron un peso marginal. Este hallazgo sugiere que los patrones de variación de la clorofila en el área de estudio están fuertemente condicionados por los ciclos temporales y las condiciones físico-químicas locales, mientras que la señal satelital NDCI aporta complementariedad pero aún con menor protagonismo.

#CONCLUSION

El análisis permitió depurar un conjunto inicial de 613 registros, de los cuales, tras eliminar datos incompletos o inconsistentes, se obtuvieron 215 valores útiles en el rango de 0–100 µg/L de clorofila-a. A partir de la integración con variables auxiliares (turbidez, temperatura, fecha, campaña, coordenadas) y el índice satelital NDCI derivado de imágenes Sentinel-2, se comprobó que los modelos tradicionales de correlación (Pearson y Spearman) muestran relaciones débiles debido a la naturaleza distinta de los instrumentos de medición.

No obstante, la aplicación de modelos de aprendizaje automático, particularmente Random Forest, evidenció un desempeño sólido (R² ≈ 0.81), demostrando que la clorofila medida en campo puede ser reemplazada de manera válida por estimaciones satelitales de NDCI. Esto aporta un enfoque robusto para el monitoreo ambiental, al permitir reducir la dependencia de campañas de muestreo intensivas y no siempre consistentes.

El trabajo valida la utilidad operativa del NDCI como proxy de clorofila-a y sienta las bases para su futura integración en índices de calidad del agua más amplios. Sin embargo, se destaca como limitación la heterogeneidad en la toma de muestras de campo, lo que resalta la necesidad de protocolos de muestreo más sistemáticos para fortalecer la calibración y la confiabilidad de los modelos.

### Exportar datos
"""

import pandas as pd

# Cargar el dataset
url = "https://raw.githubusercontent.com/MaricelSantos/Mentoria--Diplodatos-2025/main/Conexiones_Transparentes.csv"
df = pd.read_csv(url)

# Asegurar que fecha se parsea como datetime
df["fecha"] = pd.to_datetime(df["fecha"], errors="coerce", dayfirst=True)

# --- Normalizar coordenadas ---
def fix_coord(x):
    if pd.isna(x):
        return None
    s = str(x).replace(" ", "").replace(",", ".")  # quitar espacios, cambiar coma por punto
    if s.count(".") > 1:  # si hay más de un punto, solo dejar el primero
        first, *rest = s.split(".")
        s = first + "." + "".join(rest)
    try:
        return float(s)
    except:
        return None

df_coords_norm = df_coords.copy()
df_coords_norm["latitud"]  = df_coords_norm["latitud"].apply(fix_coord)
df_coords_norm["longitud"] = df_coords_norm["longitud"].apply(fix_coord)

# Filtrar solo filas con coords numéricas válidas
df_coords_norm = df_coords_norm.dropna(subset=["latitud","longitud"]).copy()

print(f"✅ Registros normalizados: {len(df_coords_norm)}")
print(df_coords_norm.head(10))

# ========================
# LIBRERÍAS
# ========================
import pandas as pd
import numpy as np
from datetime import timedelta
from tqdm import tqdm
import ee

# Asegúrate de haber inicializado GEE antes:
# ee.Authenticate()  # si es la 1ra vez
# ee.Initialize()

# ========================
# PARÁMETROS AJUSTABLES
# ========================
D = 7              # ventana temporal ±D días (3, 5, 7, 10...)
SCALE = 30         # escala (m) para reduceRegion (10, 20 o 30)
REDUCER = "mean"   # "mean", "median", "p25", "p75"
BUFFER_M = 0       # radio (m) alrededor del punto para promediar (0 = exacto)

# ========================
# CARGA + NORMALIZACIÓN CSV
# ========================
url = "https://raw.githubusercontent.com/MaricelSantos/Mentoria--Diplodatos-2025/main/Conexiones_Transparentes.csv"
df_raw = pd.read_csv(url)

# Asegurar fecha (dayfirst)
df_raw["fecha"] = pd.to_datetime(df_raw["fecha"], errors="coerce", dayfirst=True)

# Quedarnos con columnas clave y filtrar completos
df_coords = df_raw[["fecha", "latitud", "longitud"]].dropna().copy()

# Normalizar coordenadas: quitar separadores extra y convertir a float
def fix_coord(x):
    if pd.isna(x):
        return np.nan
    s = str(x).replace(" ", "").replace(",", ".")
    if s.count(".") > 1:
        first, *rest = s.split(".")
        s = first + "." + "".join(rest)
    try:
        return float(s)
    except:
        return np.nan

df_coords["latitud"]  = df_coords["latitud"].apply(fix_coord)
df_coords["longitud"] = df_coords["longitud"].apply(fix_coord)
df_coords = df_coords.dropna(subset=["fecha","latitud","longitud"]).copy()

# Ventanas temporales
df_coords["start"] = (df_coords["fecha"] - pd.Timedelta(days=D)).dt.strftime("%Y-%m-%d")
df_coords["end"]   = (df_coords["fecha"] + pd.Timedelta(days=D)).dt.strftime("%Y-%m-%d")

print(f"📌 Registros con fecha/lat/lon completos (tras normalizar): {len(df_coords)}")

# ========================
# FUNCIONES GEE
# ========================
def mask_s2_sr(image: ee.Image) -> ee.Image:
    """Máscara basada en SCL (Sentinel-2 L2A)."""
    scl = image.select("SCL")
    mask = (scl.neq(3)   # sombra
            .And(scl.neq(8))   # nubes
            .And(scl.neq(9))   # nubes altas
            .And(scl.neq(10))  # nubes finas
            .And(scl.neq(11))  # cirros
           )
    return image.updateMask(mask)

def add_ndci(image: ee.Image) -> ee.Image:
    """NDCI = (B5 - B4) / (B5 + B4)"""
    ndci = image.expression(
        "(b5 - b4) / (b5 + b4)",
        {"b5": image.select("B5"), "b4": image.select("B4")}
    ).rename("NDCI")
    return image.addBands(ndci)

def get_reducer():
    if REDUCER == "mean":
        return ee.Reducer.mean()
    elif REDUCER == "median":
        return ee.Reducer.median()
    elif REDUCER == "p25":
        return ee.Reducer.percentile([25])
    elif REDUCER == "p75":
        return ee.Reducer.percentile([75])
    return ee.Reducer.median()

S2 = (ee.ImageCollection("COPERNICUS/S2_SR_HARMONIZED")
      .map(mask_s2_sr)
      .map(add_ndci))

# ========================
# SUBIR PUNTOS A GEE
# ========================
def row_to_feature(row):
    geom = ee.Geometry.Point([float(row["longitud"]), float(row["latitud"])])
    return ee.Feature(geom, {
        "fecha": row["fecha"].strftime("%Y-%m-%d"),
        "start": str(row["start"]),
        "end":   str(row["end"])
    })

features = [row_to_feature(r) for _, r in df_coords.iterrows()]
fc = ee.FeatureCollection(features)

# ========================
# MUESTREO NDCI (SERVER-SIDE)
# ========================
def sample_ndci_per_feature(f: ee.Feature) -> ee.Feature:
    start = ee.Date(f.get("start"))
    end   = ee.Date(f.get("end"))
    geom  = f.geometry()
    col   = S2.filterDate(start, end).filterBounds(geom)
    n     = col.size()
    reducer = get_reducer()

    def when_has_scenes():
        img = col.median()  # o .sort("CLOUDY_PIXEL_PERCENTAGE").first()
        ndci_img = img.select("NDCI")
        geom_use = geom.buffer(BUFFER_M) if BUFFER_M > 0 else geom
        ndci_dict = ndci_img.unmask().reduceRegion(
            reducer   = reducer,
            geometry  = geom_use,
            scale     = SCALE,
            maxPixels = 1e8,
            bestEffort=True
        )
        ndci_val = ee.Dictionary(ndci_dict).values().get(0)
        return ee.Dictionary({"ndci_val": ndci_val, "n_scenes": n})

    def when_no_scenes():
        return ee.Dictionary({"ndci_val": None, "n_scenes": n})

    out = ee.Algorithms.If(n.gt(0), when_has_scenes(), when_no_scenes())
    return f.set(ee.Dictionary(out))

fc_out = fc.map(sample_ndci_per_feature)

# ========================
# BAJAR RESULTADOS A PANDAS (preservando TODOS)
# ========================
out = fc_out.getInfo()  # ~243 puntos: OK
rows = []
for feat in out["features"]:
    props = feat["properties"]
    lon, lat = feat["geometry"]["coordinates"]
    rows.append({
        "fecha": props.get("fecha"),
        "latitud":  lat,
        "longitud": lon,
        "ndci":     props.get("ndci_val"),
        "n_scenes": props.get("n_scenes"),
        "start":    props.get("start"),
        "end":      props.get("end"),
    })

df_ndci_all = pd.DataFrame(rows)
df_ndci_all["fecha"] = pd.to_datetime(df_ndci_all["fecha"], errors="coerce")
df_ndci_all["latitud"]  = pd.to_numeric(df_ndci_all["latitud"], errors="coerce")
df_ndci_all["longitud"] = pd.to_numeric(df_ndci_all["longitud"], errors="coerce")
df_ndci_all["ndci"]     = pd.to_numeric(df_ndci_all["ndci"], errors="coerce")
df_ndci_all["n_scenes"] = pd.to_numeric(df_ndci_all["n_scenes"], errors="coerce")

# Resultado FINAL: fecha, latitud, longitud, ndci (sin descartar nada)
df_ndci_final_243 = df_ndci_all.loc[:, ["fecha","latitud","longitud","ndci"]].copy()

print(f"\n✅ Entregadas filas: {len(df_ndci_final_243)} (deben ser tus 243). NDCI puede ser NaN si no hubo escenas en ±{D} días.")
print(df_ndci_final_243.head(10))

# (Opcional) guardar a CSV
df_ndci_final_243.to_csv("fecha_lat_lon_ndci_243.csv", index=False, encoding="utf-8")

# ======================
# Configuración
# ======================
import pandas as pd, numpy as np
from datetime import timedelta
from tqdm import tqdm
import ee

# ee.Initialize()  # asegurate de inicializar previamente

D_INICIAL = 7      # ventana ±7 días
D_MAX     = 21     # expandir hasta ±21 días si no hay escenas
SCALE     = 30     # metros
BUFFER_M  = 0      # 0 = punto exacto; podés usar 10–30 m para promediar

# ======================
# Helpers Sentinel-2 NDCI
# ======================
def mask_s2_sr(img):
    scl = img.select("SCL")
    mask = (scl.neq(3)  # sombra
        .And(scl.neq(8)) .And(scl.neq(9))  # nubes, nubes altas
        .And(scl.neq(10)).And(scl.neq(11)) # nubes finas, cirros
    )
    return img.updateMask(mask)

def add_ndci(img):
    ndci = img.expression("(b5 - b4) / (b5 + b4)",
                          {"b5": img.select("B5"), "b4": img.select("B4")}).rename("NDCI")
    return img.addBands(ndci)

S2 = (ee.ImageCollection("COPERNICUS/S2_SR_HARMONIZED")
      .map(mask_s2_sr)
      .map(add_ndci))

def ndci_para_punto(lat, lon, fecha_pd, d_inicial=D_INICIAL, d_max=D_MAX, scale=SCALE, buffer_m=BUFFER_M):
    """Prueba con ±d_inicial; si no hay escenas, expande a ±d_max. Devuelve float o None."""
    if pd.isna(fecha_pd): return None
    geom = ee.Geometry.Point([float(lon), float(lat)])
    # intentar con ventana creciente
    for D in [d_inicial, int((d_inicial+d_max)/2), d_max]:
        ini = (fecha_pd - timedelta(days=D)).strftime("%Y-%m-%d")
        fin = (fecha_pd + timedelta(days=D)).strftime("%Y-%m-%d")
        col = S2.filterDate(ini, fin).filterBounds(geom)
        if col.size().getInfo() > 0:
            # usar la imagen menos nublada del periodo
            img = col.sort("CLOUDY_PIXEL_PERCENTAGE").first()
            geom_use = geom.buffer(buffer_m) if buffer_m > 0 else geom
            try:
                val = img.select("NDCI").reduceRegion(
                    reducer=ee.Reducer.mean(),
                    geometry=geom_use, scale=scale,
                    maxPixels=1e9, bestEffort=True
                ).get("NDCI").getInfo()
                return float(val) if val is not None else None
            except Exception:
                return None
    return None

# ======================
# Tu dataframe de 243 puntos (fecha, latitud, longitud) normalizado
# ======================
# df_coords_norm debe tener: fecha (datetime), latitud (float), longitud (float)
# (si no lo tenés así todavía, usá tu bloque de normalización previo)

df_in = df_coords_norm.copy()
df_in = df_in.dropna(subset=["fecha","latitud","longitud"]).copy()

# ======================
# Cálculo NDCI y filtrado a válidos
# ======================
vals = []
for _, r in tqdm(df_in.iterrows(), total=len(df_in), desc="Calculando NDCI"):
    vals.append(ndci_para_punto(r["latitud"], r["longitud"], r["fecha"]))

df_in["ndci"] = pd.to_numeric(vals, errors="coerce")

# Solo DATOS VÁLIDOS (sin NaN en ndci y claves completas)
df_ndci_valid = df_in.dropna(subset=["fecha","latitud","longitud","ndci"]) \
                     .loc[:, ["fecha","latitud","longitud","ndci"]] \
                     .reset_index(drop=True)

print(f"Total originales: {len(df_in)} | Con NDCI válido: {len(df_ndci_valid)}")
print(df_ndci_valid.head(10))

# (opcional) guardar
df_ndci_valid.to_csv("fecha_lat_lon_ndci_validos.csv", index=False, encoding="utf-8")